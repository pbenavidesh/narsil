---
title: "Forecasting"
format:
  html: default
  revealjs:
    output-file: forecasting_pres.html
---

::: {.content-visible unless-format="revealjs"}
```{r}
#| label: pkgs
#| message: false

library(tidyverse)
library(fpp3)
```
:::

# A tidy forecasting workflow

::: {.content-visible unless-format="html"}
## A tidy forecasting workflow
:::

![](fcst_wf.png)

## Tidy data

-   Use `readr::read_csv()`, `readxl::read_excel()`, or `tidyquant::tq_get()` to import the data into R. You can find more on this [here](https://r4ds.hadley.nz/import.html).

::: notes
-   Your data should be tidy. That means:

    -   Each variable should be in its own column.
    -   Each observation should be in its own row.
    -   Each value should be in its own cell.
:::

-   [Data tidying](https://r4ds.hadley.nz/data-tidy.html) and [transforming](https://r4ds.hadley.nz/transform.html) are covered in detail in [R for Data Science](https://r4ds.hadley.nz/).

-   Transform the resulting `tibble` into a **`tsibble`**:

    -   It should have an `index` (time) variable with the proper time format[^1].
    -   The `key` argument is only necessary if the dataset contains more than one time series.

[^1]: i.e., if the TS has a monthly frequency, the index variable should be in `yearmonth` format. Other formats coud be `yearweek`, `yearquarter`, `year`, `date`.

::: {.content-hidden unless-format="revealjs"}
## Train/test split
:::

::: {.content-visible unless-format="revealjs"}
### Train/test split
:::

-   Split the data into a **training set** and a **test set**[^2]. The training set is used to estimate the model parameters, while the test set is used to evaluate the model's performance on unseen data.

-   The size of the training and test sets depends on the length of the time series and the forecasting horizon:

    -   If the forecast horizon is e. g.. 12 months, the test set should contain 12 months of data.
    -   Another common approach is to use the first 70-80% of the data for training and the remaining 20-30% for testing.

-   We can use `filter_index()` to create the training set[^3]:

[^2]: Splitting the data into a training and test set is the minimum requirement for evaluating a forecasting model. If you want to avoid overfitting and get a more reliable estimate of the model's performance, you should consider splitting the data into 3 sets: **training, validation, and test sets**. The validation set is used to tune model hyperparameters and select the best model, while the test set is used for the final evaluation of the selected model. For an even more robust evaluation of forecasting models, consider using [**time series cross-validation**](https://otexts.com/fpp3/tscv.html) methods.

[^3]: and store it in a `*_train` object.

::: {.content-hidden unless-format="revealjs"}
. . .
:::

```{r}
#| label: filter_index
#| eval: false

datos_train <- <tsibble> |> 
  filter_index("start_date" ~ "end_date")  #<1>
```

1.  Replace `start_date` and `end_date` with the desired date range for the training set.You can also use `.` to indicate the start or end of the series: `filter_index(. ~ "end_date")` or `filter_index("start_date" ~ .)`.

:::: {.content-visible unless-format="revealjs"}
::: {.callout-note collapse="true"}
## Splitting the data

In time series, the training set should always contain the earlier observations, while the test set should contain the later observations. This is because time series data is ordered in time, and we want to simulate the real-world scenario where we use past data to predict future values.
:::
::::

## Visualize

Plot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots [here](../01_time_series/r_time_series.qmd#%20TS%20Visualization).

```{r}
#| label: ts_viz
#| echo: false

aus_production |> 
  gg_tsdisplay(log(Gas))
```

## Specify & Estimate

Decide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series' features.

Train the model specification on the training set. You can use the `model()` function to fit various forecasting models[^4].

[^4]: and store the model table in a `*_fit` object.

```{r}
#| label: model_estimation
#| eval: false

datos_fit <- datos_train |> 
  model(
    model_1 = <model_function_1>(<y_t> ~ x_t),                               #<1>
    model_2 = <model_function_2>(<transformation_function>(<y_t>), <args>)   #<2>
  )
```

1.  Replace `model_function_1` with the desired forecasting method (e.g., `ARIMA()`, `ETS()`, `NAIVE()`, etc.). Replace `<y_t>` with the name of the forecast variable and `<predictor_variables>` with any predictor variables if applicable.
2.  If a transformation is needed, replace `transformation_function` with the appropriate function (e.g., `log`, `box_cox`, etc.) and include any specific arguments required by the model.

## Evaluate

-   **Fitted** values, $\hat{y}_t$: The values predicted by the model for the training set.
-   **residuals**, $e_t$: The difference between the actual values and the fitted values, calculated as

$$
e_t = y_t - \hat{y}_t
$$.

-   **innovation residuals**: Residuals on the transformed scale[^5].

[^5]: We will focus on innovation residuals whenever a transformation is used in the model.

We can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble **white noise**.

:::: notes
::: {.callout-tip appearance="simple"}
The **fitted** values and **residuals** can be extracted from the model table using `augment()`.
:::
::::

::: {.content-hidden unless-format="revealjs"}
## What is white noise?
:::

::: {.content-visible unless-format="revealjs"}
### White noise
:::

{{< video https://youtu.be/ubFq-wV3Eic?si=7So2aw3T-IkulZj6 >}}

::: {.content-hidden unless-format="revealjs"}
## Residual diagnostics
:::

::: {.content-visible unless-format="revealjs"}
### Residual diagnostics
:::

We expect residuals to behave like white noise, thus having the following properties:

*The most important:*

1.  **Uncorrelated**: There is no correlation between the values at different time points.

2.  **Zero mean**: The average value of the series is constant over time (and equal to zero).

*Nice to have:*

3.  **Constant variance**: The variability of the series is constant over time.

4.  **Normally distributed**: The values follow a normal distribution (this is not always required).

::: {.content-hidden unless-format="revealjs"}
## Refine
:::

::: {.content-visible unless-format="revealjs"}
### Refine
:::

If the residuals don't meet these properties, we could refine the model:

-   For the first 2: add predictors or change the model structure.
-   Apply a variance-stabilizing transformation (e.g., Box-Cox).
-   If the residuals are not normally distributed, only the prediction intervals are affected. We can deal with this by using bootstrap prediction intervals.

::: {.content-hidden unless-format="revealjs"}
# Forecast
:::

::: {.content-visible unless-format="revealjs"}
## Forecast
:::

Once a satisfactory model is obtained, we can proceed to forecast[^6]. Use the `forecast()` function to generate forecasts for a specified horizon `h`.:

[^6]: and store the forecasts in a `*_fcst` object.

```{r}
#| label: forecast
#| eval: false

datos_fcst <- datos_fit |> 
  forecast(h = <forecast_horizon>)  #<1>
```

1.  Replace `<forecast_horizon>` with the desired number of periods to forecast (e.g., `12` for 12 months ahead), or you can write in text `"1 year"` for a one-year forecast.

::: {.callout-note appearance="simple"}
## Forecast horizon

The forecast horizon should have the same length as the test set to evaluate the model's performance accurately.
:::

::: {.content-hidden unless-format="revealjs"}
## Forecast accuracy
:::

::: {.content-visible unless-format="revealjs"}
### Forecast accuracy
:::

We measure a forecast's accuracy by measuring the forecast error. Forecast errors are computed as:

$$
e_{T+h} = y_{T+h} - \hat{y}_{T+h|T}
$$ 

::: {.notes} 
::: {.callout-warning appearance="simple"} 
These errors depend on the scale of the data. Therefore, they are not suitable for comparing forecast accuracy across series with different scales or units. 
::: 
:::

We can also measure errors as percentage errors[^7]:

[^7]: Percentage errors are scale-independent, making them useful for comparing forecast accuracy across different series.

$$
p_t = \frac{e_{T+h}}{y_{T+h}} \times 100
$$ 

:::{.notes} 
:::{.callout-warning appearance="simple"} 
Percentage errors can be problematic when the actual values are close to zero, leading to extremely high or undefined percentage errors. 
:::
:::

or scaled errors[^8].:

[^8]: Scaled errors are also scale-independent and are useful for comparing forecast accuracy across different series.

-   For non-seasonal time series:

$$
q_{j}=\frac{e_{j}}{\frac{1}{T-1} \sum_{t=2}^{T}\left|y_{t}-y_{t-1}\right|},
$$

For seasonal time series:

$$
q_{j}=\frac{e_{j}}{\frac{1}{T-m} \sum_{t=m+1}^{T}\left|y_{t}-y_{t-m}\right|}.
$$ :::{.content-hidden unless-format="revealjs"} \## Error metrics :::

::: {.content-visible unless-format="revealjs"}
### Error metrics
:::

Using this errors, we can compute various error metrics to summarize the forecast accuracy:

+------------------+---------+----------------------------------+-----------------------------------+
| Scale            | Metric  |    Description                   | Formula                           |
+==================+=========+==================================+===================================+
|Scale-dependent   | - RMSE  | - Root Mean Squared Error        | - $\sqrt{\text{mean}(e_{t}^{2})}$ |
|                  | - MAE   | - Mean Absolute Error            | - $\text{mean}(|e_{t}|)$          |
+------------------+---------+----------------------------------+-----------------------------------+
|Scale-independent | - MAPE  | - Mean Absolute Percentage Error | - $\text{mean}(|p_{t}|)$          |
|                  | - MASE  | - Mean Absolute Scaled Error     | - $\text{mean}(|q_{t}|)$          |
|                  | - RMMSE | - Root Mean Squared Scaled Error | - $\sqrt{\text{mean}(q_{t}^{2})}$ |
+------------------+---------+----------------------------------+-----------------------------------+

: Common error metrics

::: {.content-hidden unless-format="revealjs"}
## Refit and forecast
:::

### Refit and forecast

## Communicate

::: {.notes}
This is a note.
:::

# Benchmark forecasting methods
