---
title: "Forecasting Foundations"
format:
  html: default
  revealjs:
    output-file: forecasting_pres.html
---

```{r}
#| label: pkgs
#| message: false
#| include: false

library(tidyverse)
library(fpp3)
```

# A tidy forecasting workflow

::: {.content-visible unless-format="html"}
## A tidy forecasting workflow
:::

![](fcst_wf.png)

::: {.content-hidden unless-format="revealjs"}
## Foundations
:::

::: {.content-visible unless-format="revealjs"}
### Foundations
:::

Forecasting requires methodological discipline:

- Split the data (train/test)
- Establish **benchmark models**
- Forecast
- Measure accuracy
- Select a baseline
- Refit and produce the final forecast

::: {.content-hidden unless-format="revealjs"}
## Example dataset
:::

::: {.content-visible unless-format="revealjs"}
### Example dataset
:::

We use a built-in dataset from `fpp3`: `aus_production`.

- It is already a `tsibble`
- It contains multiple economic production series
- We focus on `Gas`

```{r}
#| label: example_data
#| echo: false

aus_production |> 
  select(Quarter, Gas) |> 
  slice_head(n = 6)
```

::: {.content-hidden unless-format="revealjs"}
## Visualize
:::

::: {.content-visible unless-format="revealjs"}
### Visualize
:::

```{r}
#| label: viz
#| echo: false

aus_production |> 
  gg_tsdisplay(Gas)
```

::: {.content-hidden unless-format="revealjs"}
## Train/test split
:::

::: {.content-visible unless-format="revealjs"}
### Train/test split
:::

- Split the series into **training** and **test** sets.
- The test set length should match the **forecast horizon**.

For this example, we use the last **8 quarters** as test data.

```{r}
#| label: split
#| echo: false

gas_train <- aus_production |> 
  filter_index(. ~ "2008 Q4")

gas_test <- aus_production |> 
  filter_index("2009 Q1" ~ .)

c(n_train = nrow(gas_train), n_test = nrow(gas_test))
```

:::: {.content-visible unless-format="revealjs"}
::: {.callout-note collapse="true"}
## Splitting the data

In time series, the training set must contain earlier observations and the test set later observations.  
We mimic the real-world scenario: use past data to forecast the future.
:::
::::

::: {.content-hidden unless-format="revealjs"}
## Benchmark models
:::

::: {.content-visible unless-format="revealjs"}
### Benchmark models
:::

Before fitting complex models, we establish benchmarks.

Common benchmark methods in tidyverts:

- Mean method: `MEAN()`
- Naïve method: `NAIVE()`
- Seasonal naïve method: `SNAIVE()`
- Drift method: `RW(... drift())`

```{r}
#| label: benchmark_fit
#| eval: false

bench_fit <- gas_train |> 
  model(
    mean   = MEAN(Gas),
    naive  = NAIVE(Gas),
    snaive = SNAIVE(Gas),
    drift  = RW(Gas ~ drift())
  )
```

::: {.content-hidden unless-format="revealjs"}
## Forecast
:::

::: {.content-visible unless-format="revealjs"}
### Forecast
:::

Generate forecasts with a horizon equal to the test set length.

```{r}
#| label: benchmark_forecast
#| eval: false

bench_fcst <- bench_fit |> 
  forecast(h = nrow(gas_test))
```

(Optional) Plot forecasts:

```{r}
#| label: plot_bench_forecast
#| eval: false

bench_fcst |> 
  autoplot(gas_train, level = 95) |> 
  autolayer(gas_test, Gas, alpha = 0.7)
```

::: {.content-hidden unless-format="revealjs"}
## Forecast accuracy
:::

::: {.content-visible unless-format="revealjs"}
### Forecast accuracy
:::

We measure forecast accuracy using forecast errors:

$$
e_{T+h} = y_{T+h} - \hat{y}_{T+h|T}
$$ 

Compute accuracy metrics:

```{r}
#| label: accuracy
#| eval: false

bench_acc <- bench_fcst |> 
  accuracy(gas_test) |> 
  arrange(MASE)

bench_acc
```

Selection rule:

- Prefer models that perform well on **MASE** (scale-independent).
- For seasonal data, `snaive` is often a strong benchmark.

::: {.content-hidden unless-format="revealjs"}
## Error metrics
:::

::: {.content-visible unless-format="revealjs"}
### Error metrics
:::

Using forecast errors, we can compute summary metrics:

::: {.nonincremental}

+------------------+---------+----------------------------------+-----------------------------------+
| Scale            | Metric  |    Description                   | Formula                           |
+==================+=========+==================================+===================================+
|Scale-dependent   | - RMSE  | - Root Mean Squared Error        | - $\sqrt{\text{mean}(e_{t}^{2})}$ |
|                  | - MAE   | - Mean Absolute Error            | - $\text{mean}(|e_{t}|)$          |
+------------------+---------+----------------------------------+-----------------------------------+
|Scale-independent | - MAPE  | - Mean Absolute Percentage Error | - $\text{mean}(|p_{t}|)$          |
|                  | - MASE  | - Mean Absolute Scaled Error     | - $\text{mean}(|q_{t}|)$          |
|                  | - RMMSE | - Root Mean Squared Scaled Error | - $\sqrt{\text{mean}(q_{t}^{2})}$ |
+------------------+---------+----------------------------------+-----------------------------------+

: Common error metrics

:::

::: {.content-hidden unless-format="revealjs"}
## Refit and forecast
:::

::: {.content-visible unless-format="revealjs"}
### Refit and forecast
:::

Once a benchmark is selected based on the test set, refit it using **all available data**, then forecast the desired future horizon.

Example: refit `snaive` and forecast the next 8 quarters.

```{r}
#| label: refit_forecast
#| eval: false

final_fit <- aus_production |> 
  model(
    final_model = SNAIVE(Gas)
  )

final_fcst <- final_fit |> 
  forecast(h = "8 quarters")
```

::: {.content-hidden unless-format="revealjs"}
## Communicate
:::

::: {.content-visible unless-format="revealjs"}
### Communicate
:::

Forecasting is not finished when numbers are produced.  
Results must be communicated clearly and honestly.

Minimum communication checklist:

- Plot: history + forecast + prediction intervals
- Horizon: what the forecast period represents
- Baseline: state the benchmark model used
- Uncertainty: prediction intervals are essential
- Limitations: structural breaks, short samples, changing conditions
