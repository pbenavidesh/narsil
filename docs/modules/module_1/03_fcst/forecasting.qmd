---
title: "Forecasting"
format:
  html: default
  revealjs:
    output-file: forecasting_pres.html
---

:::{.content-visible unless-format="revealjs"}

```{r}
#| label: pkgs
#| message: false

library(tidyverse)
library(fpp3)
```


:::

# A tidy forecasting workflow

:::{.content-visible unless-format="html"}

## A tidy forecasting workflow

:::

![](fcst_wf.png)





## Tidy data

- Use `readr::read_csv()`,  `readxl::read_excel()`, or `tidyquant::tq_get()` to import the data into R. You can find more on this [here](https://r4ds.hadley.nz/import.html).

:::{.notes}
- Your data should be tidy. That means:

  - Each variable should be in its own column.
  - Each observation should be in its own row.
  - Each value should be in its own cell.

:::

- [Data tidying](https://r4ds.hadley.nz/data-tidy.html) and [transforming](https://r4ds.hadley.nz/transform.html) are covered in detail in [R for Data Science](https://r4ds.hadley.nz/).

- Transform the resulting `tibble` into a **`tsibble`**:
  - It should have an `index` (time) variable with the proper time format^[i.e., if the TS has a monthly frequency, the index variable should be in `yearmonth` format. Other formats coud be `yearweek`, `yearquarter`, `year`, `date`.]. 
  - The `key` argument is only necessary if the dataset contains more than one time series.



:::{.content-hidden unless-format="revealjs"}
## Train/test split
:::

:::{.content-visible unless-format="revealjs"}
### Train/test split
:::

- Split the data into a **training set** and a **test set**^[Splitting the data into a training and test set is the minimum requirement for evaluating a forecasting model. If you want to avoid overfitting and get a more reliable estimate of the model's performance, you should consider splitting the data into 3 sets: **training, validation, and test sets**. The validation set is used to tune model hyperparameters and select the best model, while the test set is used for the final evaluation of the selected model. 

For an even more robust evaluation of forecasting models, consider using [**time series cross-validation**](https://otexts.com/fpp3/tscv.html) methods.]. The training set is used to estimate the model parameters, while the test set is used to evaluate the model's performance on unseen data.

- We can use `filter_index()` to create the training set^[and store it in a `*_train` object.]:

. . .

```{r}
#| label: filter_index
#| eval: false

datos_train <- <tsibble> |> 
  filter_index("start_date" ~ "end_date")  #<1>
```

1. Replace `start_date` and `end_date` with the desired date range for the training set.You can also use `.` to indicate the start or end of the series: `filter_index(. ~ "end_date")` or `filter_index("start_date" ~ .)`.


:::{.content-visible unless-format="revealjs"}

:::{.callout-note collapse="true"}
## Splitting the data

In time series, the training set should always contain the earlier observations, while the test set should contain the later observations. This is because time series data is ordered in time, and we want to simulate the real-world scenario where we use past data to predict future values.
:::

:::


## Visualize



Plot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots [here](../01_time_series/r_time_series.qmd# TS Visualization).

```{r}
#| label: ts_viz
#| echo: false

aus_production |> 
  gg_tsdisplay(log(Gas))
```



## Specify & Estimate


Decide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series' features.

Train the model specification on the training set. You can use the `model()` function to fit various forecasting models^[and store the model table in a `*_fit` object.].

```{r}
#| label: model_estimation
#| eval: false

datos_fit <- datos_train |> 
  model(
    model_1 = <model_function_1>(<y_t> ~ x_t),                               #<1>
    model_2 = <model_function_2>(<transformation_function>(<y_t>), <args>)   #<2>
  )
```

1. Replace `model_function_1` with the desired forecasting method (e.g., `ARIMA()`, `ETS()`, `NAIVE()`, etc.). Replace `<y_t>` with the name of the forecast variable and `<predictor_variables>` with any predictor variables if applicable.
2. If a transformation is needed, replace `transformation_function` with the appropriate function (e.g., `log`, `box_cox`, etc.) and include any specific arguments required by the model.



## Evaluate

- **Fitted** values, $\hat{y}_t$: The values predicted by the model for the training set.
- **residuals**, $e_t$: The difference between the actual values and the fitted values, calculated as $e_t = y_t - \hat{y}_t$.
- **innovation residuals**: Residuals on the transformed scale^[We will focus on innovation residuals whenever a transformation is used in the model.].

We can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble **white noise**.

:::{.notes}
:::{.callout-tip appearance="simple"}
The **fitted** values and **residuals** can be extracted from the model table using `augment()`.
:::
:::

:::{.content-hidden unless-format="revealjs"}
## What is white noise?
:::

:::{.content-visible unless-format="revealjs"}
### White noise
:::

{{< video https://youtu.be/ubFq-wV3Eic?si=7So2aw3T-IkulZj6 >}}

:::{.notes}
White noise is a sequence of random variables that are:
- **Uncorrelated**: There is no correlation between the values at different time points.
- **Constant mean**: The average value of the series is constant over time (usually zero).
- **Constant variance**: The variability of the series is constant over time.
- **Normally distributed**: The values follow a normal distribution (this is not always required).
:::

:::{.content-hidden unless-format="revealjs"}
## Residual diagnostics
:::

:::{.content-visible unless-format="revealjs"}
### Residual diagnostics
:::



:::{.content-hidden unless-format="revealjs"}
## Refine
:::

:::{.content-visible unless-format="revealjs"}
### Refine
:::

:::{.content-hidden unless-format="revealjs"}
# Forecast
:::

:::{.content-visible unless-format="revealjs"}
## Forecast
:::


### Forecast on the test set


## Refit and forecast






## Communicate





:::{.notes}

This is a note.

:::

# Benchmark forecasting methods

