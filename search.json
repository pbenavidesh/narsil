[
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntidyverse is a meta-package that loads the core packages of the tidyverse.\n\n\nWe will always load all the required packages a the beginning of the document. When loading the tidyverse, it shows which packages are being attached, as well as any conflicts with previously loaded packages.\n\n\n\n\n\n\nNoteCore packages\n\n\n\n\n\n\ndplyr is the core package for data transformation. It is paired up with the following packages for specific column types:\n\nstringr for strings.\nforcats for factors (R’s categorical data type).\nlubridate for dates and date-times.\n\nggplot2 is the primary package for visualization.\nreadr is used to import data from delimited files (CSV, TSV, …).\ntibble is a modern reimagining of the data frame, keeping what time has proven to be effective, and throwing out what is not.\ntidyr is used to tidy data, i.e. to ensure that each variable is in its own column, each observation is in its own row, and each value is in its own cell.\npurrr is used for functional programming with R.\n\n\n\n\n\n\n\n\n\nlibrary(fpp3)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.1 ──\n\n\n✔ tsibble     1.1.6     ✔ feasts      0.4.1\n✔ tsibbledata 0.4.1     ✔ fable       0.4.1\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nfpp3 is also a meta-package that load the tidyverts ecosystem for time series analysis and forecasting.\n\n\nThe tidyverts packages are made to work seamlessly with the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\ntsibble is the main data structure we will use to analyze and model time series. It is a time series tibble.\nfeasts provides many functions and tools for feature and statistics extraction for time series.\nfable is the core package for modeling and foreasting time series.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverse",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverse",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntidyverse is a meta-package that loads the core packages of the tidyverse.\n\n\nWe will always load all the required packages a the beginning of the document. When loading the tidyverse, it shows which packages are being attached, as well as any conflicts with previously loaded packages.\n\n\n\n\n\n\nNoteCore packages\n\n\n\n\n\n\ndplyr is the core package for data transformation. It is paired up with the following packages for specific column types:\n\nstringr for strings.\nforcats for factors (R’s categorical data type).\nlubridate for dates and date-times.\n\nggplot2 is the primary package for visualization.\nreadr is used to import data from delimited files (CSV, TSV, …).\ntibble is a modern reimagining of the data frame, keeping what time has proven to be effective, and throwing out what is not.\ntidyr is used to tidy data, i.e. to ensure that each variable is in its own column, each observation is in its own row, and each value is in its own cell.\npurrr is used for functional programming with R.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverts",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverts",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "library(fpp3)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.1 ──\n\n\n✔ tsibble     1.1.6     ✔ feasts      0.4.1\n✔ tsibbledata 0.4.1     ✔ fable       0.4.1\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nfpp3 is also a meta-package that load the tidyverts ecosystem for time series analysis and forecasting.\n\n\nThe tidyverts packages are made to work seamlessly with the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\ntsibble is the main data structure we will use to analyze and model time series. It is a time series tibble.\nfeasts provides many functions and tools for feature and statistics extraction for time series.\nfable is the core package for modeling and foreasting time series.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#tsibble-objects",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#tsibble-objects",
    "title": "RStudio, R, and Time Series",
    "section": "2.1 tsibble objects",
    "text": "2.1 tsibble objects\nLet’s take a look at tourism in Australia:\n\ntourism\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\n\nA tsibble is a modified version of a tibble as to\n\n\nkey_vars(tourism)\n\n[1] \"Region\"  \"State\"   \"Purpose\"\n\nkey_data(tourism)\n\n\n  \n\n\n\n\nThe tsibble has 24320 rows and 5 columns. It shows quarterly data1 on tourism across Australia. It’s divided by Region, State, and purspose of the trip2. How many different states are there?",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#australian-states",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#australian-states",
    "title": "RStudio, R, and Time Series",
    "section": "2.2 Australian States",
    "text": "2.2 Australian States\n\ndistinct(tourism, State)",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#which-regions-are-located-in-tasmania",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#which-regions-are-located-in-tasmania",
    "title": "RStudio, R, and Time Series",
    "section": "2.3 Which regions are located in Tasmania?",
    "text": "2.3 Which regions are located in Tasmania?\n\ndistinct(filter(tourism, State == \"Tasmania\"),Region)",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#data-transformation-average-trips",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#data-transformation-average-trips",
    "title": "RStudio, R, and Time Series",
    "section": "2.4 Data Transformation: Average trips",
    "text": "2.4 Data Transformation: Average trips\nTo get the average trips by purpose, we need to do the following:\n\n\nFilter the original tsibble to get only the data from East Coast, Tasmania.\nConvert the data to a tibble.\nGroup by purpose.\nSummarise by getting the mean of the trips.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#section",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#section",
    "title": "RStudio, R, and Time Series",
    "section": "2.5 ",
    "text": "2.5 \nWith traditional code, this would look something like:\n\nsummarise(group_by(as_tibble(filter(tourism, State == \"Tasmania\", \n                                    Region == \"East Coast\")), Purpose),\n          mean_trips = mean(Trips))\n\n\n\n\n\n\n\n\nNoteOrder of code execution\n\n\n\n\n\nNote that this code must be read inside-out. This makes it harder to understand, and also harder to debug.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#section-8",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#section-8",
    "title": "RStudio, R, and Time Series",
    "section": "2.6 ",
    "text": "2.6 \nUsing the native pipe operator; |&gt;, we can improve the same code:\n\n1tourism |&gt;\n2  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt;\n3  as_tibble() |&gt;\n4  group_by(Purpose) |&gt;\n5  summarise(mean_trips = mean(Trips))\n\n\n1\n\nTake the tsibble tourism, then\n\n2\n\nfilter by State and Region, then\n\n3\n\nconvert to a tibble, then\n\n4\n\ngroup the tibble by purpose, then\n\n5\n\nsummarise by taking the mean trips\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTipThe pipe operator |&gt;\n\n\n\n\n\nThe pipe is read as “then”, and it allows us to write code in the order it’s supposed to be run.\nIt also helps to debug code easier, because you can run each function in order and see where the error is.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#plotting-tourism-across-time-4",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#plotting-tourism-across-time-4",
    "title": "RStudio, R, and Time Series",
    "section": "3.1 Plotting tourism across time",
    "text": "3.1 Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n1  autoplot(Trips) +\n2  facet_wrap(vars(Purpose), scale = \"free_y\") +\n3  theme(legend.position = \"none\")\n\n\n1\n\nautoplot() detects the data automatically and proposes a plot accordingly.\n\n2\n\nfacet_wrap() Divides a plot into subplots (facets).\n\n3\n\nyou can customize endless feautres using theme(). Here, we remove the legend, as it’s redudant.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#time-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#time-plots",
    "title": "RStudio, R, and Time Series",
    "section": "3.2 Time plots",
    "text": "3.2 Time plots\n\n\n\naus_production |&gt; \n  autoplot(Gas)\n\n\n\n\n\n\n\n\n\n\naus_production |&gt; \n  autoplot(Gas) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nThese are the most basic type of plots. We have the time variable in the x-axis, and our forecast variable in the y-axis. Time plots should be line plots, and can include or not points.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-plots",
    "title": "RStudio, R, and Time Series",
    "section": "3.3 Seasonal Plots",
    "text": "3.3 Seasonal Plots\n\naus_production |&gt; \n  gg_season(Gas)\n\n\n\n\n\n\n\n\n\nThe data here are plotted against a single “season”. It’s useful in identifying years with changes in patterns.\n\nRemoving the trend from the data:",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-subseries-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-subseries-plots",
    "title": "RStudio, R, and Time Series",
    "section": "3.4 Seasonal Subseries Plots",
    "text": "3.4 Seasonal Subseries Plots\n\naus_production |&gt; \n  gg_subseries(Gas)\n\n\n\n\n\n\n\n\n\nHere we split the plot into many subplots, one for each season. This helps us see clearly the underlying seasonal pattern. The mean for each season is represented as the blue horizontal line.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#gg_tsdisplay",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#gg_tsdisplay",
    "title": "RStudio, R, and Time Series",
    "section": "3.5 gg_tsdisplay()",
    "text": "3.5 gg_tsdisplay()\n\naus_production |&gt; \n  gg_tsdisplay(Gas, plot_type = \"season\")\n\n\n\n\n\n\n\n\n\nThis function provides a convenient way to have 3 plots: a time plot, an ACF plot, and a third option that can be customized with one of the following plot types:\n\n\n\n\n\n\nTipplot_type options\n\n\n\n\n\n\n“auto”,\n“partial”,\n“season”,\n“histogram”,\n“scatter”,\n“spectrum”",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#exporting-data-to-.csv",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#exporting-data-to-.csv",
    "title": "RStudio, R, and Time Series",
    "section": "3.6 Exporting data to .csv",
    "text": "3.6 Exporting data to .csv\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  mutate(Quarter = as.Date(Quarter)) |&gt; \n  write_csv(\"./datos/tasmania.csv\")\n\n\nYou can export to .csv by providing a tsibble or tibble (or any other type of data frame), by calling write_csv(), and specifying the output file’s name.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#footnotes",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#footnotes",
    "title": "RStudio, R, and Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nshown besides the tsibble dimension as [1Q]↩︎\nthese are specified in the key argument. This tsibble contains ↩︎",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverse",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverse",
    "title": "RStudio, R, and Time Series",
    "section": "The tidyverse",
    "text": "The tidyverse\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntidyverse is a meta-package that loads the core packages of the tidyverse.\n\n\nWe will always load all the required packages a the beginning of the document. When loading the tidyverse, it shows which packages are being attached, as well as any conflicts with previously loaded packages.\n\n\n\n\n\n\nCore packages\n\n\n\ndplyr is the core package for data transformation. It is paired up with the following packages for specific column types:\n\nstringr for strings.\nforcats for factors (R’s categorical data type).\nlubridate for dates and date-times.\n\nggplot2 is the primary package for visualization.\nreadr is used to import data from delimited files (CSV, TSV, …).\ntibble is a modern reimagining of the data frame, keeping what time has proven to be effective, and throwing out what is not.\ntidyr is used to tidy data, i.e. to ensure that each variable is in its own column, each observation is in its own row, and each value is in its own cell.\npurrr is used for functional programming with R."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverts",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverts",
    "title": "RStudio, R, and Time Series",
    "section": "The tidyverts",
    "text": "The tidyverts\n\nlibrary(fpp3)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.1 ──\n\n\n✔ tsibble     1.1.6     ✔ feasts      0.4.1\n✔ tsibbledata 0.4.1     ✔ fable       0.4.1\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nfpp3 is also a meta-package that load the tidyverts ecosystem for time series analysis and forecasting.\n\n\nThe tidyverts packages are made to work seamlessly with the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\ntsibble is the main data structure we will use to analyze and model time series. It is a time series tibble.\nfeasts provides many functions and tools for feature and statistics extraction for time series.\nfable is the core package for modeling and foreasting time series."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#tsibble-objects",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#tsibble-objects",
    "title": "RStudio, R, and Time Series",
    "section": "tsibble objects",
    "text": "tsibble objects\nLet’s take a look at tourism in Australia:\n\ntourism\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\n\nA tsibble is a modified version of a tibble as to\n\n\nkey_vars(tourism)\n\n[1] \"Region\"  \"State\"   \"Purpose\"\n\nkey_data(tourism)\n\n\n  \n\n\n\n\nThe tsibble has 24320 rows and 5 columns. It shows quarterly data1 on tourism across Australia. It’s divided by Region, State, and purspose of the trip2. How many different states are there?"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#australian-states",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#australian-states",
    "title": "RStudio, R, and Time Series",
    "section": "Australian States",
    "text": "Australian States\n\ndistinct(tourism, State)"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#which-regions-are-located-in-tasmania",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#which-regions-are-located-in-tasmania",
    "title": "RStudio, R, and Time Series",
    "section": "Which regions are located in Tasmania?",
    "text": "Which regions are located in Tasmania?\n\ndistinct(filter(tourism, State == \"Tasmania\"),Region)"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#data-transformation-average-trips",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#data-transformation-average-trips",
    "title": "RStudio, R, and Time Series",
    "section": "Data Transformation: Average trips",
    "text": "Data Transformation: Average trips\nTo get the average trips by purpose, we need to do the following:\n\nFilter the original tsibble to get only the data from East Coast, Tasmania.\nConvert the data to a tibble.\nGroup by purpose.\nSummarise by getting the mean of the trips."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "With traditional code, this would look something like:\n\nsummarise(group_by(as_tibble(filter(tourism, State == \"Tasmania\", \n                                    Region == \"East Coast\")), Purpose),\n          mean_trips = mean(Trips))\n\n\n\n\n\n\n\n\nOrder of code execution\n\n\nNote that this code must be read inside-out. This makes it harder to understand, and also harder to debug."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-1",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-1",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter()"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-2",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-2",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt;"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-3",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-3",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-4",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-4",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by()"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-5",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-5",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by(Purpose) |&gt;"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-6",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-6",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by(Purpose) |&gt;              \n  summarise(mean_trips = )"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-7",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-7",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by(Purpose) |&gt;              \n  summarise(mean_trips = mean(Trips))"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-8",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-8",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\n1tourism |&gt;\n2  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt;\n3  as_tibble() |&gt;\n4  group_by(Purpose) |&gt;\n5  summarise(mean_trips = mean(Trips))\n\n\n1\n\nTake the tsibble tourism, then\n\n2\n\nfilter by State and Region, then\n\n3\n\nconvert to a tibble, then\n\n4\n\ngroup the tibble by purpose, then\n\n5\n\nsummarise by taking the mean trips\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nThe pipe operator |&gt;\n\n\nThe pipe is read as “then”, and it allows us to write code in the order it’s supposed to be run.\nIt also helps to debug code easier, because you can run each function in order and see where the error is."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-1",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-1",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\")"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-2",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-2",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  autoplot(Trips)"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-3",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-3",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  autoplot(Trips) +\n  facet_wrap(vars(Purpose), scale = \"free_y\")"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-4",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-4",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n1  autoplot(Trips) +\n2  facet_wrap(vars(Purpose), scale = \"free_y\") +\n3  theme(legend.position = \"none\")\n\n\n1\n\nautoplot() detects the data automatically and proposes a plot accordingly.\n\n2\n\nfacet_wrap() Divides a plot into subplots (facets).\n\n3\n\nyou can customize endless feautres using theme(). Here, we remove the legend, as it’s redudant."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#time-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#time-plots",
    "title": "RStudio, R, and Time Series",
    "section": "Time plots",
    "text": "Time plots\n\n\n\naus_production |&gt; \n  autoplot(Gas)\n\n\n\n\n\n\n\n\n\n\naus_production |&gt; \n  autoplot(Gas) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nThese are the most basic type of plots. We have the time variable in the x-axis, and our forecast variable in the y-axis. Time plots should be line plots, and can include or not points."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-plots",
    "title": "RStudio, R, and Time Series",
    "section": "Seasonal Plots",
    "text": "Seasonal Plots\n\naus_production |&gt; \n  gg_season(Gas)\n\n\n\n\n\n\n\n\n\nThe data here are plotted against a single “season”. It’s useful in identifying years with changes in patterns.\n\nRemoving the trend from the data:"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-subseries-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-subseries-plots",
    "title": "RStudio, R, and Time Series",
    "section": "Seasonal Subseries Plots",
    "text": "Seasonal Subseries Plots\n\naus_production |&gt; \n  gg_subseries(Gas)\n\n\n\n\n\n\n\n\n\nHere we split the plot into many subplots, one for each season. This helps us see clearly the underlying seasonal pattern. The mean for each season is represented as the blue horizontal line."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#gg_tsdisplay",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#gg_tsdisplay",
    "title": "RStudio, R, and Time Series",
    "section": "gg_tsdisplay()",
    "text": "gg_tsdisplay()\n\naus_production |&gt; \n  gg_tsdisplay(Gas, plot_type = \"season\")\n\n\n\n\n\n\n\n\n\nThis function provides a convenient way to have 3 plots: a time plot, an ACF plot, and a third option that can be customized with one of the following plot types:\n\n\n\n\n\n\nplot_type options\n\n\n\n“auto”,\n“partial”,\n“season”,\n“histogram”,\n“scatter”,\n“spectrum”"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#exporting-data-to-.csv",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#exporting-data-to-.csv",
    "title": "RStudio, R, and Time Series",
    "section": "Exporting data to .csv",
    "text": "Exporting data to .csv\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  mutate(Quarter = as.Date(Quarter)) |&gt; \n  write_csv(\"./datos/tasmania.csv\")\n\n\nYou can export to .csv by providing a tsibble or tibble (or any other type of data frame), by calling write_csv(), and specifying the output file’s name."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#footnotes",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#footnotes",
    "title": "RStudio, R, and Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nshown besides the tsibble dimension as [1Q]\nthese are specified in the key argument. This tsibble contains"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html",
    "href": "docs/modules/module_1/03_fcst/forecasting.html",
    "title": "Forecasting",
    "section": "",
    "text": "Use readr::read_csv(), readxl::read_excel(), or tidyquant::tq_get() to import the data into R. You can find more on this here.\n\n\n\nYour data should be tidy. That means:\n\nEach variable should be in its own column.\nEach observation should be in its own row.\nEach value should be in its own cell.\n\n\n\n\nData tidying and transforming are covered in detail in R for Data Science.\nTransform the resulting tibble into a tsibble:\n\nIt should have an index (time) variable with the proper time format1.\nThe key argument is only necessary if the dataset contains more than one time series.\n\n\n\n\n\nSplit the data into a training set and a test set2. The training set is used to estimate the model parameters, while the test set is used to evaluate the model’s performance on unseen data.\nThe size of the training and test sets depends on the length of the time series and the forecasting horizon:\n\nIf the forecast horizon is e. g.. 12 months, the test set should contain 12 months of data.\nAnother common approach is to use the first 70-80% of the data for training and the remaining 20-30% for testing.\n\nWe can use filter_index() to create the training set3:\n\n\ndatos_train &lt;- &lt;tsibble&gt; |&gt; \n1  filter_index(\"start_date\" ~ \"end_date\")\n\n\n1\n\nReplace start_date and end_date with the desired date range for the training set.You can also use . to indicate the start or end of the series: filter_index(. ~ \"end_date\") or filter_index(\"start_date\" ~ .).\n\n\n\n\n\n\n\n\n\n\nNoteSplitting the data\n\n\n\n\n\nIn time series, the training set should always contain the earlier observations, while the test set should contain the later observations. This is because time series data is ordered in time, and we want to simulate the real-world scenario where we use past data to predict future values.\n\n\n\n\n\n\n\nPlot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots here.\n\n\n\n\n\n\n\n\n\n\n\n\nDecide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series’ features.\nTrain the model specification on the training set. You can use the model() function to fit various forecasting models4.\n\ndatos_fit &lt;- datos_train |&gt; \n  model(\n1    model_1 = &lt;model_function_1&gt;(&lt;y_t&gt; ~ x_t),\n2    model_2 = &lt;model_function_2&gt;(&lt;transformation_function&gt;(&lt;y_t&gt;), &lt;args&gt;)\n  )\n\n\n1\n\nReplace model_function_1 with the desired forecasting method (e.g., ARIMA(), ETS(), NAIVE(), etc.). Replace &lt;y_t&gt; with the name of the forecast variable and &lt;predictor_variables&gt; with any predictor variables if applicable.\n\n2\n\nIf a transformation is needed, replace transformation_function with the appropriate function (e.g., log, box_cox, etc.) and include any specific arguments required by the model.\n\n\n\n\n\n\n\n\nFitted values, \\hat{y}_t: The values predicted by the model for the training set.\nresiduals, e_t: The difference between the actual values and the fitted values, calculated as\n\n\ne_t = y_t - \\hat{y}_t\n.\n\ninnovation residuals: Residuals on the transformed scale5.\n\nWe can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble white noise.\n\n\n\n\n\n\n\nThe fitted values and residuals can be extracted from the model table using augment().\n\n\n\n\n\n\n\n\n\n\nWe expect residuals to behave like white noise, thus having the following properties:\nThe most important:\n\nUncorrelated: There is no correlation between the values at different time points.\nZero mean: The average value of the series is constant over time (and equal to zero).\n\nNice to have:\n\nConstant variance: The variability of the series is constant over time.\nNormally distributed: The values follow a normal distribution (this is not always required).\n\n\n\n\nIf the residuals don’t meet these properties, we could refine the model:\n\nFor the first 2: add predictors or change the model structure.\nApply a variance-stabilizing transformation (e.g., Box-Cox).\nIf the residuals are not normally distributed, only the prediction intervals are affected. We can deal with this by using bootstrap prediction intervals.\n\n\n\n\n\nOnce a satisfactory model is obtained, we can proceed to forecast6. Use the forecast() function to generate forecasts for a specified horizon h.:\n\ndatos_fcst &lt;- datos_fit |&gt; \n1  forecast(h = &lt;forecast_horizon&gt;)\n\n\n1\n\nReplace &lt;forecast_horizon&gt; with the desired number of periods to forecast (e.g., 12 for 12 months ahead), or you can write in text \"1 year\" for a one-year forecast.\n\n\n\n\n\n\n\n\n\n\nNoteForecast horizon\n\n\n\nThe forecast horizon should have the same length as the test set to evaluate the model’s performance accurately.\n\n\n\n\nWe measure a forecast’s accuracy by measuring the forecast error. Forecast errors are computed as:\n\ne_{T+h} = y_{T+h} - \\hat{y}_{T+h|T}\n\n\n\n\n\n\n\n\nThese errors depend on the scale of the data. Therefore, they are not suitable for comparing forecast accuracy across series with different scales or units.\n\n\n\n\nWe can also measure errors as percentage errors7:\n\np_t = \\frac{e_{T+h}}{y_{T+h}} \\times 100\n\n\n\n\n\n\n\n\nPercentage errors can be problematic when the actual values are close to zero, leading to extremely high or undefined percentage errors.\n\n\n\n\nor scaled errors8.:\n\nFor non-seasonal time series:\n\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-1} \\sum_{t=2}^{T}\\left|y_{t}-y_{t-1}\\right|},\n\nFor seasonal time series:\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-m} \\sum_{t=m+1}^{T}\\left|y_{t}-y_{t-m}\\right|}.\n\n\n\n\nUsing this errors, we can compute various error metrics to summarize the forecast accuracy:\n\nCommon error metrics\n\n\n\n\n\n\n\n\nScale\nMetric\nDescription\nFormula\n\n\n\n\nScale-dependent\n\nRMSE\nMAE\n\n\nRoot Mean Squared Error\nMean Absolute Error\n\n\n\\sqrt{\\text{mean}(e_{t}^{2})}\n\\text{mean}(|e_{t}|)\n\n\n\nScale-independent\n\nMAPE\nMASE\nRMMSE\n\n\nMean Absolute Percentage Error\nMean Absolute Scaled Error\nRoot Mean Squared Scaled Error\n\n\n\\text{mean}(|p_{t}|)\n\\text{mean}(|q_{t}|)\n\\sqrt{\\text{mean}(q_{t}^{2})}\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a note.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#tidy-data",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#tidy-data",
    "title": "Forecasting",
    "section": "",
    "text": "Use readr::read_csv(), readxl::read_excel(), or tidyquant::tq_get() to import the data into R. You can find more on this here.\n\n\n\nYour data should be tidy. That means:\n\nEach variable should be in its own column.\nEach observation should be in its own row.\nEach value should be in its own cell.\n\n\n\n\nData tidying and transforming are covered in detail in R for Data Science.\nTransform the resulting tibble into a tsibble:\n\nIt should have an index (time) variable with the proper time format1.\nThe key argument is only necessary if the dataset contains more than one time series.\n\n\n\n\n\nSplit the data into a training set and a test set2. The training set is used to estimate the model parameters, while the test set is used to evaluate the model’s performance on unseen data.\nThe size of the training and test sets depends on the length of the time series and the forecasting horizon:\n\nIf the forecast horizon is e. g.. 12 months, the test set should contain 12 months of data.\nAnother common approach is to use the first 70-80% of the data for training and the remaining 20-30% for testing.\n\nWe can use filter_index() to create the training set3:\n\n\ndatos_train &lt;- &lt;tsibble&gt; |&gt; \n1  filter_index(\"start_date\" ~ \"end_date\")\n\n\n1\n\nReplace start_date and end_date with the desired date range for the training set.You can also use . to indicate the start or end of the series: filter_index(. ~ \"end_date\") or filter_index(\"start_date\" ~ .).\n\n\n\n\n\n\n\n\n\n\nNoteSplitting the data\n\n\n\n\n\nIn time series, the training set should always contain the earlier observations, while the test set should contain the later observations. This is because time series data is ordered in time, and we want to simulate the real-world scenario where we use past data to predict future values.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#visualize",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#visualize",
    "title": "Forecasting",
    "section": "",
    "text": "Plot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots here.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#specify-estimate",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#specify-estimate",
    "title": "Forecasting",
    "section": "",
    "text": "Decide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series’ features.\nTrain the model specification on the training set. You can use the model() function to fit various forecasting models4.\n\ndatos_fit &lt;- datos_train |&gt; \n  model(\n1    model_1 = &lt;model_function_1&gt;(&lt;y_t&gt; ~ x_t),\n2    model_2 = &lt;model_function_2&gt;(&lt;transformation_function&gt;(&lt;y_t&gt;), &lt;args&gt;)\n  )\n\n\n1\n\nReplace model_function_1 with the desired forecasting method (e.g., ARIMA(), ETS(), NAIVE(), etc.). Replace &lt;y_t&gt; with the name of the forecast variable and &lt;predictor_variables&gt; with any predictor variables if applicable.\n\n2\n\nIf a transformation is needed, replace transformation_function with the appropriate function (e.g., log, box_cox, etc.) and include any specific arguments required by the model.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#evaluate",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#evaluate",
    "title": "Forecasting",
    "section": "",
    "text": "Fitted values, \\hat{y}_t: The values predicted by the model for the training set.\nresiduals, e_t: The difference between the actual values and the fitted values, calculated as\n\n\ne_t = y_t - \\hat{y}_t\n.\n\ninnovation residuals: Residuals on the transformed scale5.\n\nWe can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble white noise.\n\n\n\n\n\n\n\nThe fitted values and residuals can be extracted from the model table using augment().\n\n\n\n\n\n\n\n\n\n\nWe expect residuals to behave like white noise, thus having the following properties:\nThe most important:\n\nUncorrelated: There is no correlation between the values at different time points.\nZero mean: The average value of the series is constant over time (and equal to zero).\n\nNice to have:\n\nConstant variance: The variability of the series is constant over time.\nNormally distributed: The values follow a normal distribution (this is not always required).\n\n\n\n\nIf the residuals don’t meet these properties, we could refine the model:\n\nFor the first 2: add predictors or change the model structure.\nApply a variance-stabilizing transformation (e.g., Box-Cox).\nIf the residuals are not normally distributed, only the prediction intervals are affected. We can deal with this by using bootstrap prediction intervals.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#forecast-1",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#forecast-1",
    "title": "Forecasting",
    "section": "",
    "text": "Once a satisfactory model is obtained, we can proceed to forecast6. Use the forecast() function to generate forecasts for a specified horizon h.:\n\ndatos_fcst &lt;- datos_fit |&gt; \n1  forecast(h = &lt;forecast_horizon&gt;)\n\n\n1\n\nReplace &lt;forecast_horizon&gt; with the desired number of periods to forecast (e.g., 12 for 12 months ahead), or you can write in text \"1 year\" for a one-year forecast.\n\n\n\n\n\n\n\n\n\n\nNoteForecast horizon\n\n\n\nThe forecast horizon should have the same length as the test set to evaluate the model’s performance accurately.\n\n\n\n\nWe measure a forecast’s accuracy by measuring the forecast error. Forecast errors are computed as:\n\ne_{T+h} = y_{T+h} - \\hat{y}_{T+h|T}\n\n\n\n\n\n\n\n\nThese errors depend on the scale of the data. Therefore, they are not suitable for comparing forecast accuracy across series with different scales or units.\n\n\n\n\nWe can also measure errors as percentage errors7:\n\np_t = \\frac{e_{T+h}}{y_{T+h}} \\times 100\n\n\n\n\n\n\n\n\nPercentage errors can be problematic when the actual values are close to zero, leading to extremely high or undefined percentage errors.\n\n\n\n\nor scaled errors8.:\n\nFor non-seasonal time series:\n\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-1} \\sum_{t=2}^{T}\\left|y_{t}-y_{t-1}\\right|},\n\nFor seasonal time series:\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-m} \\sum_{t=m+1}^{T}\\left|y_{t}-y_{t-m}\\right|}.\n\n\n\n\nUsing this errors, we can compute various error metrics to summarize the forecast accuracy:\n\nCommon error metrics\n\n\n\n\n\n\n\n\nScale\nMetric\nDescription\nFormula\n\n\n\n\nScale-dependent\n\nRMSE\nMAE\n\n\nRoot Mean Squared Error\nMean Absolute Error\n\n\n\\sqrt{\\text{mean}(e_{t}^{2})}\n\\text{mean}(|e_{t}|)\n\n\n\nScale-independent\n\nMAPE\nMASE\nRMMSE\n\n\nMean Absolute Percentage Error\nMean Absolute Scaled Error\nRoot Mean Squared Scaled Error\n\n\n\\text{mean}(|p_{t}|)\n\\text{mean}(|q_{t}|)\n\\sqrt{\\text{mean}(q_{t}^{2})}",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#communicate",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#communicate",
    "title": "Forecasting",
    "section": "",
    "text": "This is a note.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#footnotes",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#footnotes",
    "title": "Forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e., if the TS has a monthly frequency, the index variable should be in yearmonth format. Other formats coud be yearweek, yearquarter, year, date.↩︎\nSplitting the data into a training and test set is the minimum requirement for evaluating a forecasting model. If you want to avoid overfitting and get a more reliable estimate of the model’s performance, you should consider splitting the data into 3 sets: training, validation, and test sets. The validation set is used to tune model hyperparameters and select the best model, while the test set is used for the final evaluation of the selected model. For an even more robust evaluation of forecasting models, consider using time series cross-validation methods.↩︎\nand store it in a *_train object.↩︎\nand store the model table in a *_fit object.↩︎\nWe will focus on innovation residuals whenever a transformation is used in the model.↩︎\nand store the forecasts in a *_fcst object.↩︎\nPercentage errors are scale-independent, making them useful for comparing forecast accuracy across different series.↩︎\nScaled errors are also scale-independent and are useful for comparing forecast accuracy across different series.↩︎",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#tidy-data",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#tidy-data",
    "title": "Forecasting",
    "section": "Tidy data",
    "text": "Tidy data\n\nUse readr::read_csv(), readxl::read_excel(), or tidyquant::tq_get() to import the data into R. You can find more on this here.\n\n\n\nYour data should be tidy. That means:\n\nEach variable should be in its own column.\nEach observation should be in its own row.\nEach value should be in its own cell.\n\n\n\n\nData tidying and transforming are covered in detail in R for Data Science.\nTransform the resulting tibble into a tsibble:\n\nIt should have an index (time) variable with the proper time format1.\nThe key argument is only necessary if the dataset contains more than one time series."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#traintest-split",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#traintest-split",
    "title": "Forecasting",
    "section": "Train/test split",
    "text": "Train/test split\n\nSplit the data into a training set and a test set2. The training set is used to estimate the model parameters, while the test set is used to evaluate the model’s performance on unseen data.\nThe size of the training and test sets depends on the length of the time series and the forecasting horizon:\n\nIf the forecast horizon is e. g.. 12 months, the test set should contain 12 months of data.\nAnother common approach is to use the first 70-80% of the data for training and the remaining 20-30% for testing.\n\nWe can use filter_index() to create the training set3:\n\n\n\ndatos_train &lt;- &lt;tsibble&gt; |&gt; \n1  filter_index(\"start_date\" ~ \"end_date\")\n\n\n1\n\nReplace start_date and end_date with the desired date range for the training set.You can also use . to indicate the start or end of the series: filter_index(. ~ \"end_date\") or filter_index(\"start_date\" ~ .)."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#visualize",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#visualize",
    "title": "Forecasting",
    "section": "Visualize",
    "text": "Visualize\nPlot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots here."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#specify-estimate",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#specify-estimate",
    "title": "Forecasting",
    "section": "Specify & Estimate",
    "text": "Specify & Estimate\nDecide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series’ features.\nTrain the model specification on the training set. You can use the model() function to fit various forecasting models4.\n\ndatos_fit &lt;- datos_train |&gt; \n  model(\n1    model_1 = &lt;model_function_1&gt;(&lt;y_t&gt; ~ x_t),\n2    model_2 = &lt;model_function_2&gt;(&lt;transformation_function&gt;(&lt;y_t&gt;), &lt;args&gt;)\n  )\n\n\n1\n\nReplace model_function_1 with the desired forecasting method (e.g., ARIMA(), ETS(), NAIVE(), etc.). Replace &lt;y_t&gt; with the name of the forecast variable and &lt;predictor_variables&gt; with any predictor variables if applicable.\n\n2\n\nIf a transformation is needed, replace transformation_function with the appropriate function (e.g., log, box_cox, etc.) and include any specific arguments required by the model."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#evaluate",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#evaluate",
    "title": "Forecasting",
    "section": "Evaluate",
    "text": "Evaluate\n\nFitted values, \\hat{y}_t: The values predicted by the model for the training set.\nresiduals, e_t: The difference between the actual values and the fitted values, calculated as\n\n\ne_t = y_t - \\hat{y}_t\n.\n\ninnovation residuals: Residuals on the transformed scale5.\n\nWe can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble white noise.\n\n\n\n\n\n\n\nThe fitted values and residuals can be extracted from the model table using augment()."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#what-is-white-noise",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#what-is-white-noise",
    "title": "Forecasting",
    "section": "What is white noise?",
    "text": "What is white noise?"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#residual-diagnostics",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#residual-diagnostics",
    "title": "Forecasting",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\nWe expect residuals to behave like white noise, thus having the following properties:\nThe most important:\n\nUncorrelated: There is no correlation between the values at different time points.\nZero mean: The average value of the series is constant over time (and equal to zero).\n\nNice to have:\n\nConstant variance: The variability of the series is constant over time.\nNormally distributed: The values follow a normal distribution (this is not always required)."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#refine",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#refine",
    "title": "Forecasting",
    "section": "Refine",
    "text": "Refine\nIf the residuals don’t meet these properties, we could refine the model:\n\nFor the first 2: add predictors or change the model structure.\nApply a variance-stabilizing transformation (e.g., Box-Cox).\nIf the residuals are not normally distributed, only the prediction intervals are affected. We can deal with this by using bootstrap prediction intervals."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#forecast-accuracy",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#forecast-accuracy",
    "title": "Forecasting",
    "section": "Forecast accuracy",
    "text": "Forecast accuracy\nWe measure a forecast’s accuracy by measuring the forecast error. Forecast errors are computed as:\n\ne_{T+h} = y_{T+h} - \\hat{y}_{T+h|T}\n\n\n\n\n\n\n\n\nThese errors depend on the scale of the data. Therefore, they are not suitable for comparing forecast accuracy across series with different scales or units.\n\n\n\n\nWe can also measure errors as percentage errors7:\n\np_t = \\frac{e_{T+h}}{y_{T+h}} \\times 100\n\n\n\n\n\n\n\n\nPercentage errors can be problematic when the actual values are close to zero, leading to extremely high or undefined percentage errors.\n\n\n\n\nor scaled errors8.:\n\nFor non-seasonal time series:\n\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-1} \\sum_{t=2}^{T}\\left|y_{t}-y_{t-1}\\right|},\n\nFor seasonal time series:\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-m} \\sum_{t=m+1}^{T}\\left|y_{t}-y_{t-m}\\right|}."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#error-metrics",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#error-metrics",
    "title": "Forecasting",
    "section": "Error metrics",
    "text": "Error metrics\nUsing this errors, we can compute various error metrics to summarize the forecast accuracy:\n\nCommon error metrics\n\n\n\n\n\n\n\n\nScale\nMetric\nDescription\nFormula\n\n\n\n\nScale-dependent\n\nRMSE\nMAE\n\n\nRoot Mean Squared Error\nMean Absolute Error\n\n\n\\sqrt{\\text{mean}(e_{t}^{2})}\n\\text{mean}(|e_{t}|)\n\n\n\nScale-independent\n\nMAPE\nMASE\nRMMSE\n\n\nMean Absolute Percentage Error\nMean Absolute Scaled Error\nRoot Mean Squared Scaled Error\n\n\n\\text{mean}(|p_{t}|)\n\\text{mean}(|q_{t}|)\n\\sqrt{\\text{mean}(q_{t}^{2})}"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#refit-and-forecast",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#refit-and-forecast",
    "title": "Forecasting",
    "section": "Refit and forecast",
    "text": "Refit and forecast\nRefit and forecast"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#communicate",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#communicate",
    "title": "Forecasting",
    "section": "Communicate",
    "text": "Communicate\n\nThis is a note."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#footnotes",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#footnotes",
    "title": "Forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e., if the TS has a monthly frequency, the index variable should be in yearmonth format. Other formats coud be yearweek, yearquarter, year, date.\nSplitting the data into a training and test set is the minimum requirement for evaluating a forecasting model. If you want to avoid overfitting and get a more reliable estimate of the model’s performance, you should consider splitting the data into 3 sets: training, validation, and test sets. The validation set is used to tune model hyperparameters and select the best model, while the test set is used for the final evaluation of the selected model. For an even more robust evaluation of forecasting models, consider using time series cross-validation methods.\nand store it in a *_train object.\nand store the model table in a *_fit object.\nWe will focus on innovation residuals whenever a transformation is used in the model.\nand store the forecasts in a *_fcst object.\nPercentage errors are scale-independent, making them useful for comparing forecast accuracy across different series.\nScaled errors are also scale-independent and are useful for comparing forecast accuracy across different series."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html",
    "href": "docs/modules/module_2/01_ets/ets.html",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Exponential smoothing methods are still relatively simple: they’re simply weighted averages from historical data.\n\nHowever, these forecasting methods are widely used in practice, and they can be very effective.\n\nThe exponential smoothing method is a compromise between the mean and naïve methods. It uses all historical data, but it assigns exponentially decreasing weights to older observations.\n\nIn the mean method, all observations are weighted equally (all have the same importance), while in the naïve method, only the most recent observation is used for forecasting. (we ignore all previous observations).\n\nThe smoothing parameter \\alpha controls the rate of decrease:\n\nwhen \\alpha is close to 1, the method behaves like the naïve method, giving more weight to recent observations;\nwhen \\alpha is close to 0, it behaves like the mean method, giving more equal weight to all observations.\n\\hat{y}_{T+1 | T}= \\alpha y_{T} + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^{2} y_{T-2}  + \\ldots\nwhere 0\\leq \\alpha \\leq1 is the smoothing parameter.",
    "crumbs": [
      "Modules",
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#simple-exponential-smoothing-ses",
    "href": "docs/modules/module_2/01_ets/ets.html#simple-exponential-smoothing-ses",
    "title": "Exponential smoothing",
    "section": "1.1 Simple exponential smoothing (SES)",
    "text": "1.1 Simple exponential smoothing (SES)\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t \\\\\n\\text{Smoothing equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\n\\end{aligned}\n\nwhere \\ell_t is the level at time t.\n\n\n\n\n\n\nSES has a flat forecast function, so it is appropriate for data with no trend or seasonal pattern.\n\n\n\n\nalgeria_economy &lt;- global_economy |&gt;\n  filter(Country == \"Algeria\")\n\nalg_fit &lt;- algeria_economy |&gt;\n  model(\n    SES = ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    Naive = NAIVE(Exports)\n  )\n\nalg_fc &lt;- alg_fit |&gt;\n  forecast(h = 5)\n\n\n\n\n\n\n\nTipBenchmark methods for SES\n\n\n\nThe mean and naïve methods are typically the best fit as benchmark methods when using SES.\n\n\n\nalg_fit |&gt; \n  select(SES) |&gt; \n1  report()\n\n\n1\n\nThe report() function allows us to see a model’s report (the time series modeled, the model used, the estimated parameters, and more). It needs a 1 \\times 1 dimension mable1.\n\n\n\n\nSeries: Exports \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.8399875 \n\n  Initial states:\n   l[0]\n 39.539\n\n  sigma^2:  35.6301\n\n     AIC     AICc      BIC \n446.7154 447.1599 452.8968 \n\n\n\n\n\n\n\n\n\n\n\nComparing the SES and Naive forecasts:",
    "crumbs": [
      "Modules",
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#methods-with-trend-1",
    "href": "docs/modules/module_2/01_ets/ets.html#methods-with-trend-1",
    "title": "Exponential smoothing",
    "section": "1.2 Methods with trend",
    "text": "1.2 Methods with trend\n\n1.2.1 Holt’s linear trend\n\nWe can extend SES models to allow our forecasts to include trend in the data. We need to add a new smoothing parameter \\beta^*, and its corresponding smoothing equation:\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(l_t-l_{t-1}) + (1-\\beta^*)b_{t-1}\n\\end{aligned}\n\nwhere b_t is the growth (or slope) at time t.\n\n\n\n\n\n\nHolt’s linear trend method is appropriate for data with a linear trend but no seasonal pattern.\n\n\n\n\nLet’s see an example using Holt’s linear trend method to forecast Australia’s population.\n\n\naus_economy &lt;- global_economy  |&gt; \n  filter(Code == \"AUS\") |&gt; \n  mutate(Pop = Population / 1e6)\n\naus_economy |&gt; \n  autoplot(Pop)\n\n\n\n\n\n\n\nfit &lt;- aus_economy  |&gt; \n  model(AAN   = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n        Drift = RW(Pop ~ drift()))\n\nfc &lt;- fit  |&gt;  forecast(h = 10)\n\nfc |&gt; \n  autoplot(aus_economy |&gt;  filter_index(\"2000\"~.), level = NULL)\n\n\n\n\n\n\n\nfit |&gt;  \n  glance()\n\n\n  \n\n\nfit |&gt;  \n  select(AAN)  |&gt;  \n  report()\n\nSeries: Pop \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.3266366 \n\n  Initial states:\n     l[0]      b[0]\n 10.05414 0.2224818\n\n  sigma^2:  0.0041\n\n      AIC      AICc       BIC \n-76.98569 -75.83184 -66.68347",
    "crumbs": [
      "Modules",
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#footnotes",
    "href": "docs/modules/module_2/01_ets/ets.html#footnotes",
    "title": "Exponential smoothing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(i.e., a mable containing only one model and one time series.)↩︎",
    "crumbs": [
      "Modules",
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Mean\n\n\\hat y_{T+1\\mid T}=\\tfrac{1}{T}\\sum_{i=1}^T y_i\n\n\n\n\n\n\nNaïve\n\n\\hat y_{T+1\\mid T}=y_T"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-1",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-1",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Mean\n\n\n\n\n\nNaïve\n\n\nExponential Smoothing\n\n\\hat y_{T+1\\mid T}=\\alpha y_T +  \\alpha(1-\\alpha)y_{T-1} + \\ldots\n\n\n\\alpha \\approx 1: naïve-like\n\\alpha \\approx 0: mean-like\n\n\n\n\n\nExponential smoothing methods are still relatively simple: they’re simply weighted averages from historical data.\n\nHowever, these forecasting methods are widely used in practice, and they can be very effective.\n\nThe exponential smoothing method is a compromise between the mean and naïve methods. It uses all historical data, but it assigns exponentially decreasing weights to older observations.\n\nIn the mean method, all observations are weighted equally (all have the same importance), while in the naïve method, only the most recent observation is used for forecasting. (we ignore all previous observations).\n\nThe smoothing parameter \\alpha controls the rate of decrease:\n\nwhen \\alpha is close to 1, the method behaves like the naïve method, giving more weight to recent observations;\nwhen \\alpha is close to 0, it behaves like the mean method, giving more equal weight to all observations."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-2",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-2",
    "title": "Exponential smoothing",
    "section": "",
    "text": "\\hat{y}_{T+1 | T}= \\alpha y_{T} + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^{2} y_{T-2}  + \\ldots\n\nwhere 0\\leq \\alpha \\leq1 is the smoothing parameter.\n\n\n\nTable 1: Weights for different values of \\alpha\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\alpha = 0.2\n\\alpha = 0.4\n\\alpha = 0.6\n\\alpha = 0.8\n\n\n\n\ny_t\n0.2000\n0.4000\n0.6000\n0.8000\n\n\ny_{t-1}\n0.1600\n0.2400\n0.2400\n0.1600\n\n\ny_{t-2}\n0.1280\n0.1440\n0.0960\n0.0320\n\n\ny_{t-3}\n0.1024\n0.0864\n0.0384\n0.0064\n\n\ny_{t-4}\n0.0819\n0.0518\n0.0154\n0.0013\n\n\ny_{t-5}\n0.0655\n0.0311\n0.0061\n0.0003\n\n\n\n\n\n\n\n\n\\alpha can be thought of as the memory of the time series: The smaller the value of \\alpha, the longer the memory (i.e., the more past observations are taken into account).\nConversely, a larger value of \\alpha means a shorter memory, with more emphasis on recent observations. See Table 1 for some examples."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#simple-exponential-smoothing-ses",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#simple-exponential-smoothing-ses",
    "title": "Exponential smoothing",
    "section": "Simple exponential smoothing (SES)",
    "text": "Simple exponential smoothing (SES)\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t \\\\\n\\text{Smoothing equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\n\\end{aligned}\n\nwhere \\ell_t is the level at time t.\n\n\n\n\n\n\nSES has a flat forecast function, so it is appropriate for data with no trend or seasonal pattern."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-3",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-3",
    "title": "Exponential smoothing",
    "section": "",
    "text": "algeria_economy &lt;- global_economy |&gt;\n  filter(Country == \"Algeria\")\n\nalg_fit &lt;- algeria_economy |&gt;\n  model(\n    SES = ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    Naive = NAIVE(Exports)\n  )\n\nalg_fc &lt;- alg_fit |&gt;\n  forecast(h = 5)\n\n\nalg_fit |&gt; \n  select(SES) |&gt; \n1  report()\n\n\n1\n\nThe report() function allows us to see a model’s report (the time series modeled, the model used, the estimated parameters, and more). It needs a 1 \\times 1 dimension mable1.\n\n\n\n\nSeries: Exports \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.8399875 \n\n  Initial states:\n   l[0]\n 39.539\n\n  sigma^2:  35.6301\n\n     AIC     AICc      BIC \n446.7154 447.1599 452.8968"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-5",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-5",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Comparing the SES and Naive forecasts:"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#holts-linear-trend",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#holts-linear-trend",
    "title": "Exponential smoothing",
    "section": "Holt’s linear trend",
    "text": "Holt’s linear trend\n\nWe can extend SES models to allow our forecasts to include trend in the data. We need to add a new smoothing parameter \\beta^*, and its corresponding smoothing equation:\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(l_t-l_{t-1}) + (1-\\beta^*)b_{t-1}\n\\end{aligned}\n\nwhere b_t is the growth (or slope) at time t.\n\n\n\n\n\n\nHolt’s linear trend method is appropriate for data with a linear trend but no seasonal pattern.\n\n\n\n\nLet’s see an example using Holt’s linear trend method to forecast Australia’s population."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-6",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-6",
    "title": "Exponential smoothing",
    "section": "",
    "text": "aus_economy &lt;- global_economy  |&gt; \n  filter(Code == \"AUS\") |&gt; \n  mutate(Pop = Population / 1e6)\n\naus_economy |&gt; \n  autoplot(Pop)\n\n\n\n\n\n\n\nfit &lt;- aus_economy  |&gt; \n  model(AAN   = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n        Drift = RW(Pop ~ drift()))\n\nfc &lt;- fit  |&gt;  forecast(h = 10)\n\nfc |&gt; \n  autoplot(aus_economy |&gt;  filter_index(\"2000\"~.), level = NULL)\n\n\n\n\n\n\n\nfit |&gt;  \n  glance()\n\n\n  \n\n\nfit |&gt;  \n  select(AAN)  |&gt;  \n  report()\n\nSeries: Pop \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.3266366 \n\n  Initial states:\n     l[0]      b[0]\n 10.05414 0.2224818\n\n  sigma^2:  0.0041\n\n      AIC      AICc       BIC \n-76.98569 -75.83184 -66.68347"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#footnotes",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#footnotes",
    "title": "Exponential smoothing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(i.e., a mable containing only one model and one time series.)"
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "docs/more/ets.html#h2-header",
    "href": "docs/more/ets.html#h2-header",
    "title": "Exponential Smoothing",
    "section": "1.1 H2 header",
    "text": "1.1 H2 header\nthere and back again."
  },
  {
    "objectID": "docs/more/ets.html#code-animation",
    "href": "docs/more/ets.html#code-animation",
    "title": "Exponential Smoothing",
    "section": "1.2 Code animation",
    "text": "1.2 Code animation"
  },
  {
    "objectID": "docs/more/ets.html#section",
    "href": "docs/more/ets.html#section",
    "title": "Exponential Smoothing",
    "section": "1.3 ",
    "text": "1.3 \n# Fill in the spot we created for a plot\noutput$phonePlot &lt;- renderPlot({\n  # Render a barplot\n})"
  },
  {
    "objectID": "docs/more/ets.html#section-1",
    "href": "docs/more/ets.html#section-1",
    "title": "Exponential Smoothing",
    "section": "1.4 ",
    "text": "1.4 \n# Fill in the spot we created for a plot\noutput$phonePlot &lt;- renderPlot({\n  # Render a barplot\n  barplot(WorldPhones[,input$region]*1000, \n          main=input$region,\n          ylab=\"Number of Telephones\",\n          xlab=\"Year\")\n})"
  },
  {
    "objectID": "docs/more/ets_pres.html#h2-header",
    "href": "docs/more/ets_pres.html#h2-header",
    "title": "Exponential Smoothing",
    "section": "H2 header",
    "text": "H2 header\nthere and back again."
  },
  {
    "objectID": "docs/more/ets_pres.html#code-animation",
    "href": "docs/more/ets_pres.html#code-animation",
    "title": "Exponential Smoothing",
    "section": "Code animation",
    "text": "Code animation"
  },
  {
    "objectID": "docs/more/ets_pres.html#section",
    "href": "docs/more/ets_pres.html#section",
    "title": "Exponential Smoothing",
    "section": "",
    "text": "# Fill in the spot we created for a plot\noutput$phonePlot &lt;- renderPlot({\n  # Render a barplot\n})"
  },
  {
    "objectID": "docs/more/ets_pres.html#section-1",
    "href": "docs/more/ets_pres.html#section-1",
    "title": "Exponential Smoothing",
    "section": "",
    "text": "# Fill in the spot we created for a plot\noutput$phonePlot &lt;- renderPlot({\n  # Render a barplot\n  barplot(WorldPhones[,input$region]*1000, \n          main=input$region,\n          ylab=\"Number of Telephones\",\n          xlab=\"Year\")\n})"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Welcome to the Time Series Forecasting course at ITESO. Here you will find all the materials and resources needed for the course.\nIn the Modules section, you will find all the theory and practical examples to help you understand the concepts of time series forecasting. In each document you will find attached the presentations as seen in class.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/more/dummy_pres.html#first-slide",
    "href": "docs/more/dummy_pres.html#first-slide",
    "title": "presentation_dummy",
    "section": "First slide",
    "text": "First slide\nJust as a tutorial\n```{r}\n1 + 1\n```"
  },
  {
    "objectID": "docs/more/dummy_pres.html#code-with-annotations",
    "href": "docs/more/dummy_pres.html#code-with-annotations",
    "title": "presentation_dummy",
    "section": "Code with annotations",
    "text": "Code with annotations\n\n1library(tidyverse)\n2library(fpp3)\n\n3aus_production |&gt;\n  autoplot(Beer)\n\n\n1\n\nMeta-package for data-science analysis\n\n2\n\nMeta-package for time series forecasting\n\n3\n\nTime plot of Beer production"
  },
  {
    "objectID": "docs/more/dummy_file.html",
    "href": "docs/more/dummy_file.html",
    "title": "presentation_dummy",
    "section": "",
    "text": "Just as a tutorial\n```{r}\n1 + 1\n```"
  },
  {
    "objectID": "docs/more/dummy_file.html#first-slide",
    "href": "docs/more/dummy_file.html#first-slide",
    "title": "presentation_dummy",
    "section": "",
    "text": "Just as a tutorial\n```{r}\n1 + 1\n```"
  },
  {
    "objectID": "docs/more/dummy_file.html#code-with-annotations",
    "href": "docs/more/dummy_file.html#code-with-annotations",
    "title": "presentation_dummy",
    "section": "2 Code with annotations",
    "text": "2 Code with annotations\n\n1library(tidyverse)\n2library(fpp3)\n\n3aus_production |&gt;\n  autoplot(Beer)\n\n\n1\n\nMeta-package for data-science analysis\n\n2\n\nMeta-package for time series forecasting\n\n3\n\nTime plot of Beer production"
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#packages",
    "href": "docs/modules/forecasting_wf_pres.html#packages",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Packages",
    "text": "Packages\nIt is recommended to load all the packages at the beginning of your file. We will be using the tidyverts ecosystem for the whole forecasting workflow.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(plotly)\n\n\n\n\n\n\n\nWarning\n\n\nDo not load unnecesary packages into your environment. It could lead to conflicts between functions and unwanted results."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#data",
    "href": "docs/modules/forecasting_wf_pres.html#data",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Data",
    "text": "Data\nWe will work with the Real Gross Domestic Product (GDP) for Mexico. The data is downloaded from FRED. The time series id is NGDPRNSAXDCMXQ.\nImport data\n\ngdp &lt;- tidyquant::tq_get(\n  x    = \"NGDPRNSAXDCMXQ\",\n  get  = \"economic.data\",\n  from = \"1997-01-01\"\n)\n\ngdp\n\n# A tibble: 113 × 3\n   symbol         date          price\n   &lt;chr&gt;          &lt;date&gt;        &lt;dbl&gt;\n 1 NGDPRNSAXDCMXQ 1997-01-01 3702398.\n 2 NGDPRNSAXDCMXQ 1997-04-01 3896084.\n 3 NGDPRNSAXDCMXQ 1997-07-01 3906063 \n 4 NGDPRNSAXDCMXQ 1997-10-01 4038358.\n 5 NGDPRNSAXDCMXQ 1998-01-01 4084304.\n 6 NGDPRNSAXDCMXQ 1998-04-01 4134899.\n 7 NGDPRNSAXDCMXQ 1998-07-01 4138200.\n 8 NGDPRNSAXDCMXQ 1998-10-01 4146841.\n 9 NGDPRNSAXDCMXQ 1999-01-01 4176243.\n10 NGDPRNSAXDCMXQ 1999-04-01 4232280.\n# ℹ 103 more rows\n\n\nWrangle data\nThere are some issues with our data:\n\nIt is loaded into a tibble object. We need to convert it to a tsibble.\n\n\n\n\n\n\n\nTip\n\n\nWe can use as_tsibble() to do so.\n\n\n\n\nOur data is quarterly, but it is loaded in a YYYY-MM-DD format. We need to change it to a YYYY QQ format.\n\n\n\n\n\n\n\nTip\n\n\nThere are some functions that help us achieve this, such as\n\nyearquarter()\nyearmonth()\nyearweek()\nyear()\n\ndepending on the time series’ period.\n\n\n\nWe will overwrite our data:\n\ngdp &lt;- gdp |&gt; \n  mutate(date = yearquarter(date)) |&gt; \n  as_tsibble(\n    index = date,\n    key   = symbol\n  )\n\ngdp\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe always need to specify the index argument, as it is our date variable.\nThe key argument is necessary whenever we have more than one time series in our data frame and is made up of one or more columns that uniquely identify each time series ."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#traintest-split",
    "href": "docs/modules/forecasting_wf_pres.html#traintest-split",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Train/Test Split",
    "text": "Train/Test Split\nWe will split our data in two sets: a training set, and a test set, in order to evaluate our forecasts’ accuracy.\n\ngdp_train &lt;- gdp |&gt; \n  filter_index(. ~ \"2021 Q4\")\n\ngdp_train\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\nFor all our variables, it is strongly recommended to follow the same notation process, and write our code using snake_case. Here, we called our data gdp, therefore, all the following variables will be called starting with gdp_1, such as gdp_train for our training set."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#visualization-and-eda",
    "href": "docs/modules/forecasting_wf_pres.html#visualization-and-eda",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Visualization and EDA",
    "text": "Visualization and EDA\nWhen performing time series analysis/forecasting, one of the first things to do is to create a time series plot.\n\np &lt;- gdp_train |&gt; \n  autoplot(price) +\n  labs(\n    title = \"Time series plot of the Real GDP for Mexico\",\n    y = \"GDP\"\n  )\n \nggplotly(p, dynamicTicks = TRUE) |&gt; \n  rangeslider()\n\n\n\n\n\n\n\n\nOur data exhibits an upward linear trend (with some economic cycles), and strong yearly seasonality.\n\n\n\nWe will explore it further with a season plot.\n\ngdp_train |&gt; \n  gg_season(price) |&gt; \n  ggplotly()\n\n\n\n\n\nTS Decomposition\n\ngdp_train |&gt; \n  model(stl = STL(price, robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()\n\n\n\n\n\n\n\n\nThe STL decomposition shows that the variance of the seasonal component has been increasing. We could try using a log transformation to counter this.\n\n\n\n\ngdp_train |&gt; \n  autoplot(log(price)) +\n  ggtitle(\"Log of the Real GDP of Mexico\")\n\n\n\n\n\n\n\n\ngdp_train |&gt; \n  model(stl = STL(log(price) ~ season(window = \"periodic\"), robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()"
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#model-specification",
    "href": "docs/modules/forecasting_wf_pres.html#model-specification",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Model Specification",
    "text": "Model Specification\nWe will fit two models to our time series: Seasonal Naïve, and the Drift model. We will also use the log transformation.\n\ngdp_fit &lt;- gdp_train |&gt; \n  model(\n    snaive = SNAIVE(log(price)),\n    drift  = RW(log(price) ~ drift())\n  )\n\n\n\n\n\n\n\nBenchmark models\n\n\nWe have four different benchmark models that we’ll use to compare against the rest of the more complex models:\n\nMean (MEAN( &lt;.y&gt; ))\nNaïve (NAIVE( &lt;.y&gt; ))\nSeasonal Naïve (SNAIVE( &lt;.y&gt; ))\nDrift (RW( &lt;.y&gt; ~ drift()))\n\nwhere &lt;.y&gt; is just a placeholder for the variable to model.\nChoose wisely which of these to use in each case, according to the exploratory analysis performed."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#residuals-diagnostics",
    "href": "docs/modules/forecasting_wf_pres.html#residuals-diagnostics",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Residuals Diagnostics",
    "text": "Residuals Diagnostics\nVisual analysis\n\ngdp_fit |&gt; \n  select(snaive) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Seasonal Naïve Model\")\n\n\n\n\n\n\ngdp_fit |&gt; \n  select(drift) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Drift Model\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nHere we expect to see:\n\nA time series with no apparent patterns (no trend and/or seasonality), with a mean close to zero.\nIn the ACF, we’d expect no lags with significant autocorrelation.\nNormally distributed residuals.\n\n\n\n\nPortmanteau tests of autocorrelation\n\ngdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box, lag = 24, dof = 0)\n\n\n  \n\n\n\n\n\n\n\n\n\nResiduals interpretation\n\n\nBoth models produce sub optimal residuals:\n\nThe SNAIVE correctly detects the seasonality, however, its residuals are still autocorrelated. Moreover, the residuals are not normally distributed.\nThe drift model doesn’t account for the seasonality, and their distribution is a little bit skewed.\n\nHence, we will perform our forecasts using the bootstrapping method.\n\n\n\nWe can compute some error metrics on the training set using the accuracy() function:\n\ngdp_train_accu &lt;- accuracy(gdp_fit) |&gt; \n  arrange(MAPE)\ngdp_train_accu |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE)\n\n\n  \n\n\n\n\n\n\n\n\n\nThe accuracy() function\n\n\nThe accuracy() function can be used to compute error metrics in the training data, or in the test set. What differs is the data that is given to it:\n\nFor the training metrics, you need to use the mable (the table of models, that we usually store in _fit).\nFor the forecasting error metrics, we need the fable (the forecasts table, usually stored as _fc or _fcst), and the complete set of data (both the training and test set together).\n\n\n\n\n\n\n\n\n\n\n\nFor this analysis, we are focusing on the MAPE2 metric. The drift model (2.47%) seems to have a better fit with the training set than the snaive model (3.24%)."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#modeling-using-decomposition",
    "href": "docs/modules/forecasting_wf_pres.html#modeling-using-decomposition",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Modeling using decomposition",
    "text": "Modeling using decomposition\nWe will perform a forecast using decomposition, to see if we can improve our results so far.\n\ngdp_fit_dcmp &lt;- gdp_train |&gt; \n      model(\n        stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n      )\n\ngdp_fit_dcmp\n\n\n  \n\n\n\n\n\n\n\n\n\nNote on decomposition_model()\n\n\nRemember, when using decomposition models, we need to do the following:\n\nSpecify what type of decomposition we want to use and customize it as needed.\nFit a model for the seasonally adjusted data; season_adjust.\nFit a model for the seasonal component. R uses a SNAIVE() model by default to model the seasonality. If you wish to model it using a different model, you have specify it.\n\n\nThe name of the seasonal component depends on the type of seasonality present in the time series. If it has a yearly seasonality, the component is called season_year. It could also be called season_week, season_day, and so on.\n\n\n\n\nWe can join this new model with the models we trained before. This way we can have them all in the same mable.\n\ngdp_fit &lt;- gdp_fit |&gt; \n  left_join(gdp_fit_dcmp)\n\nResiduals diagnostics\n\ngdp_fit |&gt; \n  accuracy() |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)\n\n\n  \n\n\n\n\ngdp_fit |&gt; \n  select(stlf) |&gt; \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\ngdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box)\n\n\n  \n\n\n\n\n\n\nThe MAPE seems to improve with this decomposition model. Also, the residual diagnostics do not show any seasonality present in them. However, the residuals are still autocorrelated, as the Ljung-Box test suggests."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#forecasting-on-the-test-set",
    "href": "docs/modules/forecasting_wf_pres.html#forecasting-on-the-test-set",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Forecasting on the test set",
    "text": "Forecasting on the test set\nOnce we have our models, we can produce forecasts. We will forecast our test data and check our forecasts’ performance.\n\ngdp_fc &lt;- gdp_fit |&gt; \n  forecast(h = gdp_h_fc) \n\ngdp_fc\n\n\n  \n\n\n\n\ngdp_fc |&gt; \n  autoplot(gdp) +\n  facet_wrap(~.model, ncol = 1)\n\n\n\n\n\n\ngdp_fc |&gt; \n  filter(.model == \"stlf\") |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\nWe now estimate the forecast errors:\n\ngdp_fc |&gt; \n  accuracy(gdp) |&gt; \n  select(.model:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)"
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#forecasting-the-future",
    "href": "docs/modules/forecasting_wf_pres.html#forecasting-the-future",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Forecasting the future",
    "text": "Forecasting the future\nWe now refit our model using the whole dataset. We will only model the STL decomposition model, because the other two didn’t get a strong fit.\n\ngdp_fit2 &lt;- gdp |&gt; \n  model(\n    stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n  )\ngdp_fit2\n\n\n  \n\n\n\n\ngdp_fc_fut &lt;- gdp_fit2 |&gt; \n  forecast(h = gdp_h_fc)\ngdp_fc_fut\n\n\n  \n\n\ngdp_fc_fut |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\n\n# save(gdp_fc_fut, file = \"equipo1.RData\")"
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#footnotes",
    "href": "docs/modules/forecasting_wf_pres.html#footnotes",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will make it very convenient when calling your variables. RStudio will display all the options starting with gdp_. We will usually use the following suffixes:\n\n\n_train: training set\n\n_fit: the mable (table of models)\n\n_aug: the augmented table with fitted values and residuals\n\n_dcmp: for the dable (decomposition table), containing the components and the seasonally adjusted series of a TS decomposition.\n\n_fc or _fcst: for the fable (forecasts table) that has our forecasts. \n\n\n\n\n\nThe Mean Absolute Percentage Error is a percentage error metric widely used in professional environments.\nLet\n\ne_t = y_t - \\hat{y}_t\n\nbe the error or residual.\nThen the MAPE would be computed as\n\nMAPE = \\frac{1}{T}\\sum_{t=1}^T|\\frac{e_t}{y_t}|\n."
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#packages",
    "href": "docs/modules/forecasting_workflow.html#packages",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n0.1 Packages",
    "text": "0.1 Packages\nIt is recommended to load all the packages at the beginning of your file. We will be using the tidyverts ecosystem for the whole forecasting workflow.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(plotly)\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not load unnecesary packages into your environment. It could lead to conflicts between functions and unwanted results.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#data",
    "href": "docs/modules/forecasting_workflow.html#data",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.1 Data",
    "text": "1.1 Data\nWe will work with the Real Gross Domestic Product (GDP) for Mexico. The data is downloaded from FRED. The time series id is NGDPRNSAXDCMXQ.\n\n1.1.1 Import data\n\ngdp &lt;- tidyquant::tq_get(\n  x    = \"NGDPRNSAXDCMXQ\",\n  get  = \"economic.data\",\n  from = \"1997-01-01\"\n)\n\ngdp\n\n# A tibble: 113 × 3\n   symbol         date          price\n   &lt;chr&gt;          &lt;date&gt;        &lt;dbl&gt;\n 1 NGDPRNSAXDCMXQ 1997-01-01 3702398.\n 2 NGDPRNSAXDCMXQ 1997-04-01 3896084.\n 3 NGDPRNSAXDCMXQ 1997-07-01 3906063 \n 4 NGDPRNSAXDCMXQ 1997-10-01 4038358.\n 5 NGDPRNSAXDCMXQ 1998-01-01 4084304.\n 6 NGDPRNSAXDCMXQ 1998-04-01 4134899.\n 7 NGDPRNSAXDCMXQ 1998-07-01 4138200.\n 8 NGDPRNSAXDCMXQ 1998-10-01 4146841.\n 9 NGDPRNSAXDCMXQ 1999-01-01 4176243.\n10 NGDPRNSAXDCMXQ 1999-04-01 4232280.\n# ℹ 103 more rows\n\n\n\n1.1.2 Wrangle data\nThere are some issues with our data:\n\nIt is loaded into a tibble object. We need to convert it to a tsibble.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWe can use as_tsibble() to do so.\n\n\n\n\nOur data is quarterly, but it is loaded in a YYYY-MM-DD format. We need to change it to a YYYY QQ format.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are some functions that help us achieve this, such as\n\nyearquarter()\nyearmonth()\nyearweek()\nyear()\n\ndepending on the time series’ period.\n\n\n\nWe will overwrite our data:\n\ngdp &lt;- gdp |&gt; \n  mutate(date = yearquarter(date)) |&gt; \n  as_tsibble(\n    index = date,\n    key   = symbol\n  )\n\ngdp\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nWe always need to specify the index argument, as it is our date variable.\nThe key argument is necessary whenever we have more than one time series in our data frame and is made up of one or more columns that uniquely identify each time series .",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#traintest-split",
    "href": "docs/modules/forecasting_workflow.html#traintest-split",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.2 Train/Test Split",
    "text": "1.2 Train/Test Split\nWe will split our data in two sets: a training set, and a test set, in order to evaluate our forecasts’ accuracy.\n\ngdp_train &lt;- gdp |&gt; \n  filter_index(. ~ \"2021 Q4\")\n\ngdp_train\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor all our variables, it is strongly recommended to follow the same notation process, and write our code using snake_case. Here, we called our data gdp, therefore, all the following variables will be called starting with gdp_1, such as gdp_train for our training set.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#visualization-and-eda",
    "href": "docs/modules/forecasting_workflow.html#visualization-and-eda",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.3 Visualization and EDA",
    "text": "1.3 Visualization and EDA\nWhen performing time series analysis/forecasting, one of the first things to do is to create a time series plot.\n\np &lt;- gdp_train |&gt; \n  autoplot(price) +\n  labs(\n    title = \"Time series plot of the Real GDP for Mexico\",\n    y = \"GDP\"\n  )\n \nggplotly(p, dynamicTicks = TRUE) |&gt; \n  rangeslider()\n\n\n\n\n\n\n\n\n\n\n\nOur data exhibits an upward linear trend (with some economic cycles), and strong yearly seasonality.\n\n\n\nWe will explore it further with a season plot.\n\ngdp_train |&gt; \n  gg_season(price) |&gt; \n  ggplotly()\n\n\n\n\n\n\n1.3.1 TS Decomposition\n\ngdp_train |&gt; \n  model(stl = STL(price, robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()\n\n\n\n\n\n\n\n\n\n\n\nThe STL decomposition shows that the variance of the seasonal component has been increasing. We could try using a log transformation to counter this.\n\n\n\n\ngdp_train |&gt; \n  autoplot(log(price)) +\n  ggtitle(\"Log of the Real GDP of Mexico\")\n\n\n\n\n\n\n\n\ngdp_train |&gt; \n  model(stl = STL(log(price) ~ season(window = \"periodic\"), robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#model-specification",
    "href": "docs/modules/forecasting_workflow.html#model-specification",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.4 Model Specification",
    "text": "1.4 Model Specification\nWe will fit two models to our time series: Seasonal Naïve, and the Drift model. We will also use the log transformation.\n\ngdp_fit &lt;- gdp_train |&gt; \n  model(\n    snaive = SNAIVE(log(price)),\n    drift  = RW(log(price) ~ drift())\n  )\n\n\n\n\n\n\n\nTipBenchmark models\n\n\n\n\n\nWe have four different benchmark models that we’ll use to compare against the rest of the more complex models:\n\nMean (MEAN( &lt;.y&gt; ))\nNaïve (NAIVE( &lt;.y&gt; ))\nSeasonal Naïve (SNAIVE( &lt;.y&gt; ))\nDrift (RW( &lt;.y&gt; ~ drift()))\n\nwhere &lt;.y&gt; is just a placeholder for the variable to model.\nChoose wisely which of these to use in each case, according to the exploratory analysis performed.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#residuals-diagnostics",
    "href": "docs/modules/forecasting_workflow.html#residuals-diagnostics",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.5 Residuals Diagnostics",
    "text": "1.5 Residuals Diagnostics\n\n1.5.1 Visual analysis\n\ngdp_fit |&gt; \n  select(snaive) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Seasonal Naïve Model\")\n\n\n\n\n\n\ngdp_fit |&gt; \n  select(drift) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Drift Model\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere we expect to see:\n\nA time series with no apparent patterns (no trend and/or seasonality), with a mean close to zero.\nIn the ACF, we’d expect no lags with significant autocorrelation.\nNormally distributed residuals.\n\n\n\n\n\n1.5.2 Portmanteau tests of autocorrelation\n\ngdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box, lag = 24, dof = 0)\n\n\n  \n\n\n\n\n\n\n\n\n\nCautionResiduals interpretation\n\n\n\nBoth models produce sub optimal residuals:\n\nThe SNAIVE correctly detects the seasonality, however, its residuals are still autocorrelated. Moreover, the residuals are not normally distributed.\nThe drift model doesn’t account for the seasonality, and their distribution is a little bit skewed.\n\nHence, we will perform our forecasts using the bootstrapping method.\n\n\nWe can compute some error metrics on the training set using the accuracy() function:\n\ngdp_train_accu &lt;- accuracy(gdp_fit) |&gt; \n  arrange(MAPE)\ngdp_train_accu |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE)\n\n\n  \n\n\n\n\n\n\n\n\n\nTipThe accuracy() function\n\n\n\n\n\nThe accuracy() function can be used to compute error metrics in the training data, or in the test set. What differs is the data that is given to it:\n\nFor the training metrics, you need to use the mable (the table of models, that we usually store in _fit).\nFor the forecasting error metrics, we need the fable (the forecasts table, usually stored as _fc or _fcst), and the complete set of data (both the training and test set together).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor this analysis, we are focusing on the MAPE2 metric. The drift model (2.47%) seems to have a better fit with the training set than the snaive model (3.24%).",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#modeling-using-decomposition",
    "href": "docs/modules/forecasting_workflow.html#modeling-using-decomposition",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.6 Modeling using decomposition",
    "text": "1.6 Modeling using decomposition\nWe will perform a forecast using decomposition, to see if we can improve our results so far.\n\ngdp_fit_dcmp &lt;- gdp_train |&gt; \n      model(\n        stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n      )\n\ngdp_fit_dcmp\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteNote on decomposition_model()\n\n\n\n\n\nRemember, when using decomposition models, we need to do the following:\n\nSpecify what type of decomposition we want to use and customize it as needed.\nFit a model for the seasonally adjusted data; season_adjust.\nFit a model for the seasonal component. R uses a SNAIVE() model by default to model the seasonality. If you wish to model it using a different model, you have specify it.\n\n\nThe name of the seasonal component depends on the type of seasonality present in the time series. If it has a yearly seasonality, the component is called season_year. It could also be called season_week, season_day, and so on.\n\n\n\n\nWe can join this new model with the models we trained before. This way we can have them all in the same mable.\n\ngdp_fit &lt;- gdp_fit |&gt; \n  left_join(gdp_fit_dcmp)\n\nJoining with `by = join_by(symbol)`\n\n\n\n1.6.1 Residuals diagnostics\n\ngdp_fit |&gt; \n  accuracy() |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)\n\n\n  \n\n\n\n\ngdp_fit |&gt; \n  select(stlf) |&gt; \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\ngdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box)\n\n\n  \n\n\n\n\n\n\n\n\n\nThe MAPE seems to improve with this decomposition model. Also, the residual diagnostics do not show any seasonality present in them. However, the residuals are still autocorrelated, as the Ljung-Box test suggests.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#forecasting-on-the-test-set",
    "href": "docs/modules/forecasting_workflow.html#forecasting-on-the-test-set",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.7 Forecasting on the test set",
    "text": "1.7 Forecasting on the test set\nOnce we have our models, we can produce forecasts. We will forecast our test data and check our forecasts’ performance.\n\ngdp_fc &lt;- gdp_fit |&gt; \n  forecast(h = gdp_h_fc) \n\ngdp_fc\n\n\n  \n\n\n\n\ngdp_fc |&gt; \n  autoplot(gdp) +\n  facet_wrap(~.model, ncol = 1)\n\n\n\n\n\n\ngdp_fc |&gt; \n  filter(.model == \"stlf\") |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\nWe now estimate the forecast errors:\n\ngdp_fc |&gt; \n  accuracy(gdp) |&gt; \n  select(.model:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#forecasting-the-future",
    "href": "docs/modules/forecasting_workflow.html#forecasting-the-future",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.8 Forecasting the future",
    "text": "1.8 Forecasting the future\nWe now refit our model using the whole dataset. We will only model the STL decomposition model, because the other two didn’t get a strong fit.\n\ngdp_fit2 &lt;- gdp |&gt; \n  model(\n    stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n  )\ngdp_fit2\n\n\n  \n\n\n\n\ngdp_fc_fut &lt;- gdp_fit2 |&gt; \n  forecast(h = gdp_h_fc)\ngdp_fc_fut\n\n\n  \n\n\ngdp_fc_fut |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\n\n# save(gdp_fc_fut, file = \"equipo1.RData\")",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#footnotes",
    "href": "docs/modules/forecasting_workflow.html#footnotes",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will make it very convenient when calling your variables. RStudio will display all the options starting with gdp_. We will usually use the following suffixes:\n\n\n_train: training set\n\n_fit: the mable (table of models)\n\n_aug: the augmented table with fitted values and residuals\n\n_dcmp: for the dable (decomposition table), containing the components and the seasonally adjusted series of a TS decomposition.\n\n_fc or _fcst: for the fable (forecasts table) that has our forecasts. \n\n\n↩︎\n\n\nThe Mean Absolute Percentage Error is a percentage error metric widely used in professional environments.\nLet\n\ne_t = y_t - \\hat{y}_t\n\nbe the error or residual.\nThen the MAPE would be computed as\n\nMAPE = \\frac{1}{T}\\sum_{t=1}^T|\\frac{e_t}{y_t}|\n.\n\n\n\n\n↩︎",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#is-this-a-time-series",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#is-this-a-time-series",
    "title": "Time Series Forecasting",
    "section": "Is this a time series?",
    "text": "Is this a time series?\n\n\n\n\n\n\nIf we focus solely on the regular plot, we wouldn’t have any time series. However, when we map each variable through time, we now have multiple time series: one for each country regarding life exp., GDP per capita, and population."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#stocks",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#stocks",
    "title": "Time Series Forecasting",
    "section": "Stocks",
    "text": "Stocks\n\n\n\n\n\n\n\n\n\n\n\nStocks, FX, … are all time series"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#cryptos",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#cryptos",
    "title": "Time Series Forecasting",
    "section": "Cryptos",
    "text": "Cryptos\nAny variable that is measured through time is a time series."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-1",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-1",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)\n    A--&gt;B(Those who can't predict the future)"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-2",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-2",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)\n    A--&gt;B(Those who can't predict the future)\n    A--&gt;C(Those who don't know that they can't predict the future)\n\n\n\n\n\n\nNo one, except for sorcerers and wizards, can predict the future."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-4",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-4",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Dr. Strange didn’t have the Time stone. He was using a high-tech gamer PC to run millions of simulations."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#eclipses",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#eclipses",
    "title": "Time Series Forecasting",
    "section": "Eclipses",
    "text": "Eclipses\nWe can predict eclipses with complete certainty."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-5",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-5",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "It’s not so easy to predict stock prices\n\n\n\nOther variables can’t be predicted that easily. What does it depend on?"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#electricity-demand",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#electricity-demand",
    "title": "Time Series Forecasting",
    "section": "Electricity Demand",
    "text": "Electricity Demand"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#beer-production-forecasts",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#beer-production-forecasts",
    "title": "Time Series Forecasting",
    "section": "Beer Production Forecasts",
    "text": "Beer Production Forecasts\n\n\n\n\n\n\nCan you observe any strange patterns?"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#which-us-employment-forecast-works-best",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#which-us-employment-forecast-works-best",
    "title": "Time Series Forecasting",
    "section": "Which US Employment forecast works best?",
    "text": "Which US Employment forecast works best?\n\n\n\n\n\n\nForecasting US Retail Employment using the Drift method\n\n\n\n\n\n\n\n\nForecasting US Retail Employment using the Seasonal Naive method"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-7",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-7",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Forecasting US Retail Employment using ARIMA"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html",
    "href": "docs/modules/module_1/00_intro/intro.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "If we focus solely on the regular plot, we wouldn’t have any time series. However, when we map each variable through time, we now have multiple time series: one for each country regarding life exp., GDP per capita, and population.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#is-this-a-time-series",
    "href": "docs/modules/module_1/00_intro/intro.html#is-this-a-time-series",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "If we focus solely on the regular plot, we wouldn’t have any time series. However, when we map each variable through time, we now have multiple time series: one for each country regarding life exp., GDP per capita, and population.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#stocks",
    "href": "docs/modules/module_1/00_intro/intro.html#stocks",
    "title": "Time Series Forecasting",
    "section": "Stocks",
    "text": "Stocks\n\n\n\n\n\n\n\n\n\n\n\nStocks, FX, … are all time series",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#cryptos",
    "href": "docs/modules/module_1/00_intro/intro.html#cryptos",
    "title": "Time Series Forecasting",
    "section": "Cryptos",
    "text": "Cryptos\n\n\nCrypto currencies are also time series\n\n\nAny variable that is measured through time is a time series.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-2",
    "href": "docs/modules/module_1/00_intro/intro.html#section-2",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)\n    A--&gt;B(Those who can't predict the future)\n    A--&gt;C(Those who don't know that they can't predict the future)\n\n\n\n\n\n\n\nNo one, except for sorcerers and wizards, can predict the future.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-3",
    "href": "docs/modules/module_1/00_intro/intro.html#section-3",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "What was Dr. Strange doing here?",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-4",
    "href": "docs/modules/module_1/00_intro/intro.html#section-4",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Dr. Strange didn’t have the Time stone. He was using a high-tech gamer PC to run millions of simulations.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#eclipses",
    "href": "docs/modules/module_1/00_intro/intro.html#eclipses",
    "title": "Time Series Forecasting",
    "section": "Eclipses",
    "text": "Eclipses\n\n\nWe can predict eclipses with complete certainty.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-5",
    "href": "docs/modules/module_1/00_intro/intro.html#section-5",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "It’s not so easy to predict stock prices\n\n\n\n\nOther variables can’t be predicted that easily. What does it depend on?",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#electricity-demand",
    "href": "docs/modules/module_1/00_intro/intro.html#electricity-demand",
    "title": "Time Series Forecasting",
    "section": "Electricity Demand",
    "text": "Electricity Demand",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#beer-production-forecasts",
    "href": "docs/modules/module_1/00_intro/intro.html#beer-production-forecasts",
    "title": "Time Series Forecasting",
    "section": "Beer Production Forecasts",
    "text": "Beer Production Forecasts\n\n\n\n\n\n\nCan you observe any strange patterns?",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#which-us-employment-forecast-works-best",
    "href": "docs/modules/module_1/00_intro/intro.html#which-us-employment-forecast-works-best",
    "title": "Time Series Forecasting",
    "section": "Which US Employment forecast works best?",
    "text": "Which US Employment forecast works best?\n\n\n\n\n\n\nForecasting US Retail Employment using the Drift method\n\n\n\n\n\n\n\n\nForecasting US Retail Employment using the Seasonal Naive method",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-7",
    "href": "docs/modules/module_1/00_intro/intro.html#section-7",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Forecasting US Retail Employment using ARIMA",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section",
    "title": "Time Series Decomposition",
    "section": "",
    "text": "All these time series have different shapes, patterns, and so on. When modeling them, we need to take these characteristics into account. We seek to understand the underlying patterns in the data to make better forecasts."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#ts-patterns",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#ts-patterns",
    "title": "Time Series Decomposition",
    "section": "TS Patterns",
    "text": "TS Patterns\nTime series can have distinct patterns:\n\nTrend: A long-term increase/decrease in the data.\nSeasonal: Fluctuations in the time series with a fixed and known period1.\nCycles: More commonly known as “Business cycles”, refer to rises and falls that are not of a fixed frequency2.\nChanges in variability: Changes in the spread of the data over time, i. e., an increase/decrease in the variance as the level of the series increases/decreases."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#components-of-a-time-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#components-of-a-time-series",
    "title": "Time Series Decomposition",
    "section": "Components of a Time Series",
    "text": "Components of a Time Series\nA time series can be decomposed into the following components:\n\nSeasonal component (S): The repeating short-term cycle in the series.\nTrend-cycle component (T): The long-term progression of the series.\nResidual component (R): The residuals or “noise” left after removing the seasonal and trend-cycle components."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#log-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#log-transformations",
    "title": "Time Series Decomposition",
    "section": "Log transformations",
    "text": "Log transformations\n\nSeries in levelsLog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations and adjustments help us simplify the patterns in our data, and can improve our forecasts’ accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog transformations are often useful when the data presents an increasing/decreasing variation with the level of the series.\nLog transformations are very interpretable: changes in a log value are percent changes on the original scale."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#box-cox-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#box-cox-transformations",
    "title": "Time Series Decomposition",
    "section": "Box-Cox transformations",
    "text": "Box-Cox transformations\n\nw_t= \\begin{cases}\\log \\left(y_t\\right) & \\text { if } \\lambda=0 \\\\ \\left(\\operatorname{sign}\\left(y_t\\right)\\left|y_t\\right|^\\lambda-1\\right) / \\lambda & \\text { otherwise }\\end{cases}\n\n\nIn a Box-Cox transformation, the log is always a natural logarithm. The other case is just a power transformation with scaling.\n\nWhat happens when \\lambda = 1?\n\n\n\n\n\n\nYou should choose a value of \\lambda that makes the size of the seasonal variation the same throughout the series."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#how-can-we-choose-the-value-of-lambda",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#how-can-we-choose-the-value-of-lambda",
    "title": "Time Series Decomposition",
    "section": "How can we choose the value of \\lambda?",
    "text": "How can we choose the value of \\lambda?\nHow can we choose the value of \\lambda?\nWe can use the guerrero feature to choose an optimal lambda.\n\naus_production |&gt; \n  features(Gas, features = guerrero)\n\n# A tibble: 1 × 1\n  lambda_guerrero\n            &lt;dbl&gt;\n1           0.110"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#calendar-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#calendar-adjustments",
    "title": "Time Series Decomposition",
    "section": "Calendar adjustments",
    "text": "Calendar adjustments\n\nClosing price and volumeMonthly aggregationMonthly total and mean and volume\n\n\n\n\n\n\n\n\n\n\n\ngoogle_month &lt;- google |&gt; \n  index_by(month = yearmonth(date)) |&gt; \n  summarise(\n    trading_days = n(),\n    monthly_volume = sum(volume),\n    mean_volume = mean(volume)\n  )\n\ngoogle_month\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of trading days in a month can vary due to weekends and holidays, and not because of any economic reason.\nUsing the monthly total volume can be misleading, as months with more trading days will naturally have higher total volumes.\nUsing the mean volume per trading day helps to standardize the data, making it easier to compare across months."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#population-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#population-adjustments",
    "title": "Time Series Decomposition",
    "section": "Population adjustments",
    "text": "Population adjustments\n\n\n\n\n\n\nIs the Mexican economy really that similar Australia’s economy? Is Iceland’s economy really that small?\n\n\nA greater GDP can be interpreted as having a larger economy, and a better life standard, but this is not always the case.\nComparing GDP across countries with different population sizes can be misleading.\nGDP is often used to measure the economic performance of a country, but it doesn’t account for population size.\nThe higher the population, the higher the GDP tends to be, simply because there are more people contributing to the economy.\nA more meaningful comparison can be made by looking at GDP per capita, which divides the GDP by the population size."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section-1",
    "title": "Time Series Decomposition",
    "section": "",
    "text": "PopulationGDP per capita\n\n\n\n\n\n\n\n\nThe population sizes of these countries are very different.\n\n\n\n\n\n\n\n\n\n\nGDP per capita provides a more accurate representation of the economic well-being of individuals in a country.\nIt is clear now that Iceland and Australia have a much higher GDP per capita compared to Mexico, indicating a higher standard of living for its residents."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustments",
    "title": "Time Series Decomposition",
    "section": "Inflation adjustments",
    "text": "Inflation adjustments\n\nInflation is the rate at which the general level of prices for goods and services is rising, and subsequently, purchasing power is falling.\nTo make meaningful comparisons of economic data over time, it is essential to adjust for inflation.\nThis adjustment is typically done using a price index, such as the Consumer Price Index (CPI). In Mexico, the National Consumer Price Index (INPC) is used. INEGI provides this data."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-formula",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-formula",
    "title": "Time Series Decomposition",
    "section": "Inflation adjustment formula",
    "text": "Inflation adjustment formula\nInflation adjustment formula\n\nx_t = \\frac{y_t}{z_t} * z_{2010}\n\n\nwhere:\n\ny_t is the original value at time t (nominal value).\nz_t is the price index at time t (e.g., INPC).\nz_{2010} is the price index in the base year (2010 in this case).\nx_t is the inflation-adjusted value at time t (real value)."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-example",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-example",
    "title": "Time Series Decomposition",
    "section": "Inflation adjustment example",
    "text": "Inflation adjustment example\nInflation adjustment example\n\nNominal valuesReal valuesNominal vs. Real values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naus_economy &lt;- global_economy |&gt;\n  filter(Code == \"AUS\")\n\n\nprint_retail &lt;- print_retail |&gt; \n  left_join(aus_economy, by = \"Year\") |&gt;\n  mutate(Adjusted_turnover = Turnover / CPI)"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#types-of-decompositions",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#types-of-decompositions",
    "title": "Time Series Decomposition",
    "section": "Types of Decompositions",
    "text": "Types of Decompositions\n\nA decomposition splits the time series into its underlying components:\n\nTrend-cycle\nSeasonal pattern(s)\n\nAnd what’s left of it we simply call it a “remainder component”.\nIn general, there are two types of decompositions:\n\nAdditive decomposition\n\ny_t = T_t + S_t + R_t\n\nMultiplicative decomposition\n\ny_t = T_t \\times S_t \\times R_t \\\\\n\n\nWhich one should you use?\n\n\n\nIf the seasonal variation is roughly constant over time, use an additive decomposition.\nIf the seasonal variation increases or decreases with the level of the series, use a multiplicative decomposition.\nIf you’re unsure, you can try both and see which one provides a better fit.\n\nA multiplicative decomposition is equivalent to an additive decomposition of the log-transformed series:\n\ny_t = T_t \\times S_t \\times R_t\n\nis equivalent to\n\n\\log(y_t) = \\log(T_t) + \\log(S_t) + \\log(R_t)"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#seasonally-adjusted-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#seasonally-adjusted-series",
    "title": "Time Series Decomposition",
    "section": "Seasonally adjusted series",
    "text": "Seasonally adjusted series\n\nOne use of decomposition is to obtain a seasonally adjusted series, which is the original series with the seasonal component removed.\nSeasonally adjusted series can be useful for: - Identifying and analyzing the trend-cycle component without the influence of seasonal fluctuations. - Making comparisons across different time periods without seasonal effects.\n\n\n\n\n\n\n\n\nFor an additive decomposition, the seasonally adjusted series is given by: \ny_t - S_t\n\nFor a multiplicative decomposition, the seasonally adjusted series is given by: \n\\frac{y_t}{S_t}"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "Classical decomposition",
    "text": "Classical decomposition\nIn a classical decomposition, the trend-cycle component is estimated using a moving average. Then, the seasonal component is estimated by averaging the detrended values for each season. Finally, the remainder component is obtained by subtracting the trend-cycle and seasonal components from the original series.\nAn m order moving average is given by:\n\n\\hat{T}_{t}=\\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}\n\nwhere k = (m-1)/23."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-1",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model()"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-2",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-2",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model(\n    classical = classical_decomposition() \n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-3",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-3",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model(\n    classical = classical_decomposition(y, type = \"additive\") \n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-4",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-4",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model(\n    classical = classical_decomposition(y, type = \"additive\") \n  ) |&gt; \n  components()"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-5",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-5",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-6",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-6",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\nExample of a classical decomposition\n\n1mexretail_dcmp &lt;- mexretail |&gt;\n2  model(\n3    classical = classical_decomposition(y, type = \"additive\")\n  ) |&gt; \n4  components()\n\n5mexretail_dcmp\n\n\n1\n\nWe start with our original tsibble.\n\n2\n\nInside the model() function, we specify the type of models we want to use.\n\n3\n\nIn any model used, the first thing we need to specify is our forecast variable. Then, depending on the model used, we can specify additional parameters. The model() function yields a mable4, which is a table that contains the fitted models for each time series in the tsibble.\n\n4\n\nThe components() function is used to extract the components of the decomposition (trend-cycle, seasonal, and remainder) from the fitted models in the mable. It also provides the seasonally adjusted series.\n\n5\n\nFinally, we store the result."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-8",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-8",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail_dcmp |&gt; \n  autoplot()"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#problems-of-using-a-classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#problems-of-using-a-classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "Problems of using a Classical decomposition",
    "text": "Problems of using a Classical decomposition\nProblems of using a Classical decomposition\n\nThe trend-cycle component is not estimated at the beginning and end of the series. This can be problematic if you want to forecast the series.\nIt also tends to over-smooth rises and falls.\nIt assumes that the seasonal component is constant over time, which may not be the case in many real-world scenarios.\nIt is not robust to outliers, which can significantly affect the estimates of the components.\n\n\n\n\n\n\n\n\nWarning\n\n\nIt is not recommended to use classical decomposition for forecasting because of these issues."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-decomposition-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-decomposition-1",
    "title": "Time Series Decomposition",
    "section": "STL decomposition",
    "text": "STL decomposition\n\nSTL (Seasonal and Trend decomposition using Loess) is a more advanced method for decomposing time series data5. It uses locally weighted regression (loess) to estimate the trend-cycle and seasonal components. STL is more flexible than classical decomposition and can handle changes in the seasonal component over time.\n\n\nIt can handle any type of seasonality (not just fixed periods).\nIt can handle changes in the seasonal component over time.\nIt is robust to outliers.\nIt can be used for forecasting.\nIt provides a way to control the smoothness of the trend and seasonal components through parameters.\n\n\n\n\n\n\n\n\nSTL cannot automatically handle calendar or holiday variations.\nIt only provides methods for additive models. If your data has multiplicative seasonality, you should log-transform the data before applying STL."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\n\nmexretail_stl &lt;- mexretail |&gt;                                \n  model(                                                      \n    stl = STL(y)\n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-1",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\n\nmexretail_stl &lt;- mexretail |&gt;                                \n  model(                                                      \n    stl = STL(y ~ trend(window = NULL))\n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-2",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-2",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\n\nmexretail_stl &lt;- mexretail |&gt;                                \n  model(                                                      \n    stl = STL(y ~ trend()) \n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-3",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-3",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\nSTL in R using fable\n\nThe code is basically the same as for the classical decomposition. We just need to change the model used inside the model() function.\n\n\nmexretail |&gt;                                \n  model(                                                      \n1    stl = STL(y ~\n2                trend(window = NULL) +\n3                season(window = \"periodic\"),\n4              robust = TRUE)\n  ) |&gt; \n  components() |&gt; \n  autoplot()\n\n\n1\n\nInside the STL() function, we can specify the formula for the decomposition, or don’t specify it at all. See ?STL for more details.\n\n2\n\nThe trend() function is used to specify the trend component of the decomposition. The window argument controls the smoothness of the trend component. A larger window results in a smoother trend.\n\n3\n\nThe season() function is used to specify the seasonal component of the decomposition. The window argument controls the smoothness of the seasonal component. Setting it to “periodic” means that the seasonal component will be fixed over time.\n\n4\n\nThe robust argument, when set to TRUE, makes the STL decomposition more robust to outliers in the data, so the effect of such values is sent to the residual component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting formulas in R\n\n\nIn R, we use “\\sim” instead of “=” in formula specification, i.e., y \\sim mx + b."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#footnotes",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#footnotes",
    "title": "Time Series Decomposition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA time series can have multiple seasonal patterns.\nThey usually last at least 2 years.\nIn R, you can compute any moving average by using the slider::slide_dbl() function.\nshort for “model table”\nThere are other decomposition methods primarily used by official statistics agencies, such as X-11, X-12-ARIMA, and TRAMO/SEATS. However, these methods are not as widely used in the forecasting community as STL. For more on these, see this."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html",
    "title": "Time Series Decomposition",
    "section": "",
    "text": "1library(tidyquant)\nlibrary(plotly)\n\n\n1\n\nIn addition to the regular packages, here we’ll use tidyquant and plotly",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#ts-patterns",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#ts-patterns",
    "title": "Time Series Decomposition",
    "section": "1.1 TS Patterns",
    "text": "1.1 TS Patterns\nTime series can have distinct patterns:\n\n\nTrend: A long-term increase/decrease in the data.\nSeasonal: Fluctuations in the time series with a fixed and known period1.\nCycles: More commonly known as “Business cycles”, refer to rises and falls that are not of a fixed frequency2.\nChanges in variability: Changes in the spread of the data over time, i. e., an increase/decrease in the variance as the level of the series increases/decreases.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#components-of-a-time-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#components-of-a-time-series",
    "title": "Time Series Decomposition",
    "section": "1.2 Components of a Time Series",
    "text": "1.2 Components of a Time Series\nA time series can be decomposed into the following components:\n\n\nSeasonal component (S): The repeating short-term cycle in the series.\nTrend-cycle component (T): The long-term progression of the series.\nResidual component (R): The residuals or “noise” left after removing the seasonal and trend-cycle components.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#log-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#log-transformations",
    "title": "Time Series Decomposition",
    "section": "2.1 Log transformations",
    "text": "2.1 Log transformations\n\nSeries in levelsLog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations and adjustments help us simplify the patterns in our data, and can improve our forecasts’ accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog transformations are often useful when the data presents an increasing/decreasing variation with the level of the series.\nLog transformations are very interpretable: changes in a log value are percent changes on the original scale.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#box-cox-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#box-cox-transformations",
    "title": "Time Series Decomposition",
    "section": "2.2 Box-Cox transformations",
    "text": "2.2 Box-Cox transformations\n\nw_t= \\begin{cases}\\log \\left(y_t\\right) & \\text { if } \\lambda=0 \\\\ \\left(\\operatorname{sign}\\left(y_t\\right)\\left|y_t\\right|^\\lambda-1\\right) / \\lambda & \\text { otherwise }\\end{cases}\n\n\nIn a Box-Cox transformation, the log is always a natural logarithm. The other case is just a power transformation with scaling.\n\nWhat happens when \\lambda = 1?\n\n\n\n\n\n\nYou should choose a value of \\lambda that makes the size of the seasonal variation the same throughout the series.\n\n\n\n\n2.2.1 How can we choose the value of \\lambda?\nWe can use the guerrero feature to choose an optimal lambda.\n\naus_production |&gt; \n  features(Gas, features = guerrero)\n\n# A tibble: 1 × 1\n  lambda_guerrero\n            &lt;dbl&gt;\n1           0.110",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#calendar-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#calendar-adjustments",
    "title": "Time Series Decomposition",
    "section": "3.1 Calendar adjustments",
    "text": "3.1 Calendar adjustments\n\nClosing price and volumeMonthly aggregationMonthly total and mean and volume\n\n\n\n\n\n\n\n\n\n\n\ngoogle_month &lt;- google |&gt; \n  index_by(month = yearmonth(date)) |&gt; \n  summarise(\n    trading_days = n(),\n    monthly_volume = sum(volume),\n    mean_volume = mean(volume)\n  )\n\ngoogle_month\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of trading days in a month can vary due to weekends and holidays, and not because of any economic reason.\nUsing the monthly total volume can be misleading, as months with more trading days will naturally have higher total volumes.\nUsing the mean volume per trading day helps to standardize the data, making it easier to compare across months.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#population-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#population-adjustments",
    "title": "Time Series Decomposition",
    "section": "3.2 Population adjustments",
    "text": "3.2 Population adjustments\n\n\n\n\n\n\nIs the Mexican economy really that similar Australia’s economy? Is Iceland’s economy really that small?\n\n\nA greater GDP can be interpreted as having a larger economy, and a better life standard, but this is not always the case.\nComparing GDP across countries with different population sizes can be misleading.\nGDP is often used to measure the economic performance of a country, but it doesn’t account for population size.\nThe higher the population, the higher the GDP tends to be, simply because there are more people contributing to the economy.\nA more meaningful comparison can be made by looking at GDP per capita, which divides the GDP by the population size.\n\n\n\nPopulationGDP per capita\n\n\n\n\n\n\n\n\nThe population sizes of these countries are very different.\n\n\n\n\n\n\n\n\n\n\nGDP per capita provides a more accurate representation of the economic well-being of individuals in a country.\nIt is clear now that Iceland and Australia have a much higher GDP per capita compared to Mexico, indicating a higher standard of living for its residents.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#inflation-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#inflation-adjustments",
    "title": "Time Series Decomposition",
    "section": "3.3 Inflation adjustments",
    "text": "3.3 Inflation adjustments\n\n\nInflation is the rate at which the general level of prices for goods and services is rising, and subsequently, purchasing power is falling.\nTo make meaningful comparisons of economic data over time, it is essential to adjust for inflation.\nThis adjustment is typically done using a price index, such as the Consumer Price Index (CPI). In Mexico, the National Consumer Price Index (INPC) is used. INEGI provides this data.\n\n\n\n3.3.1 Inflation adjustment formula\n\nx_t = \\frac{y_t}{z_t} * z_{2010}\n\n\nwhere:\n\ny_t is the original value at time t (nominal value).\nz_t is the price index at time t (e.g., INPC).\nz_{2010} is the price index in the base year (2010 in this case).\nx_t is the inflation-adjusted value at time t (real value).\n\n\n\n\n3.3.2 Inflation adjustment example\n\nNominal valuesReal valuesNominal vs. Real values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naus_economy &lt;- global_economy |&gt;\n  filter(Code == \"AUS\")\n\n\nprint_retail &lt;- print_retail |&gt; \n  left_join(aus_economy, by = \"Year\") |&gt;\n  mutate(Adjusted_turnover = Turnover / CPI)",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#types-of-decompositions",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#types-of-decompositions",
    "title": "Time Series Decomposition",
    "section": "4.1 Types of Decompositions",
    "text": "4.1 Types of Decompositions\n\nA decomposition splits the time series into its underlying components:\n\nTrend-cycle\nSeasonal pattern(s)\n\nAnd what’s left of it we simply call it a “remainder component”.\nIn general, there are two types of decompositions:\n\n\n4.1.1 Additive decomposition\n\ny_t = T_t + S_t + R_t\n\n\n\n4.1.2 Multiplicative decomposition\n\ny_t = T_t \\times S_t \\times R_t \\\\\n\n\nWhich one should you use?\n\n\n\nIf the seasonal variation is roughly constant over time, use an additive decomposition.\nIf the seasonal variation increases or decreases with the level of the series, use a multiplicative decomposition.\nIf you’re unsure, you can try both and see which one provides a better fit.\n\nA multiplicative decomposition is equivalent to an additive decomposition of the log-transformed series:\n\ny_t = T_t \\times S_t \\times R_t\n\nis equivalent to\n\n\\log(y_t) = \\log(T_t) + \\log(S_t) + \\log(R_t)",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#seasonally-adjusted-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#seasonally-adjusted-series",
    "title": "Time Series Decomposition",
    "section": "4.2 Seasonally adjusted series",
    "text": "4.2 Seasonally adjusted series\n\nOne use of decomposition is to obtain a seasonally adjusted series, which is the original series with the seasonal component removed.\nSeasonally adjusted series can be useful for: - Identifying and analyzing the trend-cycle component without the influence of seasonal fluctuations. - Making comparisons across different time periods without seasonal effects.\n\n\n\n\n\n\n\n\nFor an additive decomposition, the seasonally adjusted series is given by: \ny_t - S_t\n\nFor a multiplicative decomposition, the seasonally adjusted series is given by: \n\\frac{y_t}{S_t}",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "4.3 Classical decomposition",
    "text": "4.3 Classical decomposition\nIn a classical decomposition, the trend-cycle component is estimated using a moving average. Then, the seasonal component is estimated by averaging the detrended values for each season. Finally, the remainder component is obtained by subtracting the trend-cycle and seasonal components from the original series.\nAn m order moving average is given by:\n\n\\hat{T}_{t}=\\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}\n\nwhere k = (m-1)/23.\n\n4.3.1 Example of a classical decomposition\n\n1mexretail_dcmp &lt;- mexretail |&gt;\n2  model(\n3    classical = classical_decomposition(y, type = \"additive\")\n  ) |&gt; \n4  components()\n\n5mexretail_dcmp\n\n\n1\n\nWe start with our original tsibble.\n\n2\n\nInside the model() function, we specify the type of models we want to use.\n\n3\n\nIn any model used, the first thing we need to specify is our forecast variable. Then, depending on the model used, we can specify additional parameters. The model() function yields a mable4, which is a table that contains the fitted models for each time series in the tsibble.\n\n4\n\nThe components() function is used to extract the components of the decomposition (trend-cycle, seasonal, and remainder) from the fitted models in the mable. It also provides the seasonally adjusted series.\n\n5\n\nFinally, we store the result.\n\n\n\n\n\n  \n\n\n\n\nmexretail_dcmp |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n4.3.2 Problems of using a Classical decomposition\n\n\nThe trend-cycle component is not estimated at the beginning and end of the series. This can be problematic if you want to forecast the series.\nIt also tends to over-smooth rises and falls.\nIt assumes that the seasonal component is constant over time, which may not be the case in many real-world scenarios.\nIt is not robust to outliers, which can significantly affect the estimates of the components.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIt is not recommended to use classical decomposition for forecasting because of these issues.",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#footnotes",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#footnotes",
    "title": "Time Series Decomposition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA time series can have multiple seasonal patterns.↩︎\nThey usually last at least 2 years.↩︎\nIn R, you can compute any moving average by using the slider::slide_dbl() function.↩︎\nshort for “model table”↩︎\nThere are other decomposition methods primarily used by official statistics agencies, such as X-11, X-12-ARIMA, and TRAMO/SEATS. However, these methods are not as widely used in the forecasting community as STL. For more on these, see this.↩︎",
    "crumbs": [
      "Modules",
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  }
]