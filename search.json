[
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntidyverse is a meta-package that loads the core packages of the tidyverse.\n\n\nWe will always load all the required packages a the beginning of the document. When loading the tidyverse, it shows which packages are being attached, as well as any conflicts with previously loaded packages.\n\n\n\n\n\n\nNoteCore packages\n\n\n\n\n\n\ndplyr is the core package for data transformation. It is paired up with the following packages for specific column types:\n\nstringr for strings.\nforcats for factors (R’s categorical data type).\nlubridate for dates and date-times.\n\nggplot2 is the primary package for visualization.\nreadr is used to import data from delimited files (CSV, TSV, …).\ntibble is a modern reimagining of the data frame, keeping what time has proven to be effective, and throwing out what is not.\ntidyr is used to tidy data, i.e. to ensure that each variable is in its own column, each observation is in its own row, and each value is in its own cell.\npurrr is used for functional programming with R.\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(fpp3)\n\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.2 ──\n\n\n✔ tsibble     1.1.6     ✔ feasts      0.4.2\n✔ tsibbledata 0.4.1     ✔ fable       0.5.0\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nfpp3 is also a meta-package that load the tidyverts ecosystem for time series analysis and forecasting.\n\n\nThe tidyverts packages are made to work seamlessly with the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\ntsibble is the main data structure we will use to analyze and model time series. It is a time series tibble.\nfeasts provides many functions and tools for feature and statistics extraction for time series.\nfable is the core package for modeling and foreasting time series.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverse",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverse",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntidyverse is a meta-package that loads the core packages of the tidyverse.\n\n\nWe will always load all the required packages a the beginning of the document. When loading the tidyverse, it shows which packages are being attached, as well as any conflicts with previously loaded packages.\n\n\n\n\n\n\nNoteCore packages\n\n\n\n\n\n\ndplyr is the core package for data transformation. It is paired up with the following packages for specific column types:\n\nstringr for strings.\nforcats for factors (R’s categorical data type).\nlubridate for dates and date-times.\n\nggplot2 is the primary package for visualization.\nreadr is used to import data from delimited files (CSV, TSV, …).\ntibble is a modern reimagining of the data frame, keeping what time has proven to be effective, and throwing out what is not.\ntidyr is used to tidy data, i.e. to ensure that each variable is in its own column, each observation is in its own row, and each value is in its own cell.\npurrr is used for functional programming with R.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverts",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#the-tidyverts",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Code\nlibrary(fpp3)\n\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.2 ──\n\n\n✔ tsibble     1.1.6     ✔ feasts      0.4.2\n✔ tsibbledata 0.4.1     ✔ fable       0.5.0\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nfpp3 is also a meta-package that load the tidyverts ecosystem for time series analysis and forecasting.\n\n\nThe tidyverts packages are made to work seamlessly with the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\ntsibble is the main data structure we will use to analyze and model time series. It is a time series tibble.\nfeasts provides many functions and tools for feature and statistics extraction for time series.\nfable is the core package for modeling and foreasting time series.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#tsibble-objects",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#tsibble-objects",
    "title": "RStudio, R, and Time Series",
    "section": "2.1 tsibble objects",
    "text": "2.1 tsibble objects\nLet’s take a look at tourism in Australia:\n\n\nCode\ntourism\n\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\n\nA tsibble is a modified version of a tibble as to\n\n\n\nCode\nkey_vars(tourism)\n\n\n[1] \"Region\"  \"State\"   \"Purpose\"\n\n\nCode\nkey_data(tourism)\n\n\n\n  \n\n\n\n\nThe tsibble has 24320 rows and 5 columns. It shows quarterly data1 on tourism across Australia. It’s divided by Region, State, and purspose of the trip2. How many different states are there?",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#australian-states",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#australian-states",
    "title": "RStudio, R, and Time Series",
    "section": "2.2 Australian States",
    "text": "2.2 Australian States\n\n\nCode\ndistinct(tourism, State)",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#which-regions-are-located-in-tasmania",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#which-regions-are-located-in-tasmania",
    "title": "RStudio, R, and Time Series",
    "section": "2.3 Which regions are located in Tasmania?",
    "text": "2.3 Which regions are located in Tasmania?\n\n\nCode\ndistinct(filter(tourism, State == \"Tasmania\"),Region)",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#data-transformation-average-trips",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#data-transformation-average-trips",
    "title": "RStudio, R, and Time Series",
    "section": "2.4 Data Transformation: Average trips",
    "text": "2.4 Data Transformation: Average trips\nTo get the average trips by purpose, we need to do the following:\n\n\nFilter the original tsibble to get only the data from East Coast, Tasmania.\nConvert the data to a tibble.\nGroup by purpose.\nSummarise by getting the mean of the trips.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#section",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#section",
    "title": "RStudio, R, and Time Series",
    "section": "2.5 ",
    "text": "2.5 \nWith traditional code, this would look something like:\n\n\nCode\nsummarise(group_by(as_tibble(filter(tourism, State == \"Tasmania\", \n                                    Region == \"East Coast\")), Purpose),\n          mean_trips = mean(Trips))\n\n\n\n\n\n\n\n\n\nNoteOrder of code execution\n\n\n\n\n\nNote that this code must be read inside-out. This makes it harder to understand, and also harder to debug.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#section-8",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#section-8",
    "title": "RStudio, R, and Time Series",
    "section": "2.6 ",
    "text": "2.6 \nUsing the native pipe operator; |&gt;, we can improve the same code:\n\n\nCode\n1tourism |&gt;\n2  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt;\n3  as_tibble() |&gt;\n4  group_by(Purpose) |&gt;\n5  summarise(mean_trips = mean(Trips))\n\n\n\n1\n\nTake the tsibble tourism, then\n\n2\n\nfilter by State and Region, then\n\n3\n\nconvert to a tibble, then\n\n4\n\ngroup the tibble by purpose, then\n\n5\n\nsummarise by taking the mean trips\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTipThe pipe operator |&gt;\n\n\n\n\n\nThe pipe is read as “then”, and it allows us to write code in the order it’s supposed to be run.\nIt also helps to debug code easier, because you can run each function in order and see where the error is.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#plotting-tourism-across-time-4",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#plotting-tourism-across-time-4",
    "title": "RStudio, R, and Time Series",
    "section": "3.1 Plotting tourism across time",
    "text": "3.1 Plotting tourism across time\n\n\nCode\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n1  autoplot(Trips) +\n2  facet_wrap(vars(Purpose), scale = \"free_y\") +\n3  theme(legend.position = \"none\")\n\n\n\n1\n\nautoplot() detects the data automatically and proposes a plot accordingly.\n\n2\n\nfacet_wrap() Divides a plot into subplots (facets).\n\n3\n\nyou can customize endless feautres using theme(). Here, we remove the legend, as it’s redudant.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#time-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#time-plots",
    "title": "RStudio, R, and Time Series",
    "section": "3.2 Time plots",
    "text": "3.2 Time plots\n\n\n\n\nCode\naus_production |&gt; \n  autoplot(Gas)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\naus_production |&gt; \n  autoplot(Gas) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nThese are the most basic type of plots. We have the time variable in the x-axis, and our forecast variable in the y-axis. Time plots should be line plots, and can include or not points.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-plots",
    "title": "RStudio, R, and Time Series",
    "section": "3.3 Seasonal Plots",
    "text": "3.3 Seasonal Plots\n\n\nCode\naus_production |&gt; \n  gg_season(Gas)\n\n\nWarning: `gg_season()` was deprecated in feasts 0.4.2.\nℹ Please use `ggtime::gg_season()` instead.\n\n\n\n\n\n\n\n\n\n\nThe data here are plotted against a single “season”. It’s useful in identifying years with changes in patterns.\n\nRemoving the trend from the data:",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-subseries-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#seasonal-subseries-plots",
    "title": "RStudio, R, and Time Series",
    "section": "3.4 Seasonal Subseries Plots",
    "text": "3.4 Seasonal Subseries Plots\n\n\nCode\naus_production |&gt; \n  gg_subseries(Gas)\n\n\nWarning: `gg_subseries()` was deprecated in feasts 0.4.2.\nℹ Please use `ggtime::gg_subseries()` instead.\n\n\n\n\n\n\n\n\n\n\nHere we split the plot into many subplots, one for each season. This helps us see clearly the underlying seasonal pattern. The mean for each season is represented as the blue horizontal line.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#gg_tsdisplay",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#gg_tsdisplay",
    "title": "RStudio, R, and Time Series",
    "section": "3.5 gg_tsdisplay()",
    "text": "3.5 gg_tsdisplay()\n\n\nCode\naus_production |&gt; \n  gg_tsdisplay(Gas, plot_type = \"season\")\n\n\nWarning: `gg_tsdisplay()` was deprecated in feasts 0.4.2.\nℹ Please use `ggtime::gg_tsdisplay()` instead.\n\n\n\n\n\n\n\n\n\n\nThis function provides a convenient way to have 3 plots: a time plot, an ACF plot, and a third option that can be customized with one of the following plot types:\n\n\n\n\n\n\nTipplot_type options\n\n\n\n\n\n\n“auto”,\n“partial”,\n“season”,\n“histogram”,\n“scatter”,\n“spectrum”",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#exporting-data-to-.csv",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#exporting-data-to-.csv",
    "title": "RStudio, R, and Time Series",
    "section": "3.6 Exporting data to .csv",
    "text": "3.6 Exporting data to .csv\n\n\nCode\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  mutate(Quarter = as.Date(Quarter)) |&gt; \n  write_csv(\"./datos/tasmania.csv\")\n\n\n\nYou can export to .csv by providing a tsibble or tibble (or any other type of data frame), by calling write_csv(), and specifying the output file’s name.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series.html#footnotes",
    "href": "docs/modules/module_1/01_time_series/r_time_series.html#footnotes",
    "title": "RStudio, R, and Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nshown besides the tsibble dimension as [1Q]↩︎\nthese are specified in the key argument. This tsibble contains ↩︎",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.1 RStudio, R, and Time Series"
    ]
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverse",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverse",
    "title": "RStudio, R, and Time Series",
    "section": "The tidyverse",
    "text": "The tidyverse\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntidyverse is a meta-package that loads the core packages of the tidyverse.\n\n\nWe will always load all the required packages a the beginning of the document. When loading the tidyverse, it shows which packages are being attached, as well as any conflicts with previously loaded packages.\n\n\n\n\n\n\nCore packages\n\n\n\ndplyr is the core package for data transformation. It is paired up with the following packages for specific column types:\n\nstringr for strings.\nforcats for factors (R’s categorical data type).\nlubridate for dates and date-times.\n\nggplot2 is the primary package for visualization.\nreadr is used to import data from delimited files (CSV, TSV, …).\ntibble is a modern reimagining of the data frame, keeping what time has proven to be effective, and throwing out what is not.\ntidyr is used to tidy data, i.e. to ensure that each variable is in its own column, each observation is in its own row, and each value is in its own cell.\npurrr is used for functional programming with R."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverts",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#the-tidyverts",
    "title": "RStudio, R, and Time Series",
    "section": "The tidyverts",
    "text": "The tidyverts\n\nlibrary(fpp3)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.2 ──\n\n\n✔ tsibble     1.1.6     ✔ feasts      0.4.2\n✔ tsibbledata 0.4.1     ✔ fable       0.5.0\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nfpp3 is also a meta-package that load the tidyverts ecosystem for time series analysis and forecasting.\n\n\nThe tidyverts packages are made to work seamlessly with the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\ntsibble is the main data structure we will use to analyze and model time series. It is a time series tibble.\nfeasts provides many functions and tools for feature and statistics extraction for time series.\nfable is the core package for modeling and foreasting time series."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#tsibble-objects",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#tsibble-objects",
    "title": "RStudio, R, and Time Series",
    "section": "tsibble objects",
    "text": "tsibble objects\nLet’s take a look at tourism in Australia:\n\ntourism\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\n\nA tsibble is a modified version of a tibble as to\n\n\nkey_vars(tourism)\n\n[1] \"Region\"  \"State\"   \"Purpose\"\n\nkey_data(tourism)\n\n\n  \n\n\n\n\nThe tsibble has 24320 rows and 5 columns. It shows quarterly data1 on tourism across Australia. It’s divided by Region, State, and purspose of the trip2. How many different states are there?"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#australian-states",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#australian-states",
    "title": "RStudio, R, and Time Series",
    "section": "Australian States",
    "text": "Australian States\n\ndistinct(tourism, State)"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#which-regions-are-located-in-tasmania",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#which-regions-are-located-in-tasmania",
    "title": "RStudio, R, and Time Series",
    "section": "Which regions are located in Tasmania?",
    "text": "Which regions are located in Tasmania?\n\ndistinct(filter(tourism, State == \"Tasmania\"),Region)"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#data-transformation-average-trips",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#data-transformation-average-trips",
    "title": "RStudio, R, and Time Series",
    "section": "Data Transformation: Average trips",
    "text": "Data Transformation: Average trips\nTo get the average trips by purpose, we need to do the following:\n\nFilter the original tsibble to get only the data from East Coast, Tasmania.\nConvert the data to a tibble.\nGroup by purpose.\nSummarise by getting the mean of the trips."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "With traditional code, this would look something like:\n\nsummarise(group_by(as_tibble(filter(tourism, State == \"Tasmania\", \n                                    Region == \"East Coast\")), Purpose),\n          mean_trips = mean(Trips))\n\n\n\n\n\n\n\n\nOrder of code execution\n\n\nNote that this code must be read inside-out. This makes it harder to understand, and also harder to debug."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-1",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-1",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter()"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-2",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-2",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt;"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-3",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-3",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-4",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-4",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by()"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-5",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-5",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by(Purpose) |&gt;"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-6",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-6",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by(Purpose) |&gt;              \n  summarise(mean_trips = )"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-7",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-7",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\ntourism |&gt;                          \n  filter(State == \"Tasmania\",       \n         Region == \"East Coast\") |&gt; \n  as_tibble() |&gt;                    \n  group_by(Purpose) |&gt;              \n  summarise(mean_trips = mean(Trips))"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-8",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#section-8",
    "title": "RStudio, R, and Time Series",
    "section": "",
    "text": "Using the native pipe operator; |&gt;, we can improve the same code:\n\n1tourism |&gt;\n2  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt;\n3  as_tibble() |&gt;\n4  group_by(Purpose) |&gt;\n5  summarise(mean_trips = mean(Trips))\n\n\n1\n\nTake the tsibble tourism, then\n\n2\n\nfilter by State and Region, then\n\n3\n\nconvert to a tibble, then\n\n4\n\ngroup the tibble by purpose, then\n\n5\n\nsummarise by taking the mean trips\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nThe pipe operator |&gt;\n\n\nThe pipe is read as “then”, and it allows us to write code in the order it’s supposed to be run.\nIt also helps to debug code easier, because you can run each function in order and see where the error is."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-1",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-1",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\")"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-2",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-2",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  autoplot(Trips)"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-3",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-3",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  autoplot(Trips) +\n  facet_wrap(vars(Purpose), scale = \"free_y\")"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-4",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#plotting-tourism-across-time-4",
    "title": "RStudio, R, and Time Series",
    "section": "Plotting tourism across time",
    "text": "Plotting tourism across time\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n1  autoplot(Trips) +\n2  facet_wrap(vars(Purpose), scale = \"free_y\") +\n3  theme(legend.position = \"none\")\n\n\n1\n\nautoplot() detects the data automatically and proposes a plot accordingly.\n\n2\n\nfacet_wrap() Divides a plot into subplots (facets).\n\n3\n\nyou can customize endless feautres using theme(). Here, we remove the legend, as it’s redudant."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#time-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#time-plots",
    "title": "RStudio, R, and Time Series",
    "section": "Time plots",
    "text": "Time plots\n\n\n\naus_production |&gt; \n  autoplot(Gas)\n\n\n\n\n\n\n\n\n\n\naus_production |&gt; \n  autoplot(Gas) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nThese are the most basic type of plots. We have the time variable in the x-axis, and our forecast variable in the y-axis. Time plots should be line plots, and can include or not points."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-plots",
    "title": "RStudio, R, and Time Series",
    "section": "Seasonal Plots",
    "text": "Seasonal Plots\n\naus_production |&gt; \n  gg_season(Gas)\n\n\n\n\n\n\n\n\n\nThe data here are plotted against a single “season”. It’s useful in identifying years with changes in patterns.\n\nRemoving the trend from the data:"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-subseries-plots",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#seasonal-subseries-plots",
    "title": "RStudio, R, and Time Series",
    "section": "Seasonal Subseries Plots",
    "text": "Seasonal Subseries Plots\n\naus_production |&gt; \n  gg_subseries(Gas)\n\n\n\n\n\n\n\n\n\nHere we split the plot into many subplots, one for each season. This helps us see clearly the underlying seasonal pattern. The mean for each season is represented as the blue horizontal line."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#gg_tsdisplay",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#gg_tsdisplay",
    "title": "RStudio, R, and Time Series",
    "section": "gg_tsdisplay()",
    "text": "gg_tsdisplay()\n\naus_production |&gt; \n  gg_tsdisplay(Gas, plot_type = \"season\")\n\n\n\n\n\n\n\n\n\nThis function provides a convenient way to have 3 plots: a time plot, an ACF plot, and a third option that can be customized with one of the following plot types:\n\n\n\n\n\n\nplot_type options\n\n\n\n“auto”,\n“partial”,\n“season”,\n“histogram”,\n“scatter”,\n“spectrum”"
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#exporting-data-to-.csv",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#exporting-data-to-.csv",
    "title": "RStudio, R, and Time Series",
    "section": "Exporting data to .csv",
    "text": "Exporting data to .csv\n\ntourism |&gt; \n  filter(State == \"Tasmania\",\n         Region == \"East Coast\") |&gt; \n  mutate(Quarter = as.Date(Quarter)) |&gt; \n  write_csv(\"./datos/tasmania.csv\")\n\n\nYou can export to .csv by providing a tsibble or tibble (or any other type of data frame), by calling write_csv(), and specifying the output file’s name."
  },
  {
    "objectID": "docs/modules/module_1/01_time_series/r_time_series_pres.html#footnotes",
    "href": "docs/modules/module_1/01_time_series/r_time_series_pres.html#footnotes",
    "title": "RStudio, R, and Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nshown besides the tsibble dimension as [1Q]\nthese are specified in the key argument. This tsibble contains"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html",
    "href": "docs/modules/module_1/03_fcst/forecasting.html",
    "title": "Forecasting",
    "section": "",
    "text": "Use readr::read_csv(), readxl::read_excel(), or tidyquant::tq_get() to import the data into R. You can find more on this here.\n\n\n\nYour data should be tidy. That means:\n\nEach variable should be in its own column.\nEach observation should be in its own row.\nEach value should be in its own cell.\n\n\n\n\nData tidying and transforming are covered in detail in R for Data Science.\nTransform the resulting tibble into a tsibble:\n\nIt should have an index (time) variable with the proper time format1.\nThe key argument is only necessary if the dataset contains more than one time series.\n\n\n\n\n\nSplit the data into a training set and a test set2. The training set is used to estimate the model parameters, while the test set is used to evaluate the model’s performance on unseen data.\nThe size of the training and test sets depends on the length of the time series and the forecasting horizon:\n\nIf the forecast horizon is e. g.. 12 months, the test set should contain 12 months of data.\nAnother common approach is to use the first 70-80% of the data for training and the remaining 20-30% for testing.\n\nWe can use filter_index() to create the training set3:\n\n\n\nCode\ndatos_train &lt;- &lt;tsibble&gt; |&gt; \n1  filter_index(\"start_date\" ~ \"end_date\")\n\n\n\n1\n\nReplace start_date and end_date with the desired date range for the training set.You can also use . to indicate the start or end of the series: filter_index(. ~ \"end_date\") or filter_index(\"start_date\" ~ .).\n\n\n\n\n\n\n\n\n\n\nNoteSplitting the data\n\n\n\n\n\nIn time series, the training set should always contain the earlier observations, while the test set should contain the later observations. This is because time series data is ordered in time, and we want to simulate the real-world scenario where we use past data to predict future values.\n\n\n\n\n\n\n\nPlot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots here.\n\n\nWarning: `gg_tsdisplay()` was deprecated in feasts 0.4.2.\nℹ Please use `ggtime::gg_tsdisplay()` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nDecide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series’ features.\nTrain the model specification on the training set. You can use the model() function to fit various forecasting models4.\n\n\nCode\ndatos_fit &lt;- datos_train |&gt; \n  model(\n1    model_1 = &lt;model_function_1&gt;(&lt;y_t&gt; ~ x_t),\n2    model_2 = &lt;model_function_2&gt;(&lt;transformation_function&gt;(&lt;y_t&gt;), &lt;args&gt;)\n  )\n\n\n\n1\n\nReplace model_function_1 with the desired forecasting method (e.g., ARIMA(), ETS(), NAIVE(), etc.). Replace &lt;y_t&gt; with the name of the forecast variable and &lt;predictor_variables&gt; with any predictor variables if applicable.\n\n2\n\nIf a transformation is needed, replace transformation_function with the appropriate function (e.g., log, box_cox, etc.) and include any specific arguments required by the model.\n\n\n\n\n\n\n\n\nFitted values, \\hat{y}_t: The values predicted by the model for the training set.\nresiduals, e_t: The difference between the actual values and the fitted values, calculated as\n\n\ne_t = y_t - \\hat{y}_t\n.\n\ninnovation residuals: Residuals on the transformed scale5.\n\nWe can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble white noise.\n\n\n\n\n\n\n\nThe fitted values and residuals can be extracted from the model table using augment().\n\n\n\n\n\n\n\n\n\n\nWe expect residuals to behave like white noise, thus having the following properties:\nThe most important:\n\nUncorrelated: There is no correlation between the values at different time points.\nZero mean: The average value of the series is constant over time (and equal to zero).\n\nNice to have:\n\nConstant variance: The variability of the series is constant over time.\nNormally distributed: The values follow a normal distribution (this is not always required).\n\n\n\n\nIf the residuals don’t meet these properties, we could refine the model:\n\nFor the first 2: add predictors or change the model structure.\nApply a variance-stabilizing transformation (e.g., Box-Cox).\nIf the residuals are not normally distributed, only the prediction intervals are affected. We can deal with this by using bootstrap prediction intervals.\n\n\n\n\n\nOnce a satisfactory model is obtained, we can proceed to forecast6. Use the forecast() function to generate forecasts for a specified horizon h.:\n\n\nCode\ndatos_fcst &lt;- datos_fit |&gt; \n1  forecast(h = &lt;forecast_horizon&gt;)\n\n\n\n1\n\nReplace &lt;forecast_horizon&gt; with the desired number of periods to forecast (e.g., 12 for 12 months ahead), or you can write in text \"1 year\" for a one-year forecast.\n\n\n\n\n\n\n\n\n\n\nNoteForecast horizon\n\n\n\nThe forecast horizon should have the same length as the test set to evaluate the model’s performance accurately.\n\n\n\n\nWe measure a forecast’s accuracy by measuring the forecast error. Forecast errors are computed as:\n\ne_{T+h} = y_{T+h} - \\hat{y}_{T+h|T}\n\n\n\n\n\n\n\n\nThese errors depend on the scale of the data. Therefore, they are not suitable for comparing forecast accuracy across series with different scales or units.\n\n\n\n\nWe can also measure errors as percentage errors7:\n\np_t = \\frac{e_{T+h}}{y_{T+h}} \\times 100\n\n\n\n\n\n\n\n\nPercentage errors can be problematic when the actual values are close to zero, leading to extremely high or undefined percentage errors.\n\n\n\n\nor scaled errors8.:\n\nFor non-seasonal time series:\n\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-1} \\sum_{t=2}^{T}\\left|y_{t}-y_{t-1}\\right|},\n\nFor seasonal time series:\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-m} \\sum_{t=m+1}^{T}\\left|y_{t}-y_{t-m}\\right|}.\n\n\n\n\nUsing this errors, we can compute various error metrics to summarize the forecast accuracy:\n\nCommon error metrics\n\n\n\n\n\n\n\n\nScale\nMetric\nDescription\nFormula\n\n\n\n\nScale-dependent\n\nRMSE\nMAE\n\n\nRoot Mean Squared Error\nMean Absolute Error\n\n\n\\sqrt{\\text{mean}(e_{t}^{2})}\n\\text{mean}(|e_{t}|)\n\n\n\nScale-independent\n\nMAPE\nMASE\nRMMSE\n\n\nMean Absolute Percentage Error\nMean Absolute Scaled Error\nRoot Mean Squared Scaled Error\n\n\n\\text{mean}(|p_{t}|)\n\\text{mean}(|q_{t}|)\n\\sqrt{\\text{mean}(q_{t}^{2})}\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a note.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#tidy-data",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#tidy-data",
    "title": "Forecasting",
    "section": "",
    "text": "Use readr::read_csv(), readxl::read_excel(), or tidyquant::tq_get() to import the data into R. You can find more on this here.\n\n\n\nYour data should be tidy. That means:\n\nEach variable should be in its own column.\nEach observation should be in its own row.\nEach value should be in its own cell.\n\n\n\n\nData tidying and transforming are covered in detail in R for Data Science.\nTransform the resulting tibble into a tsibble:\n\nIt should have an index (time) variable with the proper time format1.\nThe key argument is only necessary if the dataset contains more than one time series.\n\n\n\n\n\nSplit the data into a training set and a test set2. The training set is used to estimate the model parameters, while the test set is used to evaluate the model’s performance on unseen data.\nThe size of the training and test sets depends on the length of the time series and the forecasting horizon:\n\nIf the forecast horizon is e. g.. 12 months, the test set should contain 12 months of data.\nAnother common approach is to use the first 70-80% of the data for training and the remaining 20-30% for testing.\n\nWe can use filter_index() to create the training set3:\n\n\n\nCode\ndatos_train &lt;- &lt;tsibble&gt; |&gt; \n1  filter_index(\"start_date\" ~ \"end_date\")\n\n\n\n1\n\nReplace start_date and end_date with the desired date range for the training set.You can also use . to indicate the start or end of the series: filter_index(. ~ \"end_date\") or filter_index(\"start_date\" ~ .).\n\n\n\n\n\n\n\n\n\n\nNoteSplitting the data\n\n\n\n\n\nIn time series, the training set should always contain the earlier observations, while the test set should contain the later observations. This is because time series data is ordered in time, and we want to simulate the real-world scenario where we use past data to predict future values.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#visualize",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#visualize",
    "title": "Forecasting",
    "section": "",
    "text": "Plot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots here.\n\n\nWarning: `gg_tsdisplay()` was deprecated in feasts 0.4.2.\nℹ Please use `ggtime::gg_tsdisplay()` instead.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#specify-estimate",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#specify-estimate",
    "title": "Forecasting",
    "section": "",
    "text": "Decide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series’ features.\nTrain the model specification on the training set. You can use the model() function to fit various forecasting models4.\n\n\nCode\ndatos_fit &lt;- datos_train |&gt; \n  model(\n1    model_1 = &lt;model_function_1&gt;(&lt;y_t&gt; ~ x_t),\n2    model_2 = &lt;model_function_2&gt;(&lt;transformation_function&gt;(&lt;y_t&gt;), &lt;args&gt;)\n  )\n\n\n\n1\n\nReplace model_function_1 with the desired forecasting method (e.g., ARIMA(), ETS(), NAIVE(), etc.). Replace &lt;y_t&gt; with the name of the forecast variable and &lt;predictor_variables&gt; with any predictor variables if applicable.\n\n2\n\nIf a transformation is needed, replace transformation_function with the appropriate function (e.g., log, box_cox, etc.) and include any specific arguments required by the model.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#evaluate",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#evaluate",
    "title": "Forecasting",
    "section": "",
    "text": "Fitted values, \\hat{y}_t: The values predicted by the model for the training set.\nresiduals, e_t: The difference between the actual values and the fitted values, calculated as\n\n\ne_t = y_t - \\hat{y}_t\n.\n\ninnovation residuals: Residuals on the transformed scale5.\n\nWe can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble white noise.\n\n\n\n\n\n\n\nThe fitted values and residuals can be extracted from the model table using augment().\n\n\n\n\n\n\n\n\n\n\nWe expect residuals to behave like white noise, thus having the following properties:\nThe most important:\n\nUncorrelated: There is no correlation between the values at different time points.\nZero mean: The average value of the series is constant over time (and equal to zero).\n\nNice to have:\n\nConstant variance: The variability of the series is constant over time.\nNormally distributed: The values follow a normal distribution (this is not always required).\n\n\n\n\nIf the residuals don’t meet these properties, we could refine the model:\n\nFor the first 2: add predictors or change the model structure.\nApply a variance-stabilizing transformation (e.g., Box-Cox).\nIf the residuals are not normally distributed, only the prediction intervals are affected. We can deal with this by using bootstrap prediction intervals.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#forecast-1",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#forecast-1",
    "title": "Forecasting",
    "section": "",
    "text": "Once a satisfactory model is obtained, we can proceed to forecast6. Use the forecast() function to generate forecasts for a specified horizon h.:\n\n\nCode\ndatos_fcst &lt;- datos_fit |&gt; \n1  forecast(h = &lt;forecast_horizon&gt;)\n\n\n\n1\n\nReplace &lt;forecast_horizon&gt; with the desired number of periods to forecast (e.g., 12 for 12 months ahead), or you can write in text \"1 year\" for a one-year forecast.\n\n\n\n\n\n\n\n\n\n\nNoteForecast horizon\n\n\n\nThe forecast horizon should have the same length as the test set to evaluate the model’s performance accurately.\n\n\n\n\nWe measure a forecast’s accuracy by measuring the forecast error. Forecast errors are computed as:\n\ne_{T+h} = y_{T+h} - \\hat{y}_{T+h|T}\n\n\n\n\n\n\n\n\nThese errors depend on the scale of the data. Therefore, they are not suitable for comparing forecast accuracy across series with different scales or units.\n\n\n\n\nWe can also measure errors as percentage errors7:\n\np_t = \\frac{e_{T+h}}{y_{T+h}} \\times 100\n\n\n\n\n\n\n\n\nPercentage errors can be problematic when the actual values are close to zero, leading to extremely high or undefined percentage errors.\n\n\n\n\nor scaled errors8.:\n\nFor non-seasonal time series:\n\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-1} \\sum_{t=2}^{T}\\left|y_{t}-y_{t-1}\\right|},\n\nFor seasonal time series:\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-m} \\sum_{t=m+1}^{T}\\left|y_{t}-y_{t-m}\\right|}.\n\n\n\n\nUsing this errors, we can compute various error metrics to summarize the forecast accuracy:\n\nCommon error metrics\n\n\n\n\n\n\n\n\nScale\nMetric\nDescription\nFormula\n\n\n\n\nScale-dependent\n\nRMSE\nMAE\n\n\nRoot Mean Squared Error\nMean Absolute Error\n\n\n\\sqrt{\\text{mean}(e_{t}^{2})}\n\\text{mean}(|e_{t}|)\n\n\n\nScale-independent\n\nMAPE\nMASE\nRMMSE\n\n\nMean Absolute Percentage Error\nMean Absolute Scaled Error\nRoot Mean Squared Scaled Error\n\n\n\\text{mean}(|p_{t}|)\n\\text{mean}(|q_{t}|)\n\\sqrt{\\text{mean}(q_{t}^{2})}",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#communicate",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#communicate",
    "title": "Forecasting",
    "section": "",
    "text": "This is a note.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting.html#footnotes",
    "href": "docs/modules/module_1/03_fcst/forecasting.html#footnotes",
    "title": "Forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e., if the TS has a monthly frequency, the index variable should be in yearmonth format. Other formats coud be yearweek, yearquarter, year, date.↩︎\nSplitting the data into a training and test set is the minimum requirement for evaluating a forecasting model. If you want to avoid overfitting and get a more reliable estimate of the model’s performance, you should consider splitting the data into 3 sets: training, validation, and test sets. The validation set is used to tune model hyperparameters and select the best model, while the test set is used for the final evaluation of the selected model. For an even more robust evaluation of forecasting models, consider using time series cross-validation methods.↩︎\nand store it in a *_train object.↩︎\nand store the model table in a *_fit object.↩︎\nWe will focus on innovation residuals whenever a transformation is used in the model.↩︎\nand store the forecasts in a *_fcst object.↩︎\nPercentage errors are scale-independent, making them useful for comparing forecast accuracy across different series.↩︎\nScaled errors are also scale-independent and are useful for comparing forecast accuracy across different series.↩︎",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.3 Forecasting principles"
    ]
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#tidy-data",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#tidy-data",
    "title": "Forecasting",
    "section": "Tidy data",
    "text": "Tidy data\n\nUse readr::read_csv(), readxl::read_excel(), or tidyquant::tq_get() to import the data into R. You can find more on this here.\n\n\n\nYour data should be tidy. That means:\n\nEach variable should be in its own column.\nEach observation should be in its own row.\nEach value should be in its own cell.\n\n\n\n\nData tidying and transforming are covered in detail in R for Data Science.\nTransform the resulting tibble into a tsibble:\n\nIt should have an index (time) variable with the proper time format1.\nThe key argument is only necessary if the dataset contains more than one time series."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#traintest-split",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#traintest-split",
    "title": "Forecasting",
    "section": "Train/test split",
    "text": "Train/test split\n\nSplit the data into a training set and a test set2. The training set is used to estimate the model parameters, while the test set is used to evaluate the model’s performance on unseen data.\nThe size of the training and test sets depends on the length of the time series and the forecasting horizon:\n\nIf the forecast horizon is e. g.. 12 months, the test set should contain 12 months of data.\nAnother common approach is to use the first 70-80% of the data for training and the remaining 20-30% for testing.\n\nWe can use filter_index() to create the training set3:\n\n\n\ndatos_train &lt;- &lt;tsibble&gt; |&gt; \n1  filter_index(\"start_date\" ~ \"end_date\")\n\n\n1\n\nReplace start_date and end_date with the desired date range for the training set.You can also use . to indicate the start or end of the series: filter_index(. ~ \"end_date\") or filter_index(\"start_date\" ~ .)."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#visualize",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#visualize",
    "title": "Forecasting",
    "section": "Visualize",
    "text": "Visualize\nPlot the time series to identify patterns, such as trend and seasonality, and anomalies. This can help us choose an appropriate forecasting method. You can find many types of plots here."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#specify-estimate",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#specify-estimate",
    "title": "Forecasting",
    "section": "Specify & Estimate",
    "text": "Specify & Estimate\nDecide whether any math transformations or adjustments are neccesary and choose a forecasting method based on the series’ features.\nTrain the model specification on the training set. You can use the model() function to fit various forecasting models4.\n\ndatos_fit &lt;- datos_train |&gt; \n  model(\n1    model_1 = &lt;model_function_1&gt;(&lt;y_t&gt; ~ x_t),\n2    model_2 = &lt;model_function_2&gt;(&lt;transformation_function&gt;(&lt;y_t&gt;), &lt;args&gt;)\n  )\n\n\n1\n\nReplace model_function_1 with the desired forecasting method (e.g., ARIMA(), ETS(), NAIVE(), etc.). Replace &lt;y_t&gt; with the name of the forecast variable and &lt;predictor_variables&gt; with any predictor variables if applicable.\n\n2\n\nIf a transformation is needed, replace transformation_function with the appropriate function (e.g., log, box_cox, etc.) and include any specific arguments required by the model."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#evaluate",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#evaluate",
    "title": "Forecasting",
    "section": "Evaluate",
    "text": "Evaluate\n\nFitted values, \\hat{y}_t: The values predicted by the model for the training set.\nresiduals, e_t: The difference between the actual values and the fitted values, calculated as\n\n\ne_t = y_t - \\hat{y}_t\n.\n\ninnovation residuals: Residuals on the transformed scale5.\n\nWe can check if a model is capturing the patterns in the data by analyzing the residuals. Ideally, the residuals should resemble white noise.\n\n\n\n\n\n\n\nThe fitted values and residuals can be extracted from the model table using augment()."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#what-is-white-noise",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#what-is-white-noise",
    "title": "Forecasting",
    "section": "What is white noise?",
    "text": "What is white noise?"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#residual-diagnostics",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#residual-diagnostics",
    "title": "Forecasting",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\nWe expect residuals to behave like white noise, thus having the following properties:\nThe most important:\n\nUncorrelated: There is no correlation between the values at different time points.\nZero mean: The average value of the series is constant over time (and equal to zero).\n\nNice to have:\n\nConstant variance: The variability of the series is constant over time.\nNormally distributed: The values follow a normal distribution (this is not always required)."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#refine",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#refine",
    "title": "Forecasting",
    "section": "Refine",
    "text": "Refine\nIf the residuals don’t meet these properties, we could refine the model:\n\nFor the first 2: add predictors or change the model structure.\nApply a variance-stabilizing transformation (e.g., Box-Cox).\nIf the residuals are not normally distributed, only the prediction intervals are affected. We can deal with this by using bootstrap prediction intervals."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#forecast-accuracy",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#forecast-accuracy",
    "title": "Forecasting",
    "section": "Forecast accuracy",
    "text": "Forecast accuracy\nWe measure a forecast’s accuracy by measuring the forecast error. Forecast errors are computed as:\n\ne_{T+h} = y_{T+h} - \\hat{y}_{T+h|T}\n\n\n\n\n\n\n\n\nThese errors depend on the scale of the data. Therefore, they are not suitable for comparing forecast accuracy across series with different scales or units.\n\n\n\n\nWe can also measure errors as percentage errors7:\n\np_t = \\frac{e_{T+h}}{y_{T+h}} \\times 100\n\n\n\n\n\n\n\n\nPercentage errors can be problematic when the actual values are close to zero, leading to extremely high or undefined percentage errors.\n\n\n\n\nor scaled errors8.:\n\nFor non-seasonal time series:\n\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-1} \\sum_{t=2}^{T}\\left|y_{t}-y_{t-1}\\right|},\n\nFor seasonal time series:\n\nq_{j}=\\frac{e_{j}}{\\frac{1}{T-m} \\sum_{t=m+1}^{T}\\left|y_{t}-y_{t-m}\\right|}."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#error-metrics",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#error-metrics",
    "title": "Forecasting",
    "section": "Error metrics",
    "text": "Error metrics\nUsing this errors, we can compute various error metrics to summarize the forecast accuracy:\n\nCommon error metrics\n\n\n\n\n\n\n\n\nScale\nMetric\nDescription\nFormula\n\n\n\n\nScale-dependent\n\nRMSE\nMAE\n\n\nRoot Mean Squared Error\nMean Absolute Error\n\n\n\\sqrt{\\text{mean}(e_{t}^{2})}\n\\text{mean}(|e_{t}|)\n\n\n\nScale-independent\n\nMAPE\nMASE\nRMMSE\n\n\nMean Absolute Percentage Error\nMean Absolute Scaled Error\nRoot Mean Squared Scaled Error\n\n\n\\text{mean}(|p_{t}|)\n\\text{mean}(|q_{t}|)\n\\sqrt{\\text{mean}(q_{t}^{2})}"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#refit-and-forecast",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#refit-and-forecast",
    "title": "Forecasting",
    "section": "Refit and forecast",
    "text": "Refit and forecast\nRefit and forecast"
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#communicate",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#communicate",
    "title": "Forecasting",
    "section": "Communicate",
    "text": "Communicate\n\nThis is a note."
  },
  {
    "objectID": "docs/modules/module_1/03_fcst/forecasting_pres.html#footnotes",
    "href": "docs/modules/module_1/03_fcst/forecasting_pres.html#footnotes",
    "title": "Forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e., if the TS has a monthly frequency, the index variable should be in yearmonth format. Other formats coud be yearweek, yearquarter, year, date.\nSplitting the data into a training and test set is the minimum requirement for evaluating a forecasting model. If you want to avoid overfitting and get a more reliable estimate of the model’s performance, you should consider splitting the data into 3 sets: training, validation, and test sets. The validation set is used to tune model hyperparameters and select the best model, while the test set is used for the final evaluation of the selected model. For an even more robust evaluation of forecasting models, consider using time series cross-validation methods.\nand store it in a *_train object.\nand store the model table in a *_fit object.\nWe will focus on innovation residuals whenever a transformation is used in the model.\nand store the forecasts in a *_fcst object.\nPercentage errors are scale-independent, making them useful for comparing forecast accuracy across different series.\nScaled errors are also scale-independent and are useful for comparing forecast accuracy across different series."
  },
  {
    "objectID": "docs/modules/module_3/01_regression/regression.html",
    "href": "docs/modules/module_3/01_regression/regression.html",
    "title": "Regression models",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggtime)",
    "crumbs": [
      "3. Adding exogenous variables to the model",
      "3.1 Linear Regression Models"
    ]
  },
  {
    "objectID": "docs/modules/module_3/01_regression/regression.html#regression-models-for-time-series-forecasting",
    "href": "docs/modules/module_3/01_regression/regression.html#regression-models-for-time-series-forecasting",
    "title": "Regression models",
    "section": "\n1 Regression Models for Time Series forecasting",
    "text": "1 Regression Models for Time Series forecasting\n\n1.1 Introduction\nRegression models can be powerful tools for understanding and forecasting time series when the relationship between a dependent variable and one or more explanatory variables is approximately linear. Unlike ARIMA models, regression models explicitly model deterministic components such as trend and seasonality.\n\n1.2 Introduction\nRegression models can help identify and forecast deterministic patterns in time series. They are particularly useful when explanatory variables or known calendar effects drive much of the observed variation.\n\n1.3 Time Series Regression Models\nA time series regression model expresses a variable y_t as a linear combination of explanatory variables:\n\ny_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\dots + \\varepsilon_t\n\nwhere \\varepsilon_t is the random error term.\nWe will use the us_change dataset from the fpp3 package, which contains quarterly percentage changes in US consumption, income, production, savings, and unemployment.\n\nCodelibrary(fpp3)\n\nus_change |&gt; \n  glimpse()\n\nRows: 198\nColumns: 6\n$ Quarter      &lt;qtr&gt; 1970 Q1, 1970 Q2, 1970 Q3, 1970 Q4, 1971 Q1, 1971 Q2, 197…\n$ Consumption  &lt;dbl&gt; 0.61856640, 0.45198402, 0.87287178, -0.27184793, 1.901344…\n$ Income       &lt;dbl&gt; 1.0448013, 1.2256472, 1.5851538, -0.2395449, 1.9759249, 1…\n$ Production   &lt;dbl&gt; -2.45248553, -0.55145947, -0.35865175, -2.18569087, 1.909…\n$ Savings      &lt;dbl&gt; 5.2990141, 7.7898938, 7.4039841, 1.1698982, 3.5356669, 5.…\n$ Unemployment &lt;dbl&gt; 0.9, 0.5, 0.5, 0.7, -0.1, -0.1, 0.1, 0.0, -0.2, -0.1, -0.…\n\n\nA simple regression model relating Consumption to Income can be fit using:\n\nCodeus_change |&gt;\nmodel(OLS = TSLM(Consumption ~ Income)) |&gt;\nreport()\n\nSeries: Consumption \nModel: TSLM \n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.58236 -0.27777  0.01862  0.32330  1.42229 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.54454    0.05403  10.079  &lt; 2e-16 ***\nIncome       0.27183    0.04673   5.817  2.4e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5905 on 196 degrees of freedom\nMultiple R-squared: 0.1472, Adjusted R-squared: 0.1429\nF-statistic: 33.84 on 1 and 196 DF, p-value: 2.4022e-08\n\n\n\n\n\n\n\n\nTipInterpretation\n\n\n\n\n\nThe estimated coefficient for Income represents the average change in consumption (percentage points) associated with a one–percentage-point change in income.",
    "crumbs": [
      "3. Adding exogenous variables to the model",
      "3.1 Linear Regression Models"
    ]
  },
  {
    "objectID": "docs/modules/module_3/01_regression/reg_pres.html#regression-models-for-time-series-forecasting",
    "href": "docs/modules/module_3/01_regression/reg_pres.html#regression-models-for-time-series-forecasting",
    "title": "Regression models",
    "section": "Regression Models for Time Series forecasting",
    "text": "Regression Models for Time Series forecasting\nIntroduction\nRegression models can be powerful tools for understanding and forecasting time series when the relationship between a dependent variable and one or more explanatory variables is approximately linear. Unlike ARIMA models, regression models explicitly model deterministic components such as trend and seasonality.\nIntroduction\nRegression models can help identify and forecast deterministic patterns in time series. They are particularly useful when explanatory variables or known calendar effects drive much of the observed variation."
  },
  {
    "objectID": "docs/modules/module_2/02_stationarity/stationarity.html",
    "href": "docs/modules/module_2/02_stationarity/stationarity.html",
    "title": "Identifying Stationarity",
    "section": "",
    "text": "A time series is said to be stationary if its statistical properties (mean and variance) are constant over time.\n\n\n\nStationarity is important because many time series forecasting methods assume that the underlying data is stationary.\nNon-stationary data can lead to inaccurate forecasts and misleading interpretations.\n\n\n\nThere are two main types of stationarity:\n\n\nStrict Stationarity: The joint distribution of any set of observations is the same regardless of the time at which they are observed.\n\nWeak Stationarity: The mean, variance, and autocovariance of the series are constant over time.\n\n\nIn practice, we often focus on weak stationarity because it is easier to test and work with.\n\n\n\n\n Back to top",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.2 Identifying Stationarity"
    ]
  },
  {
    "objectID": "docs/modules/module_2/02_stationarity/stationarity_pres.html#section",
    "href": "docs/modules/module_2/02_stationarity/stationarity_pres.html#section",
    "title": "Identifying Stationarity",
    "section": "",
    "text": "A time series is said to be stationary if its statistical properties (mean and variance) are constant over time.\n\n\nStationarity is important because many time series forecasting methods assume that the underlying data is stationary.\nNon-stationary data can lead to inaccurate forecasts and misleading interpretations.\n\nThere are two main types of stationarity:\n\n\nStrict Stationarity: The joint distribution of any set of observations is the same regardless of the time at which they are observed.\n\nWeak Stationarity: The mean, variance, and autocovariance of the series are constant over time.\n\n\nIn practice, we often focus on weak stationarity because it is easier to test and work with."
  },
  {
    "objectID": "docs/modules/module_2/02_stationarity/stationarity_pres.html#section-1",
    "href": "docs/modules/module_2/02_stationarity/stationarity_pres.html#section-1",
    "title": "Identifying Stationarity",
    "section": "",
    "text": "A time series with an upward trend.\n\n\n\n\n\nA time series with a downward trend."
  },
  {
    "objectID": "docs/modules/module_2/02_stationarity/stationarity_pres.html#section-2",
    "href": "docs/modules/module_2/02_stationarity/stationarity_pres.html#section-2",
    "title": "Identifying Stationarity",
    "section": "",
    "text": "A time series with yearly seasonality.\n\n\n\n\n\nA time series with yearly seasonality and a shifting trend."
  },
  {
    "objectID": "docs/modules/index.html",
    "href": "docs/modules/index.html",
    "title": "Modules",
    "section": "",
    "text": "Welcome to the Modules section of the course. Each module is a short, guided sequence of lessons with examples and practice opportunities."
  },
  {
    "objectID": "docs/modules/index.html#prerequisites-and-setup",
    "href": "docs/modules/index.html#prerequisites-and-setup",
    "title": "Modules",
    "section": "1 Prerequisites and setup",
    "text": "1 Prerequisites and setup\nA dedicated setup page is coming soon. For now:\n\nMake sure you can open an .Rproj, run an {r} code chunk, and install packages in R.\nIf something breaks, start with Module 1 — Introduction, which includes the course expectations and workflow."
  },
  {
    "objectID": "docs/modules/index.html#course-map",
    "href": "docs/modules/index.html#course-map",
    "title": "Modules",
    "section": "2 Course map",
    "text": "2 Course map\n\n2.1 Module 1: Forecasting models based on decomposition methods\nYou’ll build fundamentals: time series data structures, decomposition, and forecasting principles.\n\n1.0 Introduction\n1.1 RStudio, R, and Time Series\n1.2 Time Series Decomposition\n1.3 Forecasting principles\n1.4 The Forecasting Workflow\n\nWhat’s next? Start with 1.0 Introduction, then continue in order using the sidebar.\n\n\n\n2.2 Module 2: Adding ETS and ARIMA filters\nYou’ll expand to classical statistical forecasting families and diagnostics.\n\n2.1 Exponential Smoothing\n2.2 Identifying Stationarity\n2.3 ARIMA models (coming soon)\n2.4 Decomposition & ETS/ARIMA (coming soon)\n\nWhat’s next? After Module 1, begin with 2.1 Exponential Smoothing.\n\n\n\n2.3 Module 3: Adding exogenous variables to the model\nYou’ll incorporate external drivers and build stronger, more realistic forecasts.\n\n3.1 Linear Regression Models\n3.2 Build dynamic regressions (coming soon)\n3.3 Analyze the model’s performance (coming soon)\n3.4 Choose the best variables for the model (coming soon)\n\nWhat’s next? When available, start with 3.1 Linear Regression Models and follow the sequence.\n\n\n\n2.4 Module 4: Forecasting at scale\nYou’ll learn approaches for multiple series, hierarchy, reconciliation, and scalable evaluation.\n\n4.1 Understanding the challenges and approaches when dealing with multiple time series (coming soon)\n4.2 Applying hierarchical and reconciliation models (coming soon)\n4.3 Efficient forecasting workflow (coming soon)\n4.4 Evaluation of the global performance (coming soon)\n\nWhat’s next? Check back once lessons are published—this module will be best tackled after Modules 1–3."
  },
  {
    "objectID": "docs/modules/index.html#recommended-study-loop",
    "href": "docs/modules/index.html#recommended-study-loop",
    "title": "Modules",
    "section": "3 Recommended study loop",
    "text": "3 Recommended study loop\n\nRead the lesson (focus on why the method works).\nRun the code and check outputs.\nTweak one assumption (horizon, transformation, features) and observe what changes.\nPractice with the matching exercise set."
  },
  {
    "objectID": "docs/exercises/posts/prueba.html",
    "href": "docs/exercises/posts/prueba.html",
    "title": "Prueba",
    "section": "",
    "text": "Esto es una prueba.\n\nCode1 + 1\n\n[1] 2\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/exercises/index.html",
    "href": "docs/exercises/index.html",
    "title": "In-class exercises",
    "section": "",
    "text": "Welcome to the In-class exercises section of the course. Here you will find exercises designed to reinforce the concepts covered in the modules.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDescomposición de series de tiempo\n\n\nIntro a la descomposición STL usando fable\n\n\n\n\n\nFeb 4, 2026\n\n\nPablo Benavides Herrera\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nPrueba 2\n\n\n\nVisualization\n\nfoundations\n\n\n\n\n\n\n\n\n\nJan 22, 2026\n\n\nPablo Benavides-Herrera\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nPrueba\n\n\n\nfoundations\n\nR\n\n\n\n\n\n\n\n\n\nJan 21, 2026\n\n\nPablo Benavides-Herrera\n\n1 min\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html",
    "href": "docs/more/r-tools/r_4_python_users.html",
    "title": "R for Python Users",
    "section": "",
    "text": "This document is optional, but highly recommended if you come from a Python background.\nThe goal is not to teach R from scratch.\nThe goal is to help you translate mental models from Python to R, so that the rest of the course feels natural instead of frustrating."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#the-main-mindset-shift",
    "href": "docs/more/r-tools/r_4_python_users.html#the-main-mindset-shift",
    "title": "R for Python Users",
    "section": "1 The main mindset shift",
    "text": "1 The main mindset shift\nIf you come from Python, you are used to thinking in terms of:\n\nstep-by-step instructions\n\nobjects that are modified in place\n\nexplicit loops\n\n“do this, then do that”\n\nR (especially the tidyverse) encourages a different way of thinking:\n\ntransformations instead of instructions\n\nimmutable data objects\n\npipelines instead of loops\n\n“what happens to the data as it flows”\n\n\n\n\n\n\n\nTip\n\n\n\nIn tidyverse code, try to read pipelines out loud.\nIf it sounds like a sentence describing the data, you are probably doing it right."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#imperative-vs-functional-style",
    "href": "docs/more/r-tools/r_4_python_users.html#imperative-vs-functional-style",
    "title": "R for Python Users",
    "section": "2 Imperative vs functional style",
    "text": "2 Imperative vs functional style\n\nPythonR\n\n\ndf = df[df[\"value\"] &gt; 0]\ndf[\"log_value\"] = np.log(df[\"value\"])\ndf = df.groupby(\"id\").mean()\n\nEach line modifies or reassigns df\nState changes over time\nOrder matters a lot\n\n\n\ndf |&gt;\n  filter(value &gt; 0) |&gt;\n  mutate(log_value = log(value)) |&gt;\n  group_by(id) |&gt;\n  summarise(across(everything(), mean))\n\nEach step returns a new object\nNo mutation in place\nThe pipeline reads top to bottom\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThink of |&gt; as saying:\n“take the result so far, and then apply the next transformation”."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#core-object-mapping-python-r",
    "href": "docs/more/r-tools/r_4_python_users.html#core-object-mapping-python-r",
    "title": "R for Python Users",
    "section": "3 Core object mapping (Python → R)",
    "text": "3 Core object mapping (Python → R)\n\n\n\nPython\nR\n\n\n\n\npandas.DataFrame\ntibble\n\n\nSeries\nvector\n\n\nNaN\nNA\n\n\ndf.copy()\nusually unnecessary\n\n\nfor row in df\navoid\n\n\ndf.groupby()\ngroup_by()\n\n\nmethod chaining\npipe (|&gt;)\n\n\ndf.reset_index()\nrarely needed\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn R, columns are vectors, not mini-dataframes.\nThis is one of the biggest conceptual differences — and one of R’s strengths."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#mutation-the-silent-trap-for-python-users",
    "href": "docs/more/r-tools/r_4_python_users.html#mutation-the-silent-trap-for-python-users",
    "title": "R for Python Users",
    "section": "4 Mutation: the silent trap for Python users",
    "text": "4 Mutation: the silent trap for Python users\n\nPythonR\n\n\ndf[\"x\"] = df[\"x\"] * 2\nThis modifies df in place.\n\n\ndf |&gt;\n  mutate(x = x * 2)\nThis does not modify df unless you reassign it.\nCorrect usage:\ndf &lt;- df |&gt;\n  mutate(x = x * 2)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you forget to reassign in R, nothing happens.\nThis is the most common source of “why didn’t my code work?” for Python users."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#loops-just-because-you-can-doesnt-mean-you-should",
    "href": "docs/more/r-tools/r_4_python_users.html#loops-just-because-you-can-doesnt-mean-you-should",
    "title": "R for Python Users",
    "section": "5 Loops: just because you can, doesn’t mean you should",
    "text": "5 Loops: just because you can, doesn’t mean you should\n\nPythonR\n\n\n# Python-style thinking applied to R (not recommended)\nfor (i in 1:nrow(df)) {\n  df$x[i] &lt;- df$x[i] * 2\n}\n\n\ndf |&gt;\n  mutate(x = x * 2)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you feel the urge to write a for loop in R, stop and ask:\n“Is this a vectorized operation?”\nIn most cases, the answer is yes."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#indexing-brackets-vs-verbs",
    "href": "docs/more/r-tools/r_4_python_users.html#indexing-brackets-vs-verbs",
    "title": "R for Python Users",
    "section": "6 Indexing: brackets vs verbs",
    "text": "6 Indexing: brackets vs verbs\n\nPythonR\n\n\ndf.iloc[0]\ndf.loc[df[\"x\"] &gt; 0]\n\n\ndf[1, ]\ndf |&gt;\n  filter(x &gt; 0)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBase R indexing exists, but in this course we will prefer tidyverse verbs\nbecause they are clearer and less error-prone."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#equality-assignment-and-naming",
    "href": "docs/more/r-tools/r_4_python_users.html#equality-assignment-and-naming",
    "title": "R for Python Users",
    "section": "7 Equality, assignment, and naming",
    "text": "7 Equality, assignment, and naming\n\n&lt;- is the standard operator for assignment in this course\n\n= is used only for function arguments\n== is for comparison\nNames are case-sensitive\nAvoid spaces in column names (or use backticks)\n\n\n\n\n\n\n\nTip\n\n\n\nUsing &lt;- consistently makes it easier to visually distinguish\nassignment from function arguments, especially in longer pipelines."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#factors-the-weird-thing-you-didnt-ask-for",
    "href": "docs/more/r-tools/r_4_python_users.html#factors-the-weird-thing-you-didnt-ask-for",
    "title": "R for Python Users",
    "section": "8 Factors: the weird thing you didn’t ask for",
    "text": "8 Factors: the weird thing you didn’t ask for\nR has a special type called factor (categorical variable).\nSometimes you will see:\nstr(df)\nand a column is a factor when you expected a string.\n\n\n\n\n\n\nNote\n\n\n\nFor now:\n\ndon’t panic\nuse as.character() if needed\nwe will be explicit about factors when they matter"
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#what-you-actually-need-for-this-course",
    "href": "docs/more/r-tools/r_4_python_users.html#what-you-actually-need-for-this-course",
    "title": "R for Python Users",
    "section": "9 What you actually need for this course",
    "text": "9 What you actually need for this course\nYou do not need to become an R expert.\nYou need to be comfortable with:\n\ntibble\nfilter()\nmutate()\nsummarise()\ngroup_by()\npipes (|&gt;)\n\nEverything else is secondary."
  },
  {
    "objectID": "docs/more/r-tools/r_4_python_users.html#final-reassurance",
    "href": "docs/more/r-tools/r_4_python_users.html#final-reassurance",
    "title": "R for Python Users",
    "section": "10 Final reassurance",
    "text": "10 Final reassurance\n\n\n\n\n\n\nTip\n\n\n\nIf you are fluent in Python, you already have the hard skills:\n\ndata thinking\ndebugging intuition\nabstraction\n\nR is just a different dialect.\n\n\nOnce the mental model clicks, the rest of the course will feel much lighter."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html",
    "href": "docs/more/r-tools/pipes_tidyverse.html",
    "title": "Pipes and the tidyverse mental model",
    "section": "",
    "text": "This document is optional, but strongly recommended.\nIf you are new to R or come from another language, understanding pipes will make the entire course easier, clearer, and more enjoyable."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#what-a-pipe-really-means",
    "href": "docs/more/r-tools/pipes_tidyverse.html#what-a-pipe-really-means",
    "title": "Pipes and the tidyverse mental model",
    "section": "1 What a pipe really means",
    "text": "1 What a pipe really means\nIn tidyverse-style R, the pipe operator |&gt; means:\n\n“take the result of the previous step and pass it as the input of the next one”\n\nA useful way to read a pipeline is to mentally replace |&gt; with the word “then”.\nFor example:\ndf |&gt;\n  filter(value &gt; 0) |&gt;\n  summarise(mean_value = mean(value))\ncan be read as:\n\n“take df, then filter rows where value &gt; 0, then compute the mean of value.”\n\n\n\n\n\n\n\nTip\n\n\n\nIf you can read a pipeline out loud using “then” and it still makes sense,\nyou are probably writing clear tidyverse code.\n\n\n\n\n\n\n\n\nFor more on style, including guidance on pipes, see\nR for Data Science (2e) — Workflow: code style (Pipes)."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#tidyverse-code-with-and-without-pipes",
    "href": "docs/more/r-tools/pipes_tidyverse.html#tidyverse-code-with-and-without-pipes",
    "title": "Pipes and the tidyverse mental model",
    "section": "2 Tidyverse code with and without pipes",
    "text": "2 Tidyverse code with and without pipes\n\nWithout pipesWith pipes\n\n\ndf_filtered &lt;- filter(df, value &gt; 0)\ndf_transformed &lt;- mutate(df_filtered, log_value = log(value))\ndf_grouped &lt;- group_by(df_transformed, id)\ndf_summary &lt;- summarise(df_grouped, mean_value = mean(log_value))\n\nExplicit intermediate objects\n\nMore typing\n\nEasier to lose the “story” of the data\n\n\n\ndf |&gt;\n  filter(value &gt; 0) |&gt;\n  mutate(log_value = log(value)) |&gt;\n  group_by(id) |&gt;\n  summarise(mean_value = mean(log_value))\n\nOne clear data flow\n\nNo temporary variables\n\nEasier to read, explain, and debug"
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#the-tidyverse-mental-model",
    "href": "docs/more/r-tools/pipes_tidyverse.html#the-tidyverse-mental-model",
    "title": "Pipes and the tidyverse mental model",
    "section": "3 The tidyverse mental model",
    "text": "3 The tidyverse mental model\nA tidyverse pipeline usually follows this pattern:\n\nstart with a dataset\n\nfilter rows\n\ncreate or transform variables\n\ngroup the data\n\nsummarise\n\nThis is not a strict rule, but a very common and effective structure.\n\n\n\n\n\n\nNote\n\n\n\nMost tidyverse verbs return a new tibble.\nThe original data is never modified unless you explicitly reassign it.\n\n\n\n\n\n\n\n\nIf you want a broader overview of how the pieces fit together, see\nR for Data Science (2e) — Introduction."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#pipes-are-not-magic",
    "href": "docs/more/r-tools/pipes_tidyverse.html#pipes-are-not-magic",
    "title": "Pipes and the tidyverse mental model",
    "section": "4 Pipes are not magic",
    "text": "4 Pipes are not magic\nThe pipe does not change what functions do.\nIt only changes how inputs are passed.\n\n\n4.1 Without pipes\nsummarise(\n  group_by(\n    mutate(\n      filter(df, value &gt; 0),\n      log_value = log(value)\n    ),\n    id\n  ),\n  mean_value = mean(log_value)\n)\n\n\n4.2 With pipes\ndf |&gt;\n  filter(value &gt; 0) |&gt;\n  mutate(log_value = log(value)) |&gt;\n  group_by(id) |&gt;\n  summarise(mean_value = mean(log_value))\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you can rewrite a pipeline as nested function calls,\nthen the pipeline is doing exactly what you think it is."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#pipes-and-column-references",
    "href": "docs/more/r-tools/pipes_tidyverse.html#pipes-and-column-references",
    "title": "Pipes and the tidyverse mental model",
    "section": "5 Pipes and column references",
    "text": "5 Pipes and column references\nInside tidyverse verbs, column names are used directly.\ndf |&gt;\n  summarise(mean_value = mean(value, na.rm = TRUE))\nNo $, no indexing, no extra syntax.\n\n\n\n\n\n\nNote\n\n\n\nThis works because tidyverse uses data-masking:\ncolumn names are looked up automatically inside the data.\n\n\n\n\n\n\n\n\nYou do not need to master these details for this course.\nIf you are curious about what’s happening under the hood, see\nR for Data Science (2e) — Functions."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#why-we-avoid-in-this-course",
    "href": "docs/more/r-tools/pipes_tidyverse.html#why-we-avoid-in-this-course",
    "title": "Pipes and the tidyverse mental model",
    "section": "6 Why we avoid $ in this course",
    "text": "6 Why we avoid $ in this course\n\nIt breaks the pipeline mental model\n\nIt mixes different styles of R\n\nIt becomes confusing with grouped data\n\nFor clarity and consistency, we will stick to tidyverse verbs and pipelines."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#common-pipe-mistakes",
    "href": "docs/more/r-tools/pipes_tidyverse.html#common-pipe-mistakes",
    "title": "Pipes and the tidyverse mental model",
    "section": "7 Common pipe mistakes",
    "text": "7 Common pipe mistakes\n\n7.1 Breaking the pipeline\ndf |&gt;\n  filter(value &gt; 0)\n\nmutate(log_value = log(value))\nThe second line does not receive the result of the pipeline.\nCorrect:\ndf |&gt;\n  filter(value &gt; 0) |&gt;\n  mutate(log_value = log(value))\n\n\n\n7.2 Forgetting reassignment\ndf |&gt;\n  mutate(x = x * 2)\nNothing changes.\nCorrect:\ndf &lt;- df |&gt;\n  mutate(x = x * 2)\n\n\n\n\n\n\nWarning\n\n\n\nIf you forget to reassign, the pipeline runs but the result is discarded."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#pipes-and-readability",
    "href": "docs/more/r-tools/pipes_tidyverse.html#pipes-and-readability",
    "title": "Pipes and the tidyverse mental model",
    "section": "8 Pipes and readability",
    "text": "8 Pipes and readability\nPipelines are about clarity, not cleverness.\nPrefer this:\ndf |&gt;\n  filter(value &gt; 0) |&gt;\n  group_by(id) |&gt;\n  summarise(mean_value = mean(value))\nOver this:\ndf |&gt; filter(value &gt; 0) |&gt; group_by(id) |&gt; summarise(mean_value = mean(value))\n\n\n\n\n\n\nTip\n\n\n\nOne verb per line is a good default.\n\n\n\n\n\n\n\n\nFor more examples of filter(), mutate(), group_by(), and summarise(), see\nR for Data Science (2e) — Data transformation."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#when-not-to-use-pipes",
    "href": "docs/more/r-tools/pipes_tidyverse.html#when-not-to-use-pipes",
    "title": "Pipes and the tidyverse mental model",
    "section": "9 When not to use pipes",
    "text": "9 When not to use pipes\nPipes work best for linear transformations.\nThey are less useful for:\n\ncomplex branching logic\n\ndeeply nested conditionals\n\nnon–data-centric code\n\nEven so, in this course we will often prefer pipelines for consistency.\ndf |&gt;\n  summarise(mean_value = mean(value, na.rm = TRUE))"
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#pipes-in-this-course",
    "href": "docs/more/r-tools/pipes_tidyverse.html#pipes-in-this-course",
    "title": "Pipes and the tidyverse mental model",
    "section": "10 Pipes in this course",
    "text": "10 Pipes in this course\nThroughout the course:\n\nall data manipulation examples will use pipes\n\ntidyverse verbs will be preferred\n\nclarity will be valued over clever tricks\n\nIf something feels confusing, read the pipeline top to bottom, step by step —\nusing “then” as you go."
  },
  {
    "objectID": "docs/more/r-tools/pipes_tidyverse.html#final-takeaway",
    "href": "docs/more/r-tools/pipes_tidyverse.html#final-takeaway",
    "title": "Pipes and the tidyverse mental model",
    "section": "11 Final takeaway",
    "text": "11 Final takeaway\n\n\n\n\n\n\nTip\n\n\n\nIf you understand pipes, you understand most of tidyverse-based R.\n\n\nEverything else in the course builds on this idea."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html",
    "href": "docs/more/stats/expectation_variance_cov.html",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "",
    "text": "This document is optional, but strongly recommended.\nOnce uncertainty is represented through random variables, the next step is to quantify it.\nExpectation, variance, and covariance are the core mathematical tools we use to describe:\nThey appear everywhere in forecasting, whether explicitly or implicitly."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#expectation-the-long-run-average",
    "href": "docs/more/stats/expectation_variance_cov.html#expectation-the-long-run-average",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "1 Expectation: the long-run average",
    "text": "1 Expectation: the long-run average\nThe expectation (or expected value) of a random variable describes its typical value in the long run.\nFor a discrete random variable X with probability mass function p(x),\n\n\\mathbb{E}[X] = \\sum_x x \\, p(x).\n\nFor a continuous random variable with density f(x),\n\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\, f(x)\\, dx.\n\n\n\n\n\n\n\nTip\n\n\n\nExpectation is not what you expect to observe next.\nIt is what you would obtain on average, across many repetitions.\n\n\nIn forecasting, expectation often corresponds to:\n\nthe point forecast,\nthe mean of a forecast distribution,\nthe baseline around which uncertainty is measured."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#linearity-of-expectation-this-matters-a-lot",
    "href": "docs/more/stats/expectation_variance_cov.html#linearity-of-expectation-this-matters-a-lot",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "2 Linearity of expectation (this matters a lot)",
    "text": "2 Linearity of expectation (this matters a lot)\nOne of the most important properties of expectation is linearity:\n\n\\mathbb{E}[aX + b] = a\\,\\mathbb{E}[X] + b,\n\nfor constants a and b.\nEven more importantly,\n\n\\mathbb{E}[X + Y] = \\mathbb{E}[X] + \\mathbb{E}[Y],\n\nregardless of whether X and Y are independent.\n\n\n\n\n\n\nImportant\n\n\n\nLinearity of expectation holds without independence.\nThis fact underlies many results in time series and forecasting."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#variance-how-uncertain-is-a-random-variable",
    "href": "docs/more/stats/expectation_variance_cov.html#variance-how-uncertain-is-a-random-variable",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "3 Variance: how uncertain is a random variable?",
    "text": "3 Variance: how uncertain is a random variable?\nExpectation alone is not enough.\nTwo random variables may have the same mean but very different levels of uncertainty.\nThe variance of X measures how much values fluctuate around their expectation:\n\n\\operatorname{Var}(X) = \\mathbb{E}\\big[(X - \\mu)^2\\big],\n\\qquad \\mu = \\mathbb{E}[X].\n\nAn equivalent and often more convenient expression is:\n\n\\operatorname{Var}(X) = \\mathbb{E}[X^2] - \\mu^2.\n\n\n\n\n\n\n\nNote\n\n\n\nVariance is measured in squared units.\nThe standard deviation \\sigma = \\sqrt{\\operatorname{Var}(X)} restores the original scale.\n\n\nIn forecasting, variance captures:\n\nforecast uncertainty,\nvolatility,\nthe typical size of forecast errors."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#how-variance-reacts-to-transformations",
    "href": "docs/more/stats/expectation_variance_cov.html#how-variance-reacts-to-transformations",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "4 How variance reacts to transformations",
    "text": "4 How variance reacts to transformations\nVariance behaves very differently from expectation under transformations.\nFor constants a and b,\n\n\\operatorname{Var}(aX + b) = a^2 \\operatorname{Var}(X).\n\nAdding a constant does nothing to variance.\nScaling a variable scales variance quadratically.\n\n\n\n\n\n\nWarning\n\n\n\nVariance is sensitive to scale.\nThis is why transformations (e.g., logarithms) can dramatically change model behavior."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#covariance-measuring-dependence",
    "href": "docs/more/stats/expectation_variance_cov.html#covariance-measuring-dependence",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "5 Covariance: measuring dependence",
    "text": "5 Covariance: measuring dependence\nVariance describes uncertainty of a single random variable.\nCovariance describes how two random variables vary together.\nFor random variables X and Y,\n\n\\operatorname{Cov}(X, Y)\n= \\mathbb{E}\\big[(X - \\mu_X)(Y - \\mu_Y)\\big]\n= \\mathbb{E}[XY] - \\mu_X \\mu_Y.\n\n\nPositive covariance: large values of X tend to occur with large values of Y\nNegative covariance: large values of X tend to occur with small values of Y\n\n\n\n\n\n\n\nNote\n\n\n\nIf X and Y are independent, then\n\\operatorname{Cov}(X,Y) = 0.\nThe converse is not generally true."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#covariance-in-time-series",
    "href": "docs/more/stats/expectation_variance_cov.html#covariance-in-time-series",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "6 Covariance in time series",
    "text": "6 Covariance in time series\nIn time series analysis, covariance appears in a very specific form.\nGiven a sequence \\{Y_t\\}, we define the lag-h covariance as:\n\n\\operatorname{Cov}(Y_t, Y_{t-h}).\n\nThis quantity measures how observations relate to their own past.\n\n\n\n\n\n\nImportant\n\n\n\nAutocovariance is the foundation of:\n\nautocorrelation,\nstationarity,\nAR and MA models.\n\n\n\nIf you understand covariance, you are already halfway to understanding time series models."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#correlation-preview-not-the-full-story",
    "href": "docs/more/stats/expectation_variance_cov.html#correlation-preview-not-the-full-story",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "7 Correlation (preview, not the full story)",
    "text": "7 Correlation (preview, not the full story)\nCovariance depends on scale, which makes comparisons difficult.\nThe correlation coefficient rescales covariance:\n\n\\rho(X,Y) =\n\\frac{\\operatorname{Cov}(X,Y)}\n{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}.\n\nCorrelation lies between -1 and 1, but:\n\n\n\n\n\n\nWarning\n\n\n\nCorrelation measures linear association, not general dependence.\nThis distinction becomes critical in time series.\n\n\nCorrelation deserves its own refresher — and it will get one."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#where-this-shows-up-in-the-course",
    "href": "docs/more/stats/expectation_variance_cov.html#where-this-shows-up-in-the-course",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "8 Where this shows up in the course",
    "text": "8 Where this shows up in the course\nExpectation, variance, and covariance appear throughout the course:\n\npoint forecasts and forecast distributions,\nforecast error evaluation,\nresidual diagnostics,\nautocorrelation functions,\nstationarity assumptions.\n\nThey are not optional background — they are the mathematical backbone of forecasting."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#what-you-do-not-need-yet",
    "href": "docs/more/stats/expectation_variance_cov.html#what-you-do-not-need-yet",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "9 What you do not need yet",
    "text": "9 What you do not need yet\nAt this stage, you do not need:\n\nhigher-order moments,\ndistribution-specific formulas,\nclosed-form derivations.\n\nThose concepts matter, but only once the core quantities are fully internalized."
  },
  {
    "objectID": "docs/more/stats/expectation_variance_cov.html#takeaway",
    "href": "docs/more/stats/expectation_variance_cov.html#takeaway",
    "title": "Expectation, variance, and covariance: measuring uncertainty",
    "section": "10 Takeaway",
    "text": "10 Takeaway\n\n\n\n\n\n\nImportant\n\n\n\nExpectation describes the center.\nVariance describes uncertainty.\nCovariance describes dependence.\nTogether, they define how time series behave."
  },
  {
    "objectID": "docs/more/index.html",
    "href": "docs/more/index.html",
    "title": "More",
    "section": "",
    "text": "These materials are optional and are not required to pass the course.\nThey are meant to help you:\n\nRefresh prerequisite concepts (statistics, probability, regression)\nWork more comfortably in R and the tidyverse ecosystem\nImprove your workflow and reproducibility (projects, Quarto, organization)\n\nYou are encouraged to use them as needed.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCorrelation, dependence, and time series pitfalls\n\n\n\nStat refreshers\n\n\n\nA refresher on correlation and statistical dependence, with an emphasis on why correlation can be misleading in time series data.\n\n\n\n\n\nPablo Benavides-Herrera\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nExpectation, variance, and covariance: measuring uncertainty\n\n\n\nStat refreshers\n\n\n\nA refresher on expectation, variance, and covariance, focused on how uncertainty and dependence are quantified in time series.\n\n\n\n\n\nPablo Benavides-Herrera\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nPipes and the tidyverse mental model\n\n\n\nR\n\n\n\nHow to think in tidyverse pipelines: understanding pipes, data flow, and readable transformations in R.\n\n\n\n\n\nPablo Benavides-Herrera\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nR for Python Users\n\n\n\nR\n\n\n\nA conceptual introduction to R for students coming from a Python background, focused on mental models, data workflows, and tidyverse-style thinking.\n\n\n\n\n\nPablo Benavides-Herrera\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nRandom variables: uncertainty as an object\n\n\n\nStat refreshers\n\n\n\nA mathematically grounded refresher on random variables and distributions, focused on how uncertainty is represented and used in time series forecasting.\n\n\n\n\n\nPablo Benavides-Herrera\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\ntsibble: thinking in keys and time\n\n\n\nR\n\n\n\nAn introduction to tsibble and the core ideas of time, keys, and structure in tidyverts-based time series workflows.\n\n\n\n\n\nPablo Benavides-Herrera\n\n4 min\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n1 Time Series Forecasting with R",
    "section": "",
    "text": "Welcome to the Time Series Forecasting course at ITESO. Here you will find all the materials and resources needed for the course."
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "\n1 Time Series Forecasting with R",
    "section": "\n1.1 What you’ll learn",
    "text": "1.1 What you’ll learn\n\nBuild tidy time series with tsibble and modern data pipelines\nCreate forecasts with ETS, ARIMA, and regression-based models via fable\n\nEngineer time features and diagnose models with feasts)\nEvaluate with time-aware resampling and communicate uncertainty\nScale to multiple series and hierarchies when needed\n\n\n\n\n\n\n\nTipHow to succeed here\n\n\n\nForecasting improves with iteration: start simple, validate honestly, and refine. Keep a tight loop of fit → diagnose → evaluate → communicate."
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "\n1 Time Series Forecasting with R",
    "section": "\n1.2 Start here",
    "text": "1.2 Start here\n\n\n\n\n\n\nNote\n\n\n\nNew to the site? Use the links below to jump in.\n\n\n\nModules\nIn-class exercises"
  },
  {
    "objectID": "index.html#course-map",
    "href": "index.html#course-map",
    "title": "\n1 Time Series Forecasting with R",
    "section": "\n1.3 Course map",
    "text": "1.3 Course map\n\n\n\n1.3.1 1. Foundations\n\ntidy data + time indexes\n\ntsibble keys, gaps, and intervals\n\n1.3.2 2. Patterns & features\n\nseasonality, trend, decomposition\ndiagnostics and features (feasts)\n\n\n\n1.3.3 3. Core forecasting models\n\nETS + ARIMA\nmodel comparison and selection\n\n1.3.4 4. Regression workflows\n\ntime features, events, and covariates\nforecast reconciliation ideas\n\n\n\n1.3.5 5. Evaluation & communication\n\ntime series CV and backtesting\nuncertainty, intervals, and narratives\n\n1.3.6 6. Multiple series\n\ngrouped models\nhierarchies and reconciliation"
  },
  {
    "objectID": "index.html#quick-setup-check",
    "href": "index.html#quick-setup-check",
    "title": "\n1 Time Series Forecasting with R",
    "section": "\n1.4 Quick setup check",
    "text": "1.4 Quick setup check\nRun this once after installing packages to confirm your environment.\n\nCodelibrary(tidyverse)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\n\nsessionInfo()\n\n\n\n\n\n\n\n\nWarningCommon pitfalls\n\n\n\n\n\nData leakage: don’t let future information sneak into features.\n\nRandom CV: use time-aware resampling/backtesting.\n\nOverfitting: a more complex model isn’t automatically better."
  },
  {
    "objectID": "index.html#primary-toolchain",
    "href": "index.html#primary-toolchain",
    "title": "\n1 Time Series Forecasting with R",
    "section": "\n1.5 Primary toolchain",
    "text": "1.5 Primary toolchain\nWe’ll use a tidy workflow centered on:\n\n\ntidyverse for data wrangling\n\ntsibble for time-aware tibbles\n\nfable for modeling and forecasting\n\nfeasts for decomposition, features, and diagnostics\n\nRecommended reading: Forecasting: Principles and Practice (free online)."
  },
  {
    "objectID": "docs/more/stats/random_variables.html",
    "href": "docs/more/stats/random_variables.html",
    "title": "Random variables: uncertainty as an object",
    "section": "",
    "text": "This document is optional, but strongly recommended.\nIn forecasting, we do not only analyze observed data.\nWe analyze uncertainty about future values.\nRandom variables are the mathematical objects that allow us to make that uncertainty explicit, formal, and usable."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#from-data-to-uncertainty",
    "href": "docs/more/stats/random_variables.html#from-data-to-uncertainty",
    "title": "Random variables: uncertainty as an object",
    "section": "1 From data to uncertainty",
    "text": "1 From data to uncertainty\nSuppose you observe a time series:\n\ndaily sales\n\nhourly electricity demand\n\nmonthly inflation\n\nAt time t, the value is known.\nAt time t+1, it is not.\nThat future value is not just “unknown” — it is uncertain.\nA random variable is how we represent that uncertainty mathematically.\n\n\n\n\n\n\nTip\n\n\n\nIn forecasting, uncertainty is not a nuisance.\nIt is the object of interest."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#what-is-a-random-variable",
    "href": "docs/more/stats/random_variables.html#what-is-a-random-variable",
    "title": "Random variables: uncertainty as an object",
    "section": "2 What is a random variable?",
    "text": "2 What is a random variable?\nConceptually, a random variable represents a quantity whose value is not fixed in advance.\nIn forecasting, it typically represents:\n\na future observation,\na forecast error,\nor a simulated outcome.\n\nFormally, a random variable is a function\n\nX : \\Omega \\rightarrow \\mathbb{R},\n\nwhich assigns a real number to each possible outcome of an uncertain process.\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to work explicitly with sample spaces or probability axioms in this course.\nThe important point is that a random variable is a mathematical representation of uncertainty."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#discrete-and-continuous-random-variables",
    "href": "docs/more/stats/random_variables.html#discrete-and-continuous-random-variables",
    "title": "Random variables: uncertainty as an object",
    "section": "3 Discrete and continuous random variables",
    "text": "3 Discrete and continuous random variables\nRandom variables are commonly classified into two types.\n\n3.1 Discrete random variables\nA discrete random variable takes values in a countable set.\nExamples:\n\nnumber of customers tomorrow,\nnumber of units sold in an hour,\nnumber of failures in a week.\n\nUncertainty is described by a probability mass function (PMF):\n\np(x) = \\mathbb{P}(X = x),\n\nwith the properties\n\np(x) \\ge 0, \\qquad \\sum_x p(x) = 1.\n\n\n\n\n3.2 Continuous random variables\nA continuous random variable takes values in a continuum.\nExamples:\n\nelectricity demand,\ntemperature,\nforecast errors.\n\nUncertainty is described by a probability density function (PDF) f(x), satisfying\n\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1.\n\nProbabilities are assigned to intervals, not points:\n\n\\mathbb{P}(a \\le X \\le b) = \\int_a^b f(x)\\,dx.\n\n\n\n\n\n\n\nWarning\n\n\n\nFor continuous random variables,\n\n\\mathbb{P}(X = x) = 0\n\nfor any single value x.\nThis is not a technicality — it is a core conceptual distinction."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#distributions-as-models-not-data-summaries",
    "href": "docs/more/stats/random_variables.html#distributions-as-models-not-data-summaries",
    "title": "Random variables: uncertainty as an object",
    "section": "4 Distributions as models, not data summaries",
    "text": "4 Distributions as models, not data summaries\nA distribution describes how a random variable behaves.\nIt answers questions such as:\n\nWhich values are more likely?\nHow spread out are the possibilities?\nHow extreme outcomes behave?\n\nCrucially:\n\n\n\n\n\n\nImportant\n\n\n\nA distribution is not the same as a histogram.\n\nA histogram summarizes observed data.\nA distribution is a model for uncertainty.\n\n\n\nIn forecasting, distributions are assumptions — useful, but always assumptions."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#random-variables-in-time-series",
    "href": "docs/more/stats/random_variables.html#random-variables-in-time-series",
    "title": "Random variables: uncertainty as an object",
    "section": "5 Random variables in time series",
    "text": "5 Random variables in time series\nIn time series analysis, we typically work with a sequence of random variables\n\n\\{Y_t\\}_{t=1}^T.\n\nIt is essential to distinguish between:\n\nY_t: the random variable,\ny_t: its observed realization.\n\n\n\n\n\n\n\nTip\n\n\n\nUppercase letters denote random variables.\nLowercase letters denote observed values.\nThis distinction will appear repeatedly throughout the course.\n\n\nEach future time point corresponds to its own random variable.\nA forecast is therefore not a single number, but a distribution over possible values."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#why-this-matters-for-forecasting",
    "href": "docs/more/stats/random_variables.html#why-this-matters-for-forecasting",
    "title": "Random variables: uncertainty as an object",
    "section": "6 Why this matters for forecasting",
    "text": "6 Why this matters for forecasting\nRandom variables and distributions appear throughout the course, even when not explicitly mentioned:\n\nforecast distributions and intervals,\nresiduals and forecast errors,\nsimulation-based forecasting,\nmodel diagnostics.\n\nWhenever you reason about uncertainty, you are implicitly working with random variables.\n\n\n\n\n\n\nNote\n\n\n\nPoint forecasts are summaries.\nDistributions carry the full information."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#what-you-do-not-need-right-now",
    "href": "docs/more/stats/random_variables.html#what-you-do-not-need-right-now",
    "title": "Random variables: uncertainty as an object",
    "section": "7 What you do not need right now",
    "text": "7 What you do not need right now\nAt this stage, you do not need:\n\nprobability axioms,\nmanual probability calculations,\nanalytical derivations of densities.\n\nThose belong to a probability theory course.\nHere, random variables are tools for thinking, not objects of proof."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#where-this-shows-up-in-the-course",
    "href": "docs/more/stats/random_variables.html#where-this-shows-up-in-the-course",
    "title": "Random variables: uncertainty as an object",
    "section": "8 Where this shows up in the course",
    "text": "8 Where this shows up in the course\nThis refresher prepares you for:\n\nresidual analysis,\nforecast uncertainty and intervals,\ndiagnostic checks,\nsimulation-based methods.\n\nLater refreshers will build on this foundation to introduce:\n\nexpectation and variance,\ncovariance and correlation,\nstationarity and noise."
  },
  {
    "objectID": "docs/more/stats/random_variables.html#takeaway",
    "href": "docs/more/stats/random_variables.html#takeaway",
    "title": "Random variables: uncertainty as an object",
    "section": "9 Takeaway",
    "text": "9 Takeaway\nIf you remember only one idea, make it this:\n\n\n\n\n\n\nImportant\n\n\n\nIn forecasting, future values are random variables,\nand models describe their distributions — not certainties."
  },
  {
    "objectID": "docs/more/stats/correlation.html",
    "href": "docs/more/stats/correlation.html",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "",
    "text": "This document is optional, but strongly recommended.\nCorrelation is one of the most commonly used statistical tools — and one of the most frequently misunderstood.\nIn time series, this misunderstanding can lead to serious modeling errors."
  },
  {
    "objectID": "docs/more/stats/correlation.html#from-covariance-to-correlation",
    "href": "docs/more/stats/correlation.html#from-covariance-to-correlation",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "1 From covariance to correlation",
    "text": "1 From covariance to correlation\nRecall that the covariance between two random variables X and Y is defined as\n\n\\operatorname{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)].\n\nCovariance measures whether large values of X tend to occur with large (or small) values of Y.\nHowever, covariance depends on the scale of the variables.\nTo address this, we define the correlation coefficient:\n\n\\rho(X,Y) =\n\\frac{\\operatorname{Cov}(X,Y)}\n{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}.\n\nBy construction,\n\n-1 \\le \\rho(X,Y) \\le 1."
  },
  {
    "objectID": "docs/more/stats/correlation.html#what-correlation-actually-measures",
    "href": "docs/more/stats/correlation.html#what-correlation-actually-measures",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "2 What correlation actually measures",
    "text": "2 What correlation actually measures\nCorrelation measures linear association between two random variables.\nIf \\rho(X,Y) \\neq 0, there is evidence of a linear relationship.\nIf \\rho(X,Y) = 0, there is no linear relationship.\n\n\n\n\n\n\nNote\n\n\n\nZero correlation does not imply independence.\nIt only implies absence of linear dependence.\n\n\nThere exist dependent random variables with zero correlation."
  },
  {
    "objectID": "docs/more/stats/correlation.html#dependence-is-broader-than-correlation",
    "href": "docs/more/stats/correlation.html#dependence-is-broader-than-correlation",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "3 Dependence is broader than correlation",
    "text": "3 Dependence is broader than correlation\nTwo random variables X and Y are independent if their joint distribution factorizes:\n\nf_{X,Y}(x,y) = f_X(x) f_Y(y).\n\nIndependence is a strong condition.\nCorrelation, by contrast, captures only one specific aspect of dependence.\n\n\n\n\n\n\nImportant\n\n\n\nIndependence \\Rightarrow zero correlation\nZero correlation \\nRightarrow independence\n\n\nThis distinction is often ignored — with consequences."
  },
  {
    "objectID": "docs/more/stats/correlation.html#why-time-series-make-this-worse",
    "href": "docs/more/stats/correlation.html#why-time-series-make-this-worse",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "4 Why time series make this worse",
    "text": "4 Why time series make this worse\nIn time series, observations are not independent across time.\nWe work with a sequence of random variables \\{Y_t\\}, where dependence across time is the norm, not the exception.\nCorrelation computed directly on time series data is therefore affected by:\n\ntrends,\nseasonality,\npersistence,\nshared temporal structure."
  },
  {
    "objectID": "docs/more/stats/correlation.html#spurious-correlation",
    "href": "docs/more/stats/correlation.html#spurious-correlation",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "5 Spurious correlation",
    "text": "5 Spurious correlation\nConsider two unrelated time series, each with a strong trend.\nEven if the underlying processes are independent, their sample correlation can be large.\nThis phenomenon is known as spurious correlation.\n\n\n\n\n\n\nWarning\n\n\n\nHigh correlation does not imply meaningful dependence\nwhen time series share common structure.\n\n\nThis is one of the most common pitfalls in applied forecasting."
  },
  {
    "objectID": "docs/more/stats/correlation.html#correlation-across-time-autocorrelation",
    "href": "docs/more/stats/correlation.html#correlation-across-time-autocorrelation",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "6 Correlation across time: autocorrelation",
    "text": "6 Correlation across time: autocorrelation\nIn time series analysis, correlation most often appears as autocorrelation.\nThe lag-h autocorrelation is defined as\n\n\\rho(h) =\n\\frac{\\operatorname{Cov}(Y_t, Y_{t-h})}\n{\\operatorname{Var}(Y_t)}.\n\nAutocorrelation measures linear dependence across time, not across variables.\n\n\n\n\n\n\nTip\n\n\n\nAutocorrelation is not a nuisance.\nIt is the signal that time series models are built to capture."
  },
  {
    "objectID": "docs/more/stats/correlation.html#why-correlation-is-not-enough",
    "href": "docs/more/stats/correlation.html#why-correlation-is-not-enough",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "7 Why correlation is not enough",
    "text": "7 Why correlation is not enough\nCorrelation answers a narrow question:\n\nIs there a linear relationship?\n\nForecasting requires a broader understanding of dependence:\n\nhow long dependence persists,\nhow it decays over time,\nwhether it is stable.\n\nThese questions cannot be answered by a single correlation coefficient."
  },
  {
    "objectID": "docs/more/stats/correlation.html#where-this-shows-up-in-the-course",
    "href": "docs/more/stats/correlation.html#where-this-shows-up-in-the-course",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "8 Where this shows up in the course",
    "text": "8 Where this shows up in the course\nThis refresher is foundational for:\n\nautocorrelation and partial autocorrelation functions,\nstationarity assumptions,\nAR and MA models,\nresidual diagnostics.\n\nMisunderstanding correlation leads directly to misinterpreting these tools."
  },
  {
    "objectID": "docs/more/stats/correlation.html#what-you-do-not-need-yet",
    "href": "docs/more/stats/correlation.html#what-you-do-not-need-yet",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "9 What you do not need yet",
    "text": "9 What you do not need yet\nAt this stage, you do not need:\n\nnonlinear dependence measures,\ncopulas,\nmutual information.\n\nThose tools exist, but linear dependence is צור enough for the models covered in this course."
  },
  {
    "objectID": "docs/more/stats/correlation.html#takeaway",
    "href": "docs/more/stats/correlation.html#takeaway",
    "title": "Correlation, dependence, and time series pitfalls",
    "section": "10 Takeaway",
    "text": "10 Takeaway\n\n\n\n\n\n\nImportant\n\n\n\nCorrelation measures linear association.\nDependence is broader.\nIn time series, confusing the two leads to spurious conclusions."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html",
    "href": "docs/more/r-tools/tsibble.html",
    "title": "tsibble: thinking in keys and time",
    "section": "",
    "text": "This document is optional, but strongly recommended.\nIf tidyverse pipelines explain how data flows, tsibble explains what kind of data you are working with.\nFor time series, this distinction is essential."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#why-tsibble-exists",
    "href": "docs/more/r-tools/tsibble.html#why-tsibble-exists",
    "title": "tsibble: thinking in keys and time",
    "section": "1 Why tsibble exists",
    "text": "1 Why tsibble exists\nMost time series errors come from one simple problem:\n\nwe forget that time is a structure, not just a column.\n\nA tsibble makes time explicit, checked, and enforced.\nInstead of asking: - “do I have a date column?”\nwe ask: - “what defines time in this data?” - “what uniquely identifies a series?” - “are observations ordered and regular?”"
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#from-tibble-to-tsibble",
    "href": "docs/more/r-tools/tsibble.html#from-tibble-to-tsibble",
    "title": "tsibble: thinking in keys and time",
    "section": "2 From tibble to tsibble",
    "text": "2 From tibble to tsibble\nA tsibble is a tibble with extra rules.\nConceptually:\n\na tibble is just rows and columns\n\na tsibble is:\n\na table of time-indexed observations\nwith an explicit time variable (index)\nand optional identifiers (keys)\n\n\n\n\n\n\n\n\nNote\n\n\n\nA tsibble is still a tibble.\nAll tidyverse verbs work the same way."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#the-two-core-ideas-index-and-key",
    "href": "docs/more/r-tools/tsibble.html#the-two-core-ideas-index-and-key",
    "title": "tsibble: thinking in keys and time",
    "section": "3 The two core ideas: index and key",
    "text": "3 The two core ideas: index and key\nEvery tsibble is defined by:\n\nan index → the time variable\n\na key → what distinguishes one series from another\n\n\n3.1 Index\nThe index answers:\n\n“along which variable does time move forward?”\n\nExamples: - date - year - yearweek - yearmonth\n\n\n3.2 Key\nThe key answers:\n\n“what defines a unique time series?”\n\nExamples:\n\na store\n\na product\n\na sensor\n\na country\n\nA dataset can have: - no key (single series) - one key - multiple keys (hierarchical series)"
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#creating-a-tsibble",
    "href": "docs/more/r-tools/tsibble.html#creating-a-tsibble",
    "title": "tsibble: thinking in keys and time",
    "section": "4 Creating a tsibble",
    "text": "4 Creating a tsibble\nlibrary(tsibble)\n\ndata_ts &lt;- data |&gt;\n  as_tsibble(\n    index = date,\n    key = id\n  )\nReading this with the pipeline mental model:\n\ntake data, then declare that date is time,\nthen declare that id identifies each series.\n\n\n\n\n\n\n\nTip\n\n\n\nAlways read as_tsibble() as a declaration, not a transformation."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#what-tsibble-checks-for-you",
    "href": "docs/more/r-tools/tsibble.html#what-tsibble-checks-for-you",
    "title": "tsibble: thinking in keys and time",
    "section": "5 What tsibble checks for you",
    "text": "5 What tsibble checks for you\nWhen you create a tsibble, R checks that:\n\nthe index is ordered\neach key–index combination is unique\ntime moves forward consistently\n\nIf something is wrong, it fails early.\n\n\n\n\n\n\nWarning\n\n\n\nErrors at as_tsibble() time are a feature.\nThey prevent silent mistakes later in modeling and forecasting."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#regular-vs-irregular-time",
    "href": "docs/more/r-tools/tsibble.html#regular-vs-irregular-time",
    "title": "tsibble: thinking in keys and time",
    "section": "6 Regular vs irregular time",
    "text": "6 Regular vs irregular time\nSome series have observations at fixed intervals:\n\ndaily\nmonthly\nyearly\n\nOthers do not.\nA tsibble keeps track of this distinction.\nis_regular(data_ts)\nIf a series is regular, many models become simpler and more efficient.\n\n\n\n\n\n\nNote\n\n\n\nRegularity is a property of the index, not the values."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#missing-time-is-different-from-missing-values",
    "href": "docs/more/r-tools/tsibble.html#missing-time-is-different-from-missing-values",
    "title": "tsibble: thinking in keys and time",
    "section": "7 Missing time is different from missing values",
    "text": "7 Missing time is different from missing values\nA critical distinction in time series:\n\nmissing values → NA\nmissing time points → gaps\n\ntsibble makes this difference explicit.\ndata_ts |&gt;\n  has_gaps()\nTo fill missing time points:\ndata_ts |&gt;\n  fill_gaps()\n\n\n\n\n\n\nTip\n\n\n\nDo not confuse “no observation” with “observation equals NA”."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#tsibble-and-tidyverse-pipelines",
    "href": "docs/more/r-tools/tsibble.html#tsibble-and-tidyverse-pipelines",
    "title": "tsibble: thinking in keys and time",
    "section": "8 tsibble and tidyverse pipelines",
    "text": "8 tsibble and tidyverse pipelines\nBecause tsibble extends tibble, pipelines feel natural:\ndata |&gt;\n  as_tsibble(index = date, key = id) |&gt;\n  filter(date &gt;= yearmonth(\"2019 Jan\")) |&gt;\n  group_by(id) |&gt;\n  summarise(mean_value = mean(value, na.rm = TRUE))\nThe difference is not syntax — it is semantics."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#why-models-require-tsibbles",
    "href": "docs/more/r-tools/tsibble.html#why-models-require-tsibbles",
    "title": "tsibble: thinking in keys and time",
    "section": "9 Why models require tsibbles",
    "text": "9 Why models require tsibbles\nIn tidyverts:\n\nmodels expect a clear notion of time\nforecasts depend on index behavior\nkeys define independent series\n\nWithout a tsibble, the model cannot know:\n\nwhat “next” means\nhow many steps ahead to forecast\nwhether series are independent\n\n\n\n\n\n\n\nNote\n\n\n\ntsibble is the contract between your data and the forecasting models."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#common-mistakes",
    "href": "docs/more/r-tools/tsibble.html#common-mistakes",
    "title": "tsibble: thinking in keys and time",
    "section": "10 Common mistakes",
    "text": "10 Common mistakes\n\n10.1 Forgetting the key\nas_tsibble(data, index = date)\nThis creates a single series, even if multiple series are present.\n\n\n10.2 Using the wrong index\nUsing an identifier as time will often succeed syntactically, but fail conceptually.\n\n\n\n\n\n\nWarning\n\n\n\nIf your forecasts look strange, always check the index and key first."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#how-this-fits-in-the-course",
    "href": "docs/more/r-tools/tsibble.html#how-this-fits-in-the-course",
    "title": "tsibble: thinking in keys and time",
    "section": "11 How this fits in the course",
    "text": "11 How this fits in the course\nThroughout the course:\n\nall time series data will be stored as tsibbles\nmodels will assume valid indices and keys\ndiagnostics rely on tsibble structure\n\nIf something breaks early, it usually means the data structure is wrong — not the model.\n\n\n\n\n\n\n\nNoteFurther reading\n\n\n\n\nFor the conceptual foundations of tidy data and tibbles, see\nR for Data Science (2e) — Tibbles.\nFor joins and keys, which strongly relate to tsibble keys, see\nR for Data Science (2e) — Joins. For the full tsibble framework and design philosophy, see the\ntsibble package documentation."
  },
  {
    "objectID": "docs/more/r-tools/tsibble.html#final-takeaway",
    "href": "docs/more/r-tools/tsibble.html#final-takeaway",
    "title": "tsibble: thinking in keys and time",
    "section": "12 Final takeaway",
    "text": "12 Final takeaway\n\n\n\n\n\n\nTip\n\n\n\nIf tidyverse pipelines describe how data changes,\ntsibbles describe what kind of time-based data you have.\n\n\nGet the structure right, and the models will make sense."
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About the course",
    "section": "",
    "text": "This is the public course hub for Time Series Forecasting.\nIt is intentionally built as a working reference:\n\nlecture notes you can revisit during the semester\ncode-first examples you can reuse in projects\nexercises that build muscle memory for real forecasting workflows\n\n\n\n\n\n\n\nThis course is taught at ITESO University as part of the undergraduate and graduate curriculum in Data Science, Financial Engineering, and related programs.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you only follow one rule: always compare against a baseline.\nFancy models without a baseline are just expensive opinions."
  },
  {
    "objectID": "docs/about.html#what-this-site-is",
    "href": "docs/about.html#what-this-site-is",
    "title": "About the course",
    "section": "",
    "text": "This is the public course hub for Time Series Forecasting.\nIt is intentionally built as a working reference:\n\nlecture notes you can revisit during the semester\ncode-first examples you can reuse in projects\nexercises that build muscle memory for real forecasting workflows\n\n\n\n\n\n\n\nThis course is taught at ITESO University as part of the undergraduate and graduate curriculum in Data Science, Financial Engineering, and related programs.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you only follow one rule: always compare against a baseline.\nFancy models without a baseline are just expensive opinions."
  },
  {
    "objectID": "docs/about.html#who-its-for",
    "href": "docs/about.html#who-its-for",
    "title": "About the course",
    "section": "Who it’s for",
    "text": "Who it’s for\nYou’ll do best in this course if you can:\n\nmanipulate data frames confidently (filter, mutate, group_by, join)\nunderstand basic probability/statistics and linear regression\nstay organized with reproducible work (projects, scripts, reports)\n\n\n\n\n\n\n\nImportant\n\n\n\nThis course is not “click-and-run forecasting”.\nYou will be expected to justify assumptions, diagnose models, and communicate tradeoffs."
  },
  {
    "objectID": "docs/about.html#how-to-use-the-site",
    "href": "docs/about.html#how-to-use-the-site",
    "title": "About the course",
    "section": "How to use the site",
    "text": "How to use the site\nUse the navbar as your map:\n\nModules: the core storyline (what you must learn)\nExercises: practice and feedback loops (what makes it stick)\nMore: optional refreshers (R tooling, stats, side quests)\nAbout: this page\n\nA practical routine:\n\nRead the module page.\nRun the code (don’t just skim it).\nDo the exercise.\nWrite a short explanation of what changed and why."
  },
  {
    "objectID": "docs/about.html#tools-and-conventions",
    "href": "docs/about.html#tools-and-conventions",
    "title": "About the course",
    "section": "Tools and conventions",
    "text": "Tools and conventions\nWe work primarily in R, using the tidy ecosystem for forecasting:\n\nnative pipe |&gt; (read it as “then”)\ntidy data + functional pipelines\nforecasting as a workflow: split → fit → evaluate → iterate → communicate\n\nMathematics is included when it adds clarity, not as decoration.\n\n\n\n\n\n\nIf you come from Python, you can still follow along.\nThe mental model matters more than the syntax."
  },
  {
    "objectID": "docs/about.html#elendil-ta-ai-code-assistant",
    "href": "docs/about.html#elendil-ta-ai-code-assistant",
    "title": "About the course",
    "section": "Elendil TA (AI code assistant)",
    "text": "Elendil TA (AI code assistant)\nElendil TA is available from the navbar. Use it to:\n\ndebug errors and understand messages\ntranslate ideas into R code\ncheck your reasoning and interpretation\n\nWhat not to do:\n\npaste entire assignments and ask for the final answer\nuse it as a substitute for understanding your own results\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you can’t explain your model, you don’t own it.\nTreat AI output as draft code that must be validated."
  },
  {
    "objectID": "docs/about.html#feedback-corrections-and-reuse",
    "href": "docs/about.html#feedback-corrections-and-reuse",
    "title": "About the course",
    "section": "Feedback, corrections, and reuse",
    "text": "Feedback, corrections, and reuse\nThis site is maintained like a small open-source project.\n\nFound a typo? Open an issue.\nWant to improve an explanation? Propose a pull request.\nWant to reuse material? Please cite the repository and keep attribution."
  },
  {
    "objectID": "docs/about.html#credits-and-inspiration",
    "href": "docs/about.html#credits-and-inspiration",
    "title": "About the course",
    "section": "Credits and inspiration",
    "text": "Credits and inspiration\nThis course is strongly inspired by Forecasting: Principles and Practice (FPP3) and the broader forecasting community."
  },
  {
    "objectID": "docs/exercises/posts/prueba_2.html",
    "href": "docs/exercises/posts/prueba_2.html",
    "title": "Prueba 2",
    "section": "",
    "text": "Esta es otra prueba\n\nCode2 + 2\n\n[1] 4\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/exercises/posts/26-02-04 - ts_dcmp.html",
    "href": "docs/exercises/posts/26-02-04 - ts_dcmp.html",
    "title": "Descomposición de series de tiempo",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(fpp3)"
  },
  {
    "objectID": "docs/exercises/posts/26-02-04 - ts_dcmp.html#pkgs",
    "href": "docs/exercises/posts/26-02-04 - ts_dcmp.html#pkgs",
    "title": "Descomposición de series de tiempo",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(fpp3)"
  },
  {
    "objectID": "docs/exercises/posts/26-02-04 - ts_dcmp.html#descarga-de-datos-del-fred-usando-tidyquant",
    "href": "docs/exercises/posts/26-02-04 - ts_dcmp.html#descarga-de-datos-del-fred-usando-tidyquant",
    "title": "Descomposición de series de tiempo",
    "section": "2 Descarga de datos del FRED usando tidyquant",
    "text": "2 Descarga de datos del FRED usando tidyquant\n\n\nCode\nur_cal &lt;- tidyquant::tq_get(\n  x    = \"CAURN\",\n  get  = \"economic.data\",\n  from = \"1976-01-01\",\n  to   = \"2025-09-01\"\n)\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\nur_cal\n\n\n\n  \n\n\n\n\n2.1 exportar a csv\n\n\nCode\nur_cal |&gt; # se pone el pipe con CTRL/CMD + SHIFT + M\n  write_csv(\"ur_cal.csv\")"
  },
  {
    "objectID": "docs/exercises/posts/26-02-04 - ts_dcmp.html#descomposición-stl",
    "href": "docs/exercises/posts/26-02-04 - ts_dcmp.html#descomposición-stl",
    "title": "Descomposición de series de tiempo",
    "section": "3 Descomposición STL",
    "text": "3 Descomposición STL\n\n\nCode\nur_cal\n\n\n\n  \n\n\n\n\n3.1 Convertir la tabla a tsibble\nLa tabla es una tibble y necesitamos que sea una tsibble. Podemos convertirla utilizando la función as_tsibble(), y especificando el index (variable temporal). En caso de que la tabla contenga más de una serie de tiempo, es necesario especificar también el key (columna(s) que le indican a R cómo distinguir a cada serie).\n\n\nCode\nur_cal_tsb &lt;- ur_cal |&gt;\n  as_tsibble(index = date)\n\nur_cal_tsb\n\n\n\n  \n\n\n\nLa tsibble está mal porque R cree que la serie tiene una periodicidad diaria y no mensual. Esto se controla con el formato de la fecha. Vamos a convertir la fecha a formato año-mes (yearmonth).\n\n\nCode\nur_cal_tsb &lt;- ur_cal |&gt;\n1  mutate(date = yearmonth(date)) |&gt;\n2  as_tsibble(index = date)\n\nur_cal_tsb\n\n\n\n1\n\nConvertimos la columna date a formato yearmonth. también existen funciones como yearquarter() y yearweek().\n\n2\n\nConvertimos la tabla a tsibble, especificando la columna date como índice temporal.\n\n\n\n\n\n  \n\n\n\nAhora sí, podemos hacer la descomposición STL utilizando la función model() y la función STL().\n\n\n3.2 Descomposición STL\n\n\nCode\nur_cal_dcmp &lt;- ur_cal_tsb |&gt;\n  model(\n1    stl = STL(price, robust = TRUE)\n  )\n\nur_cal_dcmp |&gt; \n2  components() |&gt;\n3  autoplot()\n\n\n\n1\n\nEspecificamos que queremos hacer una descomposición STL de la variable price.\n\n2\n\nExtraemos los componentes de la descomposición utilizando la función components().\n\n3\n\nGraficamos los componentes utilizando la función autoplot()."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#packages",
    "href": "docs/modules/forecasting_wf_pres.html#packages",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Packages",
    "text": "Packages\nIt is recommended to load all the packages at the beginning of your file. We will be using the tidyverts ecosystem for the whole forecasting workflow.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(plotly)\n\n\n\n\n\n\n\nWarning\n\n\nDo not load unnecesary packages into your environment. It could lead to conflicts between functions and unwanted results."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#data",
    "href": "docs/modules/forecasting_wf_pres.html#data",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Data",
    "text": "Data\nWe will work with the Real Gross Domestic Product (GDP) for Mexico. The data is downloaded from FRED. The time series id is NGDPRNSAXDCMXQ.\nImport data\n\ngdp &lt;- tidyquant::tq_get(\n  x    = \"NGDPRNSAXDCMXQ\",\n  get  = \"economic.data\",\n  from = \"1997-01-01\"\n)\n\ngdp\n\n# A tibble: 115 × 3\n   symbol         date          price\n   &lt;chr&gt;          &lt;date&gt;        &lt;dbl&gt;\n 1 NGDPRNSAXDCMXQ 1997-01-01 3702398.\n 2 NGDPRNSAXDCMXQ 1997-04-01 3896084.\n 3 NGDPRNSAXDCMXQ 1997-07-01 3906063 \n 4 NGDPRNSAXDCMXQ 1997-10-01 4038358.\n 5 NGDPRNSAXDCMXQ 1998-01-01 4084304.\n 6 NGDPRNSAXDCMXQ 1998-04-01 4134899.\n 7 NGDPRNSAXDCMXQ 1998-07-01 4138200.\n 8 NGDPRNSAXDCMXQ 1998-10-01 4146841.\n 9 NGDPRNSAXDCMXQ 1999-01-01 4176243.\n10 NGDPRNSAXDCMXQ 1999-04-01 4232280.\n# ℹ 105 more rows\n\n\nWrangle data\nThere are some issues with our data:\n\nIt is loaded into a tibble object. We need to convert it to a tsibble.\n\n\n\n\n\n\n\nTip\n\n\nWe can use as_tsibble() to do so.\n\n\n\n\nOur data is quarterly, but it is loaded in a YYYY-MM-DD format. We need to change it to a YYYY QQ format.\n\n\n\n\n\n\n\nTip\n\n\nThere are some functions that help us achieve this, such as\n\nyearquarter()\nyearmonth()\nyearweek()\nyear()\n\ndepending on the time series’ period.\n\n\n\nWe will overwrite our data:\n\ngdp &lt;- gdp |&gt; \n  mutate(date = yearquarter(date)) |&gt; \n  as_tsibble(\n    index = date,\n    key   = symbol\n  )\n\ngdp\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe always need to specify the index argument, as it is our date variable.\nThe key argument is necessary whenever we have more than one time series in our data frame and is made up of one or more columns that uniquely identify each time series ."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#traintest-split",
    "href": "docs/modules/forecasting_wf_pres.html#traintest-split",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Train/Test Split",
    "text": "Train/Test Split\nWe will split our data in two sets: a training set, and a test set, in order to evaluate our forecasts’ accuracy.\n\ngdp_train &lt;- gdp |&gt; \n  filter_index(. ~ \"2021 Q4\")\n\ngdp_train\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\nFor all our variables, it is strongly recommended to follow the same notation process, and write our code using snake_case. Here, we called our data gdp, therefore, all the following variables will be called starting with gdp_1, such as gdp_train for our training set."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#visualization-and-eda",
    "href": "docs/modules/forecasting_wf_pres.html#visualization-and-eda",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Visualization and EDA",
    "text": "Visualization and EDA\nWhen performing time series analysis/forecasting, one of the first things to do is to create a time series plot.\n\np &lt;- gdp_train |&gt; \n  autoplot(price) +\n  labs(\n    title = \"Time series plot of the Real GDP for Mexico\",\n    y = \"GDP\"\n  )\n \nggplotly(p, dynamicTicks = TRUE) |&gt; \n  rangeslider()\n\n\n\n\n\n\n\n\nOur data exhibits an upward linear trend (with some economic cycles), and strong yearly seasonality.\n\n\n\nWe will explore it further with a season plot.\n\ngdp_train |&gt; \n  gg_season(price) |&gt; \n  ggplotly()\n\n\n\n\n\nTS Decomposition\n\ngdp_train |&gt; \n  model(stl = STL(price, robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()\n\n\n\n\n\n\n\n\nThe STL decomposition shows that the variance of the seasonal component has been increasing. We could try using a log transformation to counter this.\n\n\n\n\ngdp_train |&gt; \n  autoplot(log(price)) +\n  ggtitle(\"Log of the Real GDP of Mexico\")\n\n\n\n\n\n\n\n\ngdp_train |&gt; \n  model(stl = STL(log(price) ~ season(window = \"periodic\"), robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()"
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#model-specification",
    "href": "docs/modules/forecasting_wf_pres.html#model-specification",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Model Specification",
    "text": "Model Specification\nWe will fit two models to our time series: Seasonal Naïve, and the Drift model. We will also use the log transformation.\n\ngdp_fit &lt;- gdp_train |&gt; \n  model(\n    snaive = SNAIVE(log(price)),\n    drift  = RW(log(price) ~ drift())\n  )\n\n\n\n\n\n\n\nBenchmark models\n\n\nWe have four different benchmark models that we’ll use to compare against the rest of the more complex models:\n\nMean (MEAN( &lt;.y&gt; ))\nNaïve (NAIVE( &lt;.y&gt; ))\nSeasonal Naïve (SNAIVE( &lt;.y&gt; ))\nDrift (RW( &lt;.y&gt; ~ drift()))\n\nwhere &lt;.y&gt; is just a placeholder for the variable to model.\nChoose wisely which of these to use in each case, according to the exploratory analysis performed."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#residuals-diagnostics",
    "href": "docs/modules/forecasting_wf_pres.html#residuals-diagnostics",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Residuals Diagnostics",
    "text": "Residuals Diagnostics\nVisual analysis\n\ngdp_fit |&gt; \n  select(snaive) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Seasonal Naïve Model\")\n\n\n\n\n\n\ngdp_fit |&gt; \n  select(drift) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Drift Model\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nHere we expect to see:\n\nA time series with no apparent patterns (no trend and/or seasonality), with a mean close to zero.\nIn the ACF, we’d expect no lags with significant autocorrelation.\nNormally distributed residuals.\n\n\n\n\nPortmanteau tests of autocorrelation\n\ngdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box, lag = 24, dof = 0)\n\n\n  \n\n\n\n\n\n\n\n\n\nResiduals interpretation\n\n\nBoth models produce sub optimal residuals:\n\nThe SNAIVE correctly detects the seasonality, however, its residuals are still autocorrelated. Moreover, the residuals are not normally distributed.\nThe drift model doesn’t account for the seasonality, and their distribution is a little bit skewed.\n\nHence, we will perform our forecasts using the bootstrapping method.\n\n\n\nWe can compute some error metrics on the training set using the accuracy() function:\n\ngdp_train_accu &lt;- accuracy(gdp_fit) |&gt; \n  arrange(MAPE)\ngdp_train_accu |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE)\n\n\n  \n\n\n\n\n\n\n\n\n\nThe accuracy() function\n\n\nThe accuracy() function can be used to compute error metrics in the training data, or in the test set. What differs is the data that is given to it:\n\nFor the training metrics, you need to use the mable (the table of models, that we usually store in _fit).\nFor the forecasting error metrics, we need the fable (the forecasts table, usually stored as _fc or _fcst), and the complete set of data (both the training and test set together).\n\n\n\n\n\n\n\n\n\n\n\nFor this analysis, we are focusing on the MAPE2 metric. The drift model (2.47%) seems to have a better fit with the training set than the snaive model (3.24%)."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#modeling-using-decomposition",
    "href": "docs/modules/forecasting_wf_pres.html#modeling-using-decomposition",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Modeling using decomposition",
    "text": "Modeling using decomposition\nWe will perform a forecast using decomposition, to see if we can improve our results so far.\n\ngdp_fit_dcmp &lt;- gdp_train |&gt; \n      model(\n        stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n      )\n\ngdp_fit_dcmp\n\n\n  \n\n\n\n\n\n\n\n\n\nNote on decomposition_model()\n\n\nRemember, when using decomposition models, we need to do the following:\n\nSpecify what type of decomposition we want to use and customize it as needed.\nFit a model for the seasonally adjusted data; season_adjust.\nFit a model for the seasonal component. R uses a SNAIVE() model by default to model the seasonality. If you wish to model it using a different model, you have specify it.\n\n\nThe name of the seasonal component depends on the type of seasonality present in the time series. If it has a yearly seasonality, the component is called season_year. It could also be called season_week, season_day, and so on.\n\n\n\n\nWe can join this new model with the models we trained before. This way we can have them all in the same mable.\n\ngdp_fit &lt;- gdp_fit |&gt; \n  left_join(gdp_fit_dcmp)\n\nResiduals diagnostics\n\ngdp_fit |&gt; \n  accuracy() |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)\n\n\n  \n\n\n\n\ngdp_fit |&gt; \n  select(stlf) |&gt; \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\ngdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box)\n\n\n  \n\n\n\n\n\n\nThe MAPE seems to improve with this decomposition model. Also, the residual diagnostics do not show any seasonality present in them. However, the residuals are still autocorrelated, as the Ljung-Box test suggests."
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#forecasting-on-the-test-set",
    "href": "docs/modules/forecasting_wf_pres.html#forecasting-on-the-test-set",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Forecasting on the test set",
    "text": "Forecasting on the test set\nOnce we have our models, we can produce forecasts. We will forecast our test data and check our forecasts’ performance.\n\ngdp_fc &lt;- gdp_fit |&gt; \n  forecast(h = gdp_h_fc) \n\ngdp_fc\n\n\n  \n\n\n\n\ngdp_fc |&gt; \n  autoplot(gdp) +\n  facet_wrap(~.model, ncol = 1)\n\n\n\n\n\n\ngdp_fc |&gt; \n  filter(.model == \"stlf\") |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\nWe now estimate the forecast errors:\n\ngdp_fc |&gt; \n  accuracy(gdp) |&gt; \n  select(.model:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)"
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#forecasting-the-future",
    "href": "docs/modules/forecasting_wf_pres.html#forecasting-the-future",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Forecasting the future",
    "text": "Forecasting the future\nWe now refit our model using the whole dataset. We will only model the STL decomposition model, because the other two didn’t get a strong fit.\n\ngdp_fit2 &lt;- gdp |&gt; \n  model(\n    stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n  )\ngdp_fit2\n\n\n  \n\n\n\n\ngdp_fc_fut &lt;- gdp_fit2 |&gt; \n  forecast(h = gdp_h_fc)\ngdp_fc_fut\n\n\n  \n\n\ngdp_fc_fut |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\n\n# save(gdp_fc_fut, file = \"equipo1.RData\")"
  },
  {
    "objectID": "docs/modules/forecasting_wf_pres.html#footnotes",
    "href": "docs/modules/forecasting_wf_pres.html#footnotes",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will make it very convenient when calling your variables. RStudio will display all the options starting with gdp_. We will usually use the following suffixes:\n\n\n_train: training set\n\n_fit: the mable (table of models)\n\n_aug: the augmented table with fitted values and residuals\n\n_dcmp: for the dable (decomposition table), containing the components and the seasonally adjusted series of a TS decomposition.\n\n_fc or _fcst: for the fable (forecasts table) that has our forecasts. \n\n\n\n\n\nThe Mean Absolute Percentage Error is a percentage error metric widely used in professional environments.\nLet\n\ne_t = y_t - \\hat{y}_t\n\nbe the error or residual.\nThen the MAPE would be computed as\n\nMAPE = \\frac{1}{T}\\sum_{t=1}^T|\\frac{e_t}{y_t}|\n."
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#packages",
    "href": "docs/modules/forecasting_workflow.html#packages",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n0.1 Packages",
    "text": "0.1 Packages\nIt is recommended to load all the packages at the beginning of your file. We will be using the tidyverts ecosystem for the whole forecasting workflow.\n\nCodelibrary(tidyverse)\nlibrary(fpp3)\nlibrary(plotly)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not load unnecesary packages into your environment. It could lead to conflicts between functions and unwanted results.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#data",
    "href": "docs/modules/forecasting_workflow.html#data",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.1 Data",
    "text": "1.1 Data\nWe will work with the Real Gross Domestic Product (GDP) for Mexico. The data is downloaded from FRED. The time series id is NGDPRNSAXDCMXQ.\n\n1.1.1 Import data\n\nCodegdp &lt;- tidyquant::tq_get(\n  x    = \"NGDPRNSAXDCMXQ\",\n  get  = \"economic.data\",\n  from = \"1997-01-01\"\n)\n\ngdp\n\n# A tibble: 115 × 3\n   symbol         date          price\n   &lt;chr&gt;          &lt;date&gt;        &lt;dbl&gt;\n 1 NGDPRNSAXDCMXQ 1997-01-01 3702398.\n 2 NGDPRNSAXDCMXQ 1997-04-01 3896084.\n 3 NGDPRNSAXDCMXQ 1997-07-01 3906063 \n 4 NGDPRNSAXDCMXQ 1997-10-01 4038358.\n 5 NGDPRNSAXDCMXQ 1998-01-01 4084304.\n 6 NGDPRNSAXDCMXQ 1998-04-01 4134899.\n 7 NGDPRNSAXDCMXQ 1998-07-01 4138200.\n 8 NGDPRNSAXDCMXQ 1998-10-01 4146841.\n 9 NGDPRNSAXDCMXQ 1999-01-01 4176243.\n10 NGDPRNSAXDCMXQ 1999-04-01 4232280.\n# ℹ 105 more rows\n\n\n\n1.1.2 Wrangle data\nThere are some issues with our data:\n\nIt is loaded into a tibble object. We need to convert it to a tsibble.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWe can use as_tsibble() to do so.\n\n\n\n\nOur data is quarterly, but it is loaded in a YYYY-MM-DD format. We need to change it to a YYYY QQ format.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are some functions that help us achieve this, such as\n\nyearquarter()\nyearmonth()\nyearweek()\nyear()\n\ndepending on the time series’ period.\n\n\n\nWe will overwrite our data:\n\nCodegdp &lt;- gdp |&gt; \n  mutate(date = yearquarter(date)) |&gt; \n  as_tsibble(\n    index = date,\n    key   = symbol\n  )\n\ngdp\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nWe always need to specify the index argument, as it is our date variable.\nThe key argument is necessary whenever we have more than one time series in our data frame and is made up of one or more columns that uniquely identify each time series .",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#traintest-split",
    "href": "docs/modules/forecasting_workflow.html#traintest-split",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.2 Train/Test Split",
    "text": "1.2 Train/Test Split\nWe will split our data in two sets: a training set, and a test set, in order to evaluate our forecasts’ accuracy.\n\nCodegdp_train &lt;- gdp |&gt; \n  filter_index(. ~ \"2021 Q4\")\n\ngdp_train\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor all our variables, it is strongly recommended to follow the same notation process, and write our code using snake_case. Here, we called our data gdp, therefore, all the following variables will be called starting with gdp_1, such as gdp_train for our training set.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#visualization-and-eda",
    "href": "docs/modules/forecasting_workflow.html#visualization-and-eda",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.3 Visualization and EDA",
    "text": "1.3 Visualization and EDA\nWhen performing time series analysis/forecasting, one of the first things to do is to create a time series plot.\n\nCodep &lt;- gdp_train |&gt; \n  autoplot(price) +\n  labs(\n    title = \"Time series plot of the Real GDP for Mexico\",\n    y = \"GDP\"\n  )\n \nggplotly(p, dynamicTicks = TRUE) |&gt; \n  rangeslider()\n\n\n\n\n\n\n\n\n\n\n\nOur data exhibits an upward linear trend (with some economic cycles), and strong yearly seasonality.\n\n\n\nWe will explore it further with a season plot.\n\nCodegdp_train |&gt; \n  gg_season(price) |&gt; \n  ggplotly()\n\nWarning: `gg_season()` was deprecated in feasts 0.4.2.\nℹ Please use `ggtime::gg_season()` instead.\n\n\n\n\n\n\n\n1.3.1 TS Decomposition\n\nCodegdp_train |&gt; \n  model(stl = STL(price, robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()\n\n\n\n\n\n\n\n\n\n\n\nThe STL decomposition shows that the variance of the seasonal component has been increasing. We could try using a log transformation to counter this.\n\n\n\n\nCodegdp_train |&gt; \n  autoplot(log(price)) +\n  ggtitle(\"Log of the Real GDP of Mexico\")\n\n\n\n\n\n\n\n\nCodegdp_train |&gt; \n  model(stl = STL(log(price) ~ season(window = \"periodic\"), robust = TRUE)) |&gt; \n  components() |&gt; \n  autoplot() |&gt; \n  ggplotly()",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#model-specification",
    "href": "docs/modules/forecasting_workflow.html#model-specification",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.4 Model Specification",
    "text": "1.4 Model Specification\nWe will fit two models to our time series: Seasonal Naïve, and the Drift model. We will also use the log transformation.\n\nCodegdp_fit &lt;- gdp_train |&gt; \n  model(\n    snaive = SNAIVE(log(price)),\n    drift  = RW(log(price) ~ drift())\n  )\n\n\n\n\n\n\n\n\nTipBenchmark models\n\n\n\n\n\nWe have four different benchmark models that we’ll use to compare against the rest of the more complex models:\n\nMean (MEAN( &lt;.y&gt; ))\nNaïve (NAIVE( &lt;.y&gt; ))\nSeasonal Naïve (SNAIVE( &lt;.y&gt; ))\nDrift (RW( &lt;.y&gt; ~ drift()))\n\nwhere &lt;.y&gt; is just a placeholder for the variable to model.\nChoose wisely which of these to use in each case, according to the exploratory analysis performed.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#residuals-diagnostics",
    "href": "docs/modules/forecasting_workflow.html#residuals-diagnostics",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.5 Residuals Diagnostics",
    "text": "1.5 Residuals Diagnostics\n\n1.5.1 Visual analysis\n\nCodegdp_fit |&gt; \n  select(snaive) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Seasonal Naïve Model\")\n\n\n\n\n\n\nCodegdp_fit |&gt; \n  select(drift) |&gt; \n  gg_tsresiduals() +\n  ggtitle(\"Residuals Diagnostics for the Drift Model\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere we expect to see:\n\nA time series with no apparent patterns (no trend and/or seasonality), with a mean close to zero.\nIn the ACF, we’d expect no lags with significant autocorrelation.\nNormally distributed residuals.\n\n\n\n\n\n1.5.2 Portmanteau tests of autocorrelation\n\nCodegdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box, lag = 24, dof = 0)\n\n\n  \n\n\n\n\n\n\n\n\n\nCautionResiduals interpretation\n\n\n\nBoth models produce sub optimal residuals:\n\nThe SNAIVE correctly detects the seasonality, however, its residuals are still autocorrelated. Moreover, the residuals are not normally distributed.\nThe drift model doesn’t account for the seasonality, and their distribution is a little bit skewed.\n\nHence, we will perform our forecasts using the bootstrapping method.\n\n\nWe can compute some error metrics on the training set using the accuracy() function:\n\nCodegdp_train_accu &lt;- accuracy(gdp_fit) |&gt; \n  arrange(MAPE)\ngdp_train_accu |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE)\n\n\n  \n\n\n\n\n\n\n\n\n\nTipThe accuracy() function\n\n\n\n\n\nThe accuracy() function can be used to compute error metrics in the training data, or in the test set. What differs is the data that is given to it:\n\nFor the training metrics, you need to use the mable (the table of models, that we usually store in _fit).\nFor the forecasting error metrics, we need the fable (the forecasts table, usually stored as _fc or _fcst), and the complete set of data (both the training and test set together).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor this analysis, we are focusing on the MAPE2 metric. The drift model (2.47%) seems to have a better fit with the training set than the snaive model (3.24%).",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#modeling-using-decomposition",
    "href": "docs/modules/forecasting_workflow.html#modeling-using-decomposition",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.6 Modeling using decomposition",
    "text": "1.6 Modeling using decomposition\nWe will perform a forecast using decomposition, to see if we can improve our results so far.\n\nCodegdp_fit_dcmp &lt;- gdp_train |&gt; \n      model(\n        stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n      )\n\ngdp_fit_dcmp\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteNote on decomposition_model()\n\n\n\n\n\nRemember, when using decomposition models, we need to do the following:\n\nSpecify what type of decomposition we want to use and customize it as needed.\nFit a model for the seasonally adjusted data; season_adjust.\nFit a model for the seasonal component. R uses a SNAIVE() model by default to model the seasonality. If you wish to model it using a different model, you have specify it.\n\n\nThe name of the seasonal component depends on the type of seasonality present in the time series. If it has a yearly seasonality, the component is called season_year. It could also be called season_week, season_day, and so on.\n\n\n\n\nWe can join this new model with the models we trained before. This way we can have them all in the same mable.\n\nCodegdp_fit &lt;- gdp_fit |&gt; \n  left_join(gdp_fit_dcmp)\n\nJoining with `by = join_by(symbol)`\n\n\n\n1.6.1 Residuals diagnostics\n\nCodegdp_fit |&gt; \n  accuracy() |&gt; \n  select(symbol:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)\n\n\n  \n\n\n\n\nCodegdp_fit |&gt; \n  select(stlf) |&gt; \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nCodegdp_fit |&gt; \n  augment() |&gt; \n  features(.innov, ljung_box)\n\n\n  \n\n\n\n\n\n\n\n\n\nThe MAPE seems to improve with this decomposition model. Also, the residual diagnostics do not show any seasonality present in them. However, the residuals are still autocorrelated, as the Ljung-Box test suggests.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#forecasting-on-the-test-set",
    "href": "docs/modules/forecasting_workflow.html#forecasting-on-the-test-set",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.7 Forecasting on the test set",
    "text": "1.7 Forecasting on the test set\nOnce we have our models, we can produce forecasts. We will forecast our test data and check our forecasts’ performance.\n\nCodegdp_fc &lt;- gdp_fit |&gt; \n  forecast(h = gdp_h_fc) \n\ngdp_fc\n\n\n  \n\n\n\n\nCodegdp_fc |&gt; \n  autoplot(gdp) +\n  facet_wrap(~.model, ncol = 1)\n\n\n\n\n\n\nCodegdp_fc |&gt; \n  filter(.model == \"stlf\") |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\nWe now estimate the forecast errors:\n\nCodegdp_fc |&gt; \n  accuracy(gdp) |&gt; \n  select(.model:.type, MAPE, RMSE, MAE, MASE) |&gt; \n  arrange(MAPE)",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#forecasting-the-future",
    "href": "docs/modules/forecasting_workflow.html#forecasting-the-future",
    "title": "The Forecasting Workflow using fable\n",
    "section": "\n1.8 Forecasting the future",
    "text": "1.8 Forecasting the future\nWe now refit our model using the whole dataset. We will only model the STL decomposition model, because the other two didn’t get a strong fit.\n\nCodegdp_fit2 &lt;- gdp |&gt; \n  model(\n    stlf = decomposition_model(\n          STL(log(price) ~ season(window = \"periodic\"), robust = TRUE),\n          RW(season_adjust ~ drift())\n        )\n  )\ngdp_fit2\n\n\n  \n\n\n\n\nCodegdp_fc_fut &lt;- gdp_fit2 |&gt; \n  forecast(h = gdp_h_fc)\ngdp_fc_fut\n\n\n  \n\n\nCodegdp_fc_fut |&gt; \n  autoplot(gdp)\n\n\n\n\n\n\n\n\nCode# save(gdp_fc_fut, file = \"equipo1.RData\")",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/forecasting_workflow.html#footnotes",
    "href": "docs/modules/forecasting_workflow.html#footnotes",
    "title": "The Forecasting Workflow using fable\n",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will make it very convenient when calling your variables. RStudio will display all the options starting with gdp_. We will usually use the following suffixes:\n\n\n_train: training set\n\n_fit: the mable (table of models)\n\n_aug: the augmented table with fitted values and residuals\n\n_dcmp: for the dable (decomposition table), containing the components and the seasonally adjusted series of a TS decomposition.\n\n_fc or _fcst: for the fable (forecasts table) that has our forecasts. \n\n\n↩︎\n\n\nThe Mean Absolute Percentage Error is a percentage error metric widely used in professional environments.\nLet\n\ne_t = y_t - \\hat{y}_t\n\nbe the error or residual.\nThen the MAPE would be computed as\n\nMAPE = \\frac{1}{T}\\sum_{t=1}^T|\\frac{e_t}{y_t}|\n.\n\n\n\n\n↩︎",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.4 The Forecasting Workflow"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Mean\n\n\\hat y_{T+1\\mid T}=\\tfrac{1}{T}\\sum_{i=1}^T y_i\n\n\n\n\n\n\nNaïve\n\n\\hat y_{T+1\\mid T}=y_T"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-1",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-1",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Mean\n\n\n\n\n\nNaïve\n\n\nExponential Smoothing\n\n\\hat y_{T+1\\mid T}=\\alpha y_T +  \\alpha(1-\\alpha)y_{T-1} + \\ldots\n\n\n\\alpha \\approx 1: naïve-like\n\\alpha \\approx 0: mean-like\n\n\n\n\n\nExponential smoothing methods are still relatively simple: they’re simply weighted averages from historical data.\n\nHowever, these forecasting methods are widely used in practice, and they can be very effective.\n\nThe exponential smoothing method is a compromise between the mean and naïve methods. It uses all historical data, but it assigns exponentially decreasing weights to older observations.\n\nIn the mean method, all observations are weighted equally (all have the same importance), while in the naïve method, only the most recent observation is used for forecasting. (we ignore all previous observations).\n\nThe smoothing parameter \\alpha controls the rate of decrease:\n\nwhen \\alpha is close to 1, the method behaves like the naïve method, giving more weight to recent observations;\nwhen \\alpha is close to 0, it behaves like the mean method, giving more equal weight to all observations."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-2",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-2",
    "title": "Exponential smoothing",
    "section": "",
    "text": "\\hat{y}_{T+1 | T}= \\alpha y_{T} + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^{2} y_{T-2}  + \\ldots\n\nwhere 0\\leq \\alpha \\leq1 is the smoothing parameter.\n\n\n\nTable 1: Weights for different values of \\alpha\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\alpha = 0.2\n\\alpha = 0.4\n\\alpha = 0.6\n\\alpha = 0.8\n\n\n\n\ny_t\n0.2000\n0.4000\n0.6000\n0.8000\n\n\ny_{t-1}\n0.1600\n0.2400\n0.2400\n0.1600\n\n\ny_{t-2}\n0.1280\n0.1440\n0.0960\n0.0320\n\n\ny_{t-3}\n0.1024\n0.0864\n0.0384\n0.0064\n\n\ny_{t-4}\n0.0819\n0.0518\n0.0154\n0.0013\n\n\ny_{t-5}\n0.0655\n0.0311\n0.0061\n0.0003\n\n\n\n\n\n\n\n\n\n\\alpha can be thought of as the memory of the time series: The smaller the value of \\alpha, the longer the memory (i.e., the more past observations are taken into account).\nConversely, a larger value of \\alpha means a shorter memory, with more emphasis on recent observations. See Table 1 for some examples."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#simple-exponential-smoothing-ses",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#simple-exponential-smoothing-ses",
    "title": "Exponential smoothing",
    "section": "Simple exponential smoothing (SES)",
    "text": "Simple exponential smoothing (SES)\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t \\\\\n\\text{Smoothing equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\n\\end{aligned}\n\nwhere \\ell_t is the level at time t.\n\n\n\n\n\n\nSES has a flat forecast function, so it is appropriate for data with no trend or seasonal pattern."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-algerias-exports",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-algerias-exports",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Algeria’s exports",
    "text": "Example: Forecasting Algeria’s exports\n\nalgeria_economy &lt;- global_economy |&gt;\n  filter(Country == \"Algeria\")\n  \nalgeria_economy |&gt; \n  autoplot(Exports)\n\n\n\n\n\n\n\n\n\nalg_fit &lt;- algeria_economy |&gt;\n  model(\n1    SES = ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    Naive = NAIVE(Exports)\n  )\n\nalg_fc &lt;- alg_fit |&gt;\n  forecast(h = 5)\n\n\n1\n\nWe specify trend(\"N\") and season(\"N\") to indicate that we want a simple exponential smoothing (SES) model, which assumes no trend and no seasonality. The model will estimate the smoothing parameter \\alpha automatically.\n\n\n\n\n\n\n\n\n\n\nObtaining the report() of a model\n\n\n\nalg_fit |&gt; \n  select(SES) |&gt; \n1  report()\n\n\n1\n\nThe report() function allows us to see a model’s report (the time series modeled, the model used, the estimated parameters, and more). It needs a 1 \\times 1 dimension mable1.\n\n\n\n\nSeries: Exports \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.8399875 \n\n  Initial states:\n   l[0]\n 39.539\n\n  sigma^2:  35.6301\n\n     AIC     AICc      BIC \n446.7154 447.1599 452.8968"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-4",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-4",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Comparing the SES and Naive forecasts:"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#holts-linear-trend",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#holts-linear-trend",
    "title": "Exponential smoothing",
    "section": "Holt’s linear trend",
    "text": "Holt’s linear trend\n\nWe can extend SES models to allow our forecasts to include trend in the data. We need to add a new smoothing parameter \\beta^*, and its corresponding smoothing equation:\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t + hb_t \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t - \\ell_{t-1}) + (1-\\beta^*)b_{t-1}\n\\end{aligned}\n\nwhere b_t is the growth (or slope) at time t.\n\n\n\n\n\n\nWhen to use Holt’s linear trend method\n\n\n\nHolt’s linear trend method is appropriate for data with a linear trend but no seasonal pattern.\nThe proper benchmark method to compare against is the drift method.\n\n\n\n\n\nLet’s see an example using Holt’s linear trend method to forecast Brazil’s population."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-1",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-1",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population",
    "text": "Example: Forecasting Brazil’s population"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-2",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-2",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population",
    "text": "Example: Forecasting Brazil’s population\n\nbra_fit &lt;- bra_economy |&gt; \n  model(\n    Holt  = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\n\nbra_fit"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-3",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-3",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population",
    "text": "Example: Forecasting Brazil’s population\n\nbra_fit &lt;- bra_economy |&gt; \n  model(\n    Holt  = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    Drift = RW(Pop ~ drift())\n  )\n\nbra_fit"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-4",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-4",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population",
    "text": "Example: Forecasting Brazil’s population\n\nbra_fit &lt;- bra_economy |&gt; \n  model(\n    Holt  = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    Drift = RW(Pop ~ drift())\n  )\n\nbra_fit |&gt;  \n  select(Holt) |&gt;  \n  report()\n\nSeries: Pop \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.9998999 \n\n  Initial states:\n     l[0]     b[0]\n 70.06297 2.132884\n\n  sigma^2:  0.0021\n\n      AIC      AICc       BIC \n-115.2553 -114.1014 -104.9531"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-5",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-5",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population",
    "text": "Example: Forecasting Brazil’s population\n\nbra_fit &lt;- bra_economy |&gt; \n  model(\n1    Holt  = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    Drift = RW(Pop ~ drift())\n  )\n\nbra_fit |&gt;  \n  select(Holt) |&gt;  \n  report()\n\nbra_fc &lt;- bra_fit |&gt;  \n  forecast(h = 15)\n\nbra_fc |&gt; \n  autoplot(bra_economy, level = NULL) +\n  labs(title = \"Brazilian population\",\n       y = \"Millions\") +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\n\n1\n\nWe specify trend(\"A\") to indicate that we want a linear trend. The model will estimate the smoothing parameters \\alpha and \\beta^* automatically.\n\n\n\n\n\n\n\n\n\n\n\nSeries: Pop \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.9998999 \n\n  Initial states:\n     l[0]     b[0]\n 70.06297 2.132884\n\n  sigma^2:  0.0021\n\n      AIC      AICc       BIC \n-115.2553 -114.1014 -104.9531"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#damped-trend",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#damped-trend",
    "title": "Exponential smoothing",
    "section": "Damped trend",
    "text": "Damped trend\n\n\nHolt’s linear trend method assume that the trend will continue indefinitely at the same rate. However, in many real-world scenarios, this assumption may not hold true. This methods tend to overestimate (or underestimate) long-term forecasts when the trend is strong.\nWe can include a damping parameter \\phi, which reduces the trend over time.\n\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t +  (\\phi + \\phi^2 + \\ldots + \\phi^h) b_t \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha y_t + (1 - \\alpha) (\\ell_{t-1} + \\phi b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*)\\phi b_{t-1}\n\\end{aligned}\n\nwhere 0 &lt; \\phi &lt; 12 is the damping parameter.\n\n\n\n\n\n\nWhat would happen if \\phi = 1? What about if \\phi = 0?\n\n\n\nIf \\phi = 1, the model reduces to Holt’s linear trend method, meaning the trend continues indefinitely at the same rate.\nIf \\phi = 0, the trend component is completely eliminated, and the model behaves like simple exponential smoothing (SES), where forecasts are based solely on the level component without any trend influence."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-continued",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-continued",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population (continued)",
    "text": "Example: Forecasting Brazil’s population (continued)\n\nbra_economy |&gt; \n  model(\n    Holt   = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-continued-1",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-continued-1",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population (continued)",
    "text": "Example: Forecasting Brazil’s population (continued)\n\nbra_economy |&gt; \n  model(\n    Holt   = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    Damped = ETS(Pop ~ error(\"A\") + trend(\"Ad\", phi = 0.9) + season(\"N\"))\n  )"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-continued-2",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-brazils-population-continued-2",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Brazil’s population (continued)",
    "text": "Example: Forecasting Brazil’s population (continued)\n\nbra_economy |&gt; \n  model(\n    Holt   = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n1    Damped = ETS(Pop ~ error(\"A\") + trend(\"Ad\", phi = 0.9) + season(\"N\"))\n  ) |&gt; \n  forecast(h = 15) |&gt; \n  autoplot(bra_economy, level = NULL) +\n  labs(title = \"Brazilian population\",\n       y = \"Millions\") +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\n\n1\n\nWe specify trend(\"Ad\") to indicate that we want a damped trend, and phi = 0.9 sets the damping parameter to 0.9. We could also let the model estimate \\phi automatically by omitting the phi argument."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#holt-winters-method",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#holt-winters-method",
    "title": "Exponential smoothing",
    "section": "Holt-Winters method",
    "text": "Holt-Winters method\nHW - Additive\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t + hb_t + s_{t+h-m(k+1)} \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha (y_t - s_{t-m}) + (1 - \\alpha) (\\ell_{t-1} + b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*) b_{t-1} \\\\\n\\text{Seasonal equation} \\quad & s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m}\n\\end{aligned}\n\nwhere s_t is the seasonal component at time t, m is the period of the seasonality3, and k = \\lfloor (h-1)/m \\rfloor.\nHW - Multiplicative\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = (\\ell_t + hb_t) s_{t+h-m(k+1)} \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha \\frac{y_t}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*) b_{t-1} \\\\\n\\text{Seasonal equation} \\quad & s_t = \\gamma \\frac{y_t}{\\ell_{t-1} + b_{t-1}} + (1-\\gamma)s_{t-m}\n\\end{aligned}\n\n\n\n\n\n\n\nWhen to use Holt-Winters methods\n\n\n\nHolt-Winters methods are appropriate for data with a trend and seasonal pattern.\nUse an additive model when the seasonal fluctuations are roughly constant over time.\nUse a multiplicative model when the seasonal variation increase or decrease over time.\nThe proper benchmark method to compare against is the seasonal naïve method. for seasonal data.\n\nIf the data contains both trend and seasonality, then A decomposition model using STL4 + Drift5 + SNAIVE6 is often a strong competitor."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Australian holiday trips",
    "text": "Example: Forecasting Australian holiday trips\n\naus_holidays &lt;- tourism |&gt; \n  filter(Purpose == \"Holiday\") |&gt;\n  summarise(Trips = sum(Trips))\n\naus_holidays"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-1",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-1",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Australian holiday trips",
    "text": "Example: Forecasting Australian holiday trips\n\naus_holidays &lt;- tourism |&gt; \n  filter(Purpose == \"Holiday\") |&gt;\n  summarise(Trips = sum(Trips))\n\naus_holidays |&gt;\n  autoplot(Trips)"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-2",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-2",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Australian holiday trips",
    "text": "Example: Forecasting Australian holiday trips\n\naus_fit &lt;- aus_holidays |&gt; \n  model(\n    Additive       = ETS(Trips ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n  )"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-3",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-3",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Australian holiday trips",
    "text": "Example: Forecasting Australian holiday trips\n\naus_fit &lt;- aus_holidays |&gt; \n  model(\n    Additive       = ETS(Trips ~ error(\"A\") + trend(\"A\") + season(\"A\")),\n    Multiplicative = ETS(Trips ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n  )"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-4",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-4",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Australian holiday trips",
    "text": "Example: Forecasting Australian holiday trips\n\n\n\n\n\n\nThe tidy() function for models\n\n\n\naus_fit |&gt; \n1  tidy()\n\n\n1\n\nThe tidy() function allows us to see the estimated parameters of each model in a tidy table."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-5",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-australian-holiday-trips-5",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting Australian holiday trips",
    "text": "Example: Forecasting Australian holiday trips\n\naus_fc &lt;- aus_fit |&gt; \n  forecast(h = \"3 years\")\n\n\naus_fc |&gt; \n  autoplot(aus_holidays, level = NULL) + xlab(\"Year\") +\n  labs(\n    title   = \"Forecasting Australian holiday trips using Holt-Winters\",\n    y       = \"Overnight trips (millions)\",\n    caption = \"Can you spot any differences between both forecasts?\"\n  ) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  guides(colour = guide_legend(title = \"Forecast\"))"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#holt-winters-damped-method",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#holt-winters-damped-method",
    "title": "Exponential smoothing",
    "section": "Holt-Winters’ damped method",
    "text": "Holt-Winters’ damped method\n\n\nSimilar to Holt’s linear trend method, we can also include a damping parameter \\phi in the Holt-Winters method to reduce the trend over time.\n\nThis can be done on both additive or multiplicative seasonal models.\n\nA configuration that has proven to be robust and accurate in practice is to use a multiplicative seasonal component with a damped trend.\n\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = [\\ell_t +(\\phi + \\phi^2 + \\ldots + \\phi^h)b_t] s_{t+h-m(k+1)} \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha \\frac{y_t}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*) \\phi b_{t-1} \\\\\n\\text{Seasonal equation} \\quad & s_t = \\gamma \\frac{y_t}{\\ell_{t-1} + \\phi b_{t-1}} + (1-\\gamma)s_{t-m}\n\\end{aligned}"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-daily-pedestrian-traffic",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-daily-pedestrian-traffic",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting daily pedestrian traffic",
    "text": "Example: Forecasting daily pedestrian traffic\n\nsth_cross_ped &lt;- pedestrian |&gt;\n  filter(Date &gt;= \"2016-07-01\",\n         Sensor == \"Southern Cross Station\") |&gt;\n  index_by(Date) |&gt;\n  summarise(Count = sum(Count)/1000)\n\nsth_cross_ped |&gt;\n  filter(Date &lt;= \"2016-07-31\") |&gt;\n  model(\n    hw = ETS(Count ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n  ) |&gt;\n  forecast(h = \"2 weeks\") |&gt;\n  autoplot(sth_cross_ped |&gt; filter(Date &lt;= \"2016-08-14\")) +\n  labs(title = \"Daily traffic: Southern Cross\",\n       y=\"Pedestrians ('000)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe setup ETS(y ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) is often a robust choice for seasonal data with trend."
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-daily-pedestrian-traffic-continued",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#example-forecasting-daily-pedestrian-traffic-continued",
    "title": "Exponential smoothing",
    "section": "Example: Forecasting daily pedestrian traffic (continued)",
    "text": "Example: Forecasting daily pedestrian traffic (continued)\n\nped_fit &lt;- pedestrian |&gt; \n  filter(\n    Date &gt;= \"2016-07-01\",\n    Sensor != \"Birrarung Marr\"\n  ) |&gt; \n  index_by(Date) |&gt;\n  group_by_key() |&gt; \n  summarise(Count = sum(Count)/1000) |&gt; \n  model(\n    ETS_auto       = ETS(Count),                       \n    ets_with_trend = ETS(Count ~ trend(c(\"A\", \"Ad\"))), \n  )\n\nped_fit"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#section-5",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#section-5",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Table 2: ETS component combinations (trend × seasonal)\n\n\n\n\n\n\n\n\n\n\n\n\nTrend component\nN (None)\nA (Additive)\nM (Multiplicative)\n\n\n\n\nN (None)\n(N,N),\n(N,A)\n(N,M)\n\n\nA (Additive)\n(A,N),\n(A,A)\n(A,M)\n\n\nA_d (Additive damped)\n(A_d,N),\n(A_d, A)\n(A_d,M)\n\n\n\n\n\n\n\n\n\n\nTable 3: Names of some popular ETS models\n\n\n\n\n\n\nNotation\nMethod\n\n\n\n\n(N,N)\nSimple Exponential Smoothing (SES)\n\n\n(A,N)\nHolt’s Linear Trend\n\n\n(A_d,N)\nAdditive damped Trend\n\n\n(A,A)\nHolt-Winters’ Additive\n\n\n(A,M)\nHolt-Winters’ Multiplicative\n\n\n(A_d,M)\nHolt-Winters’ damped"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets_pres.html#footnotes",
    "href": "docs/modules/module_2/01_ets/ets_pres.html#footnotes",
    "title": "Exponential smoothing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(i.e., a mable containing only one model and one time series.)\nIn practice, we restrict 0.8 \\leq \\phi \\leq 0.98 because the damping effect would be too great for smaller values than 0.8 and almost non distinguishable from a linear trend for greater values than 0.98.\ne.g., m=4 for quarterly data, m=12 for monthly data, …\nas the decomposition method\nfor the seasonally adjusted series\nfor the seasonal component"
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html",
    "href": "docs/modules/module_2/01_ets/ets.html",
    "title": "Exponential smoothing",
    "section": "",
    "text": "Exponential smoothing methods are still relatively simple: they’re simply weighted averages from historical data.\n\nHowever, these forecasting methods are widely used in practice, and they can be very effective.\n\nThe exponential smoothing method is a compromise between the mean and naïve methods. It uses all historical data, but it assigns exponentially decreasing weights to older observations.\n\nIn the mean method, all observations are weighted equally (all have the same importance), while in the naïve method, only the most recent observation is used for forecasting. (we ignore all previous observations).\n\nThe smoothing parameter \\alpha controls the rate of decrease:\n\nwhen \\alpha is close to 1, the method behaves like the naïve method, giving more weight to recent observations;\nwhen \\alpha is close to 0, it behaves like the mean method, giving more equal weight to all observations.\n\\hat{y}_{T+1 | T}= \\alpha y_{T} + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^{2} y_{T-2}  + \\ldots\nwhere 0\\leq \\alpha \\leq1 is the smoothing parameter.",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#simple-exponential-smoothing-ses",
    "href": "docs/modules/module_2/01_ets/ets.html#simple-exponential-smoothing-ses",
    "title": "Exponential smoothing",
    "section": "1.1 Simple exponential smoothing (SES)",
    "text": "1.1 Simple exponential smoothing (SES)\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t \\\\\n\\text{Smoothing equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\n\\end{aligned}\n\nwhere \\ell_t is the level at time t.\n\n\n\n\n\n\nSES has a flat forecast function, so it is appropriate for data with no trend or seasonal pattern.\n\n\n\n\n1.1.0.1 Example: Forecasting Algeria’s exports\n\n\nCode\nalgeria_economy &lt;- global_economy |&gt;\n  filter(Country == \"Algeria\")\n  \nalgeria_economy |&gt; \n  autoplot(Exports)\n\n\n\n\n\n\n\n\n\n\n\nCode\nalg_fit &lt;- algeria_economy |&gt;\n  model(\n1    SES = ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    Naive = NAIVE(Exports)\n  )\n\nalg_fc &lt;- alg_fit |&gt;\n  forecast(h = 5)\n\n\n\n1\n\nWe specify trend(\"N\") and season(\"N\") to indicate that we want a simple exponential smoothing (SES) model, which assumes no trend and no seasonality. The model will estimate the smoothing parameter \\alpha automatically.\n\n\n\n\n\n\n\n\n\n\nTipBenchmark methods for SES\n\n\n\nThe mean and naïve methods are typically the best fit as benchmark methods when using SES.\n\n\n\n\n\n\n\n\nNoteObtaining the report() of a model\n\n\n\n\n\n\n\nCode\nalg_fit |&gt; \n  select(SES) |&gt; \n1  report()\n\n\n\n1\n\nThe report() function allows us to see a model’s report (the time series modeled, the model used, the estimated parameters, and more). It needs a 1 \\times 1 dimension mable1.\n\n\n\n\nSeries: Exports \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.8399875 \n\n  Initial states:\n   l[0]\n 39.539\n\n  sigma^2:  35.6301\n\n     AIC     AICc      BIC \n446.7154 447.1599 452.8968 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the SES and Naive forecasts:",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#methods-with-trend-1",
    "href": "docs/modules/module_2/01_ets/ets.html#methods-with-trend-1",
    "title": "Exponential smoothing",
    "section": "1.2 Methods with trend",
    "text": "1.2 Methods with trend\n\n1.2.1 Holt’s linear trend\n\nWe can extend SES models to allow our forecasts to include trend in the data. We need to add a new smoothing parameter \\beta^*, and its corresponding smoothing equation:\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t + hb_t \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t - \\ell_{t-1}) + (1-\\beta^*)b_{t-1}\n\\end{aligned}\n\nwhere b_t is the growth (or slope) at time t.\n\n\n\n\n\n\nTipWhen to use Holt’s linear trend method\n\n\n\n\n\n\nHolt’s linear trend method is appropriate for data with a linear trend but no seasonal pattern.\nThe proper benchmark method to compare against is the drift method.\n\n\n\n\n\nLet’s see an example using Holt’s linear trend method to forecast Brazil’s population.\n\n\n1.2.1.1 Example: Forecasting Brazil’s population\n\n\nCode\nbra_economy &lt;- global_economy |&gt; \n  filter(Code == \"BRA\") |&gt; \n  mutate(Pop = Population / 1e6)\n\nbra_economy |&gt; \n  autoplot(Pop)\n\n\n\n\n\n\n\n\n\n\n\nCode\nbra_fit &lt;- bra_economy |&gt; \n  model(\n1    Holt  = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    Drift = RW(Pop ~ drift())\n  )\n\nbra_fit |&gt;  \n  select(Holt) |&gt;  \n  report()\n\nbra_fc &lt;- bra_fit |&gt;  \n  forecast(h = 15)\n\nbra_fc |&gt; \n  autoplot(bra_economy, level = NULL) +\n  labs(title = \"Brazilian population\",\n       y = \"Millions\") +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\n\n\n1\n\nWe specify trend(\"A\") to indicate that we want a linear trend. The model will estimate the smoothing parameters \\alpha and \\beta^* automatically.\n\n\n\n\n\n\n\n\n\n\n\nSeries: Pop \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.9998999 \n\n  Initial states:\n     l[0]     b[0]\n 70.06297 2.132884\n\n  sigma^2:  0.0021\n\n      AIC      AICc       BIC \n-115.2553 -114.1014 -104.9531 \n\n\n\n\n\n1.2.2 Damped trend\n\n\nHolt’s linear trend method assume that the trend will continue indefinitely at the same rate. However, in many real-world scenarios, this assumption may not hold true. This methods tend to overestimate (or underestimate) long-term forecasts when the trend is strong.\nWe can include a damping parameter \\phi, which reduces the trend over time.\n\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t +  (\\phi + \\phi^2 + \\ldots + \\phi^h) b_t \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha y_t + (1 - \\alpha) (\\ell_{t-1} + \\phi b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*)\\phi b_{t-1}\n\\end{aligned}\n\nwhere 0 &lt; \\phi &lt; 12 is the damping parameter.\n\n\n\n\n\n\nCautionWhat would happen if \\phi = 1? What about if \\phi = 0?\n\n\n\n\n\n\nIf \\phi = 1, the model reduces to Holt’s linear trend method, meaning the trend continues indefinitely at the same rate.\nIf \\phi = 0, the trend component is completely eliminated, and the model behaves like simple exponential smoothing (SES), where forecasts are based solely on the level component without any trend influence.\n\n\n\n\n\n1.2.2.1 Example: Forecasting Brazil’s population (continued)\n\n\nCode\nbra_economy |&gt; \n  model(\n    Holt   = ETS(Pop ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n1    Damped = ETS(Pop ~ error(\"A\") + trend(\"Ad\", phi = 0.9) + season(\"N\"))\n  ) |&gt; \n  forecast(h = 15) |&gt; \n  autoplot(bra_economy, level = NULL) +\n  labs(title = \"Brazilian population\",\n       y = \"Millions\") +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\n\n\n1\n\nWe specify trend(\"Ad\") to indicate that we want a damped trend, and phi = 0.9 sets the damping parameter to 0.9. We could also let the model estimate \\phi automatically by omitting the phi argument.",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#methods-with-seasonality-1",
    "href": "docs/modules/module_2/01_ets/ets.html#methods-with-seasonality-1",
    "title": "Exponential smoothing",
    "section": "1.3 Methods with seasonality",
    "text": "1.3 Methods with seasonality\n\n1.3.1 Holt-Winters method\n\n1.3.1.1 HW - Additive\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = \\ell_t + hb_t + s_{t+h-m(k+1)} \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha (y_t - s_{t-m}) + (1 - \\alpha) (\\ell_{t-1} + b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*) b_{t-1} \\\\\n\\text{Seasonal equation} \\quad & s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m}\n\\end{aligned}\n\nwhere s_t is the seasonal component at time t, m is the period of the seasonality3, and k = \\lfloor (h-1)/m \\rfloor.\n\n\n1.3.1.2 HW - Multiplicative\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = (\\ell_t + hb_t) s_{t+h-m(k+1)} \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha \\frac{y_t}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*) b_{t-1} \\\\\n\\text{Seasonal equation} \\quad & s_t = \\gamma \\frac{y_t}{\\ell_{t-1} + b_{t-1}} + (1-\\gamma)s_{t-m}\n\\end{aligned}\n\n\n\n\n\n\n\nTipWhen to use Holt-Winters methods\n\n\n\n\n\n\nHolt-Winters methods are appropriate for data with a trend and seasonal pattern.\nUse an additive model when the seasonal fluctuations are roughly constant over time.\nUse a multiplicative model when the seasonal variation increase or decrease over time.\nThe proper benchmark method to compare against is the seasonal naïve method. for seasonal data.\n\nIf the data contains both trend and seasonality, then A decomposition model using STL4 + Drift5 + SNAIVE6 is often a strong competitor.\n\n\n\n\n\n\n\n1.3.1.3 Example: Forecasting Australian holiday trips\nWe will forecast the number of holiday trips in Australia using additive and multiplicative Holt-Winters methods.\n\n\nCode\naus_holidays &lt;- tourism |&gt; \n  filter(Purpose == \"Holiday\") |&gt;\n  summarise(Trips = sum(Trips))\n\naus_holidays |&gt;\n  autoplot(Trips)\n\n\n\n1\n\nWe specify error(\"A\") and season(\"A\") to indicate that we want an additive Holt-Winters model. We will usually set the error() according to the season() type.\n\n2\n\nWe specify error(\"M\") and season(\"M\") to indicate that we want a multiplicative Holt-Winters model. Both models will estimate the smoothing parameters \\alpha, \\beta^*, and \\gamma automatically.\n\n\n\n\n\n\n\n\n\n\n\nCode\naus_fit &lt;- aus_holidays |&gt; \n  model(\n1    Additive       = ETS(Trips ~ error(\"A\") + trend(\"A\") + season(\"A\")),\n2    Multiplicative = ETS(Trips ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n  )\n  \naus_fc &lt;- aus_fit |&gt; \n  forecast(h = \"3 years\")\n\naus_fc |&gt; \n  autoplot(aus_holidays, level = NULL) + xlab(\"Year\") +\n  labs(\n    title   = \"Forecasting Australian holiday trips using Holt-Winters\",\n    y       = \"Overnight trips (millions)\",\n    caption = \"Can you spot any differences between both forecasts?\"\n  ) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteThe tidy() function for models\n\n\n\n\n\n\n\nCode\naus_fit |&gt; \n1  tidy()\n\n\n\n1\n\nThe tidy() function allows us to see the estimated parameters of each model in a tidy table.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n1.3.2 Holt-Winters’ damped method\n\n\nSimilar to Holt’s linear trend method, we can also include a damping parameter \\phi in the Holt-Winters method to reduce the trend over time.\n\nThis can be done on both additive or multiplicative seasonal models.\n\nA configuration that has proven to be robust and accurate in practice is to use a multiplicative seasonal component with a damped trend.\n\n\n\n\\begin{aligned}\n\\text{Forecast equation} \\quad & \\hat{y}_{t+h|t} = [\\ell_t +(\\phi + \\phi^2 + \\ldots + \\phi^h)b_t] s_{t+h-m(k+1)} \\\\\n\\text{Level equation} \\quad & \\ell_t = \\alpha \\frac{y_t}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\\n\\text{Trend equation} \\quad & b_t = \\beta^*(\\ell_t-\\ell_{t-1}) + (1-\\beta^*) \\phi b_{t-1} \\\\\n\\text{Seasonal equation} \\quad & s_t = \\gamma \\frac{y_t}{\\ell_{t-1} + \\phi b_{t-1}} + (1-\\gamma)s_{t-m}\n\\end{aligned}\n\n\n1.3.2.1 Example: Forecasting daily pedestrian traffic\n\n\nCode\nsth_cross_ped &lt;- pedestrian |&gt;\n  filter(Date &gt;= \"2016-07-01\",\n         Sensor == \"Southern Cross Station\") |&gt;\n  index_by(Date) |&gt;\n  summarise(Count = sum(Count)/1000)\n\nsth_cross_ped |&gt;\n  filter(Date &lt;= \"2016-07-31\") |&gt;\n  model(\n    hw = ETS(Count ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n  ) |&gt;\n  forecast(h = \"2 weeks\") |&gt;\n  autoplot(sth_cross_ped |&gt; filter(Date &lt;= \"2016-08-14\")) +\n  labs(title = \"Daily traffic: Southern Cross\",\n       y=\"Pedestrians ('000)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe setup ETS(y ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) is often a robust choice for seasonal data with trend.",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#automatic-ets-selection-1",
    "href": "docs/modules/module_2/01_ets/ets.html#automatic-ets-selection-1",
    "title": "Exponential smoothing",
    "section": "1.4 Automatic ETS selection",
    "text": "1.4 Automatic ETS selection\n\n\nUsing fable, we can automatically select the best ETS model for our data using the ETS() function. We achieve this by not specifying the error, trend, or seasonal components.\nYou can also get a semi-automatic selection by specifying some components and leaving others to be selected automatically, or by providing a set of possible values for a component.\nAutomatic selection is especially useful when we have multiple time series to model, as it allows us to fit different ETS models to each series.\n\n\n\n1.4.0.1 Example: Forecasting daily pedestrian traffic (continued)\n\n\nCode\nped_fit &lt;- pedestrian |&gt; \n  filter(\n    Date &gt;= \"2016-07-01\",\n    Sensor != \"Birrarung Marr\"\n  ) |&gt; \n  index_by(Date) |&gt;\n  group_by_key() |&gt; \n  summarise(Count = sum(Count)/1000) |&gt; \n  model(\n1    ETS_auto       = ETS(Count),\n2    ets_with_trend = ETS(Count ~ trend(c(\"A\", \"Ad\"))),\n  )\n\n3ped_fit\n\n\n\n1\n\nLeaving the right-hand side of the formula empty allows fable to automatically select the best ETS model for each time series.\n\n2\n\nHere, we specify that the trend component can be either additive or damped, while the error and seasonal components will be selected automatically.\n\n3\n\nThe output shows the selected model for each sensor.",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#the-lineup-of-exponential-smoothing-methods-1",
    "href": "docs/modules/module_2/01_ets/ets.html#the-lineup-of-exponential-smoothing-methods-1",
    "title": "Exponential smoothing",
    "section": "1.5 The lineup of exponential smoothing methods",
    "text": "1.5 The lineup of exponential smoothing methods\n\n\n\nTable 2: ETS component combinations (trend × seasonal)\n\n\n\n\n\n\n\n\n\n\n\n\nTrend component\nN (None)\nA (Additive)\nM (Multiplicative)\n\n\n\n\nN (None)\n(N,N),\n(N,A)\n(N,M)\n\n\nA (Additive)\n(A,N),\n(A,A)\n(A,M)\n\n\nA_d (Additive damped)\n(A_d,N),\n(A_d, A)\n(A_d,M)\n\n\n\n\n\n\n\n\n\n\nTable 3: Names of some popular ETS models\n\n\n\n\n\n\nNotation\nMethod\n\n\n\n\n(N,N)\nSimple Exponential Smoothing (SES)\n\n\n(A,N)\nHolt’s Linear Trend\n\n\n(A_d,N)\nAdditive damped Trend\n\n\n(A,A)\nHolt-Winters’ Additive\n\n\n(A,M)\nHolt-Winters’ Multiplicative\n\n\n(A_d,M)\nHolt-Winters’ damped",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#in-summary-1",
    "href": "docs/modules/module_2/01_ets/ets.html#in-summary-1",
    "title": "Exponential smoothing",
    "section": "1.6 In summary",
    "text": "1.6 In summary\n\nExponential smoothing methods are a family of forecasting methods that use weighted averages of past observations to make forecasts.\nThe weights decrease exponentially for older observations, controlled by smoothing parameters.\nDifferent configurations of ETS models can be used to handle various data patterns, including trend and seasonality.\nThe choice of model components (error(c(\"A\", \"M\")), trend(c(\"N\", \"A\", \"Ad\")), seasonality(c(\"N\", \"A\", \"M\"))) should be based on the characteristics of the data.\n\nThat is, we choose the model by viewing the time plot.\n\nAutomatic ETS selection can be a powerful tool for fitting models to multiple time series efficiently.",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_2/01_ets/ets.html#footnotes",
    "href": "docs/modules/module_2/01_ets/ets.html#footnotes",
    "title": "Exponential smoothing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(i.e., a mable containing only one model and one time series.)↩︎\nIn practice, we restrict 0.8 \\leq \\phi \\leq 0.98 because the damping effect would be too great for smaller values than 0.8 and almost non distinguishable from a linear trend for greater values than 0.98.↩︎\ne.g., m=4 for quarterly data, m=12 for monthly data, …↩︎\nas the decomposition method↩︎\nfor the seasonally adjusted series↩︎\nfor the seasonal component↩︎",
    "crumbs": [
      "2. Adding ETS and ARIMA filters",
      "2.1 Exponential Smoothing"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#is-this-a-time-series",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#is-this-a-time-series",
    "title": "Time Series Forecasting",
    "section": "Is this a time series?",
    "text": "Is this a time series?\n\n\n\n\n\n\nIf we focus solely on the regular plot, we wouldn’t have any time series. However, when we map each variable through time, we now have multiple time series: one for each country regarding life exp., GDP per capita, and population."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#stocks",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#stocks",
    "title": "Time Series Forecasting",
    "section": "Stocks",
    "text": "Stocks\n\n\n\n\n\n\n\n\n\n\n\nStocks, FX, … are all time series"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#cryptos",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#cryptos",
    "title": "Time Series Forecasting",
    "section": "Cryptos",
    "text": "Cryptos\nAny variable that is measured through time is a time series."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-1",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-1",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)\n    A--&gt;B(Those who can't predict the future)"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-2",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-2",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)\n    A--&gt;B(Those who can't predict the future)\n    A--&gt;C(Those who don't know that they can't predict the future)\n\n\n\n\n\n\nNo one, except for sorcerers and wizards, can predict the future."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-4",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-4",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Dr. Strange didn’t have the Time stone. He was using a high-tech gamer PC to run millions of simulations."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#eclipses",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#eclipses",
    "title": "Time Series Forecasting",
    "section": "Eclipses",
    "text": "Eclipses\nWe can predict eclipses with complete certainty."
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-5",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-5",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "It’s not so easy to predict stock prices\n\n\n\nOther variables can’t be predicted that easily. What does it depend on?"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#electricity-demand",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#electricity-demand",
    "title": "Time Series Forecasting",
    "section": "Electricity Demand",
    "text": "Electricity Demand"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#beer-production-forecasts",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#beer-production-forecasts",
    "title": "Time Series Forecasting",
    "section": "Beer Production Forecasts",
    "text": "Beer Production Forecasts\n\n\n\n\n\n\nCan you observe any strange patterns?"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#which-us-employment-forecast-works-best",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#which-us-employment-forecast-works-best",
    "title": "Time Series Forecasting",
    "section": "Which US Employment forecast works best?",
    "text": "Which US Employment forecast works best?\n\n\n\n\n\n\nForecasting US Retail Employment using the Drift method\n\n\n\n\n\n\n\n\nForecasting US Retail Employment using the Seasonal Naive method"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro_pres.html#section-7",
    "href": "docs/modules/module_1/00_intro/intro_pres.html#section-7",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Forecasting US Retail Employment using ARIMA"
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html",
    "href": "docs/modules/module_1/00_intro/intro.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "If we focus solely on the regular plot, we wouldn’t have any time series. However, when we map each variable through time, we now have multiple time series: one for each country regarding life exp., GDP per capita, and population.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#is-this-a-time-series",
    "href": "docs/modules/module_1/00_intro/intro.html#is-this-a-time-series",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "If we focus solely on the regular plot, we wouldn’t have any time series. However, when we map each variable through time, we now have multiple time series: one for each country regarding life exp., GDP per capita, and population.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#stocks",
    "href": "docs/modules/module_1/00_intro/intro.html#stocks",
    "title": "Time Series Forecasting",
    "section": "Stocks",
    "text": "Stocks\n\n\n\n\n\n\n\n\n\n\n\nStocks, FX, … are all time series",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#cryptos",
    "href": "docs/modules/module_1/00_intro/intro.html#cryptos",
    "title": "Time Series Forecasting",
    "section": "Cryptos",
    "text": "Cryptos\n\n\nCrypto currencies are also time series\n\n\nAny variable that is measured through time is a time series.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-2",
    "href": "docs/modules/module_1/00_intro/intro.html#section-2",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "flowchart LR\n    A(There are two types of Data Scientists)\n    A--&gt;B(Those who can't predict the future)\n    A--&gt;C(Those who don't know that they can't predict the future)\n\n\n\n\n\n\n\nNo one, except for sorcerers and wizards, can predict the future.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-3",
    "href": "docs/modules/module_1/00_intro/intro.html#section-3",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "What was Dr. Strange doing here?",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-4",
    "href": "docs/modules/module_1/00_intro/intro.html#section-4",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Dr. Strange didn’t have the Time stone. He was using a high-tech gamer PC to run millions of simulations.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#eclipses",
    "href": "docs/modules/module_1/00_intro/intro.html#eclipses",
    "title": "Time Series Forecasting",
    "section": "Eclipses",
    "text": "Eclipses\n\n\nWe can predict eclipses with complete certainty.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-5",
    "href": "docs/modules/module_1/00_intro/intro.html#section-5",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "It’s not so easy to predict stock prices\n\n\n\n\nOther variables can’t be predicted that easily. What does it depend on?",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#electricity-demand",
    "href": "docs/modules/module_1/00_intro/intro.html#electricity-demand",
    "title": "Time Series Forecasting",
    "section": "Electricity Demand",
    "text": "Electricity Demand",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#beer-production-forecasts",
    "href": "docs/modules/module_1/00_intro/intro.html#beer-production-forecasts",
    "title": "Time Series Forecasting",
    "section": "Beer Production Forecasts",
    "text": "Beer Production Forecasts\n\n\n\n\n\n\nCan you observe any strange patterns?",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#which-us-employment-forecast-works-best",
    "href": "docs/modules/module_1/00_intro/intro.html#which-us-employment-forecast-works-best",
    "title": "Time Series Forecasting",
    "section": "Which US Employment forecast works best?",
    "text": "Which US Employment forecast works best?\n\n\n\n\n\n\nForecasting US Retail Employment using the Drift method\n\n\n\n\n\n\n\n\nForecasting US Retail Employment using the Seasonal Naive method",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/00_intro/intro.html#section-7",
    "href": "docs/modules/module_1/00_intro/intro.html#section-7",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Forecasting US Retail Employment using ARIMA",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.0 Introduction"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section",
    "title": "Time Series Decomposition",
    "section": "",
    "text": "All these time series have different shapes, patterns, and so on. When modeling them, we need to take these characteristics into account. We seek to understand the underlying patterns in the data to make better forecasts."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#ts-patterns",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#ts-patterns",
    "title": "Time Series Decomposition",
    "section": "TS Patterns",
    "text": "TS Patterns\nTime series can have distinct patterns:\n\nTrend: A long-term increase/decrease in the data.\nSeasonal: Fluctuations in the time series with a fixed and known period1.\nCycles: More commonly known as “Business cycles”, refer to rises and falls that are not of a fixed frequency2.\nChanges in variability: Changes in the spread of the data over time, i. e., an increase/decrease in the variance as the level of the series increases/decreases."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#components-of-a-time-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#components-of-a-time-series",
    "title": "Time Series Decomposition",
    "section": "Components of a Time Series",
    "text": "Components of a Time Series\nA time series can be decomposed into the following components:\n\nSeasonal component (S): The repeating short-term cycle in the series.\nTrend-cycle component (T): The long-term progression of the series.\nResidual component (R): The residuals or “noise” left after removing the seasonal and trend-cycle components."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#log-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#log-transformations",
    "title": "Time Series Decomposition",
    "section": "Log transformations",
    "text": "Log transformations\n\nSeries in levelsLog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations and adjustments help us simplify the patterns in our data, and can improve our forecasts’ accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog transformations are often useful when the data presents an increasing/decreasing variation with the level of the series.\nLog transformations are very interpretable: changes in a log value are percent changes on the original scale."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#box-cox-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#box-cox-transformations",
    "title": "Time Series Decomposition",
    "section": "Box-Cox transformations",
    "text": "Box-Cox transformations\n\nw_t= \\begin{cases}\\log \\left(y_t\\right) & \\text { if } \\lambda=0 \\\\ \\left(\\operatorname{sign}\\left(y_t\\right)\\left|y_t\\right|^\\lambda-1\\right) / \\lambda & \\text { otherwise }\\end{cases}\n\n\nIn a Box-Cox transformation, the log is always a natural logarithm. The other case is just a power transformation with scaling.\n\nWhat happens when \\lambda = 1?\n\n\n\n\n\n\nYou should choose a value of \\lambda that makes the size of the seasonal variation the same throughout the series."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#how-can-we-choose-the-value-of-lambda",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#how-can-we-choose-the-value-of-lambda",
    "title": "Time Series Decomposition",
    "section": "How can we choose the value of \\lambda?",
    "text": "How can we choose the value of \\lambda?\nWe can use the guerrero feature to choose an optimal lambda.\n\naus_production |&gt; \n  features(Gas, features = guerrero)\n\n# A tibble: 1 × 1\n  lambda_guerrero\n            &lt;dbl&gt;\n1           0.110"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#calendar-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#calendar-adjustments",
    "title": "Time Series Decomposition",
    "section": "Calendar adjustments",
    "text": "Calendar adjustments\n\nClosing price and volumeMonthly aggregationMonthly total and mean and volume\n\n\n\n\n\n\n\n\n\n\n\ngoogle_month &lt;- google |&gt; \n  index_by(month = yearmonth(date)) |&gt; \n  summarise(\n    trading_days = n(),\n    monthly_volume = sum(volume),\n    mean_volume = mean(volume)\n  )\n\ngoogle_month\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of trading days in a month can vary due to weekends and holidays, and not because of any economic reason.\nUsing the monthly total volume can be misleading, as months with more trading days will naturally have higher total volumes.\nUsing the mean volume per trading day helps to standardize the data, making it easier to compare across months."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#section-1",
    "title": "Time Series Decomposition",
    "section": "",
    "text": "Is the Mexican economy really that similar Australia’s economy? Is Iceland’s economy really that small?\n\n\nA greater GDP can be interpreted as having a larger economy, and a better life standard, but this is not always the case.\nComparing GDP across countries with different population sizes can be misleading.\nGDP is often used to measure the economic performance of a country, but it doesn’t account for population size.\nThe higher the population, the higher the GDP tends to be, simply because there are more people contributing to the economy.\nA more meaningful comparison can be made by looking at GDP per capita, which divides the GDP by the population size."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#population-adjustments-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#population-adjustments-1",
    "title": "Time Series Decomposition",
    "section": "Population adjustments",
    "text": "Population adjustments\n\nPopulationGDP per capita\n\n\n\n\n\n\n\n\nThe population sizes of these countries are very different.\n\n\n\n\n\n\n\n\n\n\nGDP per capita provides a more accurate representation of the economic well-being of individuals in a country.\nIt is clear now that Iceland and Australia have a much higher GDP per capita compared to Mexico, indicating a higher standard of living for its residents."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustments",
    "title": "Time Series Decomposition",
    "section": "Inflation adjustments",
    "text": "Inflation adjustments\n\nInflation is the rate at which the general level of prices for goods and services is rising, and subsequently, purchasing power is falling.\nTo make meaningful comparisons of economic data over time, it is essential to adjust for inflation.\nThis adjustment is typically done using a price index, such as the Consumer Price Index (CPI). In Mexico, the National Consumer Price Index (INPC) is used. INEGI provides this data."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-formula",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-formula",
    "title": "Time Series Decomposition",
    "section": "Inflation adjustment formula",
    "text": "Inflation adjustment formula\nInflation adjustment formula\n\nx_t = \\frac{y_t}{z_t} * z_{2010}\n\n\nwhere:\n\ny_t is the original value at time t (nominal value).\nz_t is the price index at time t (e.g., INPC).\nz_{2010} is the price index in the base year (2010 in this case).\nx_t is the inflation-adjusted value at time t (real value)."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-example",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#inflation-adjustment-example",
    "title": "Time Series Decomposition",
    "section": "Inflation adjustment example",
    "text": "Inflation adjustment example\n\nNominal valuesReal valuesNominal vs. Real values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naus_economy &lt;- global_economy |&gt;\n  filter(Code == \"AUS\")\n\n\nprint_retail &lt;- print_retail |&gt; \n  left_join(aus_economy, by = \"Year\") |&gt;\n  mutate(Adjusted_turnover = Turnover / CPI)"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#types-of-decompositions",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#types-of-decompositions",
    "title": "Time Series Decomposition",
    "section": "Types of Decompositions",
    "text": "Types of Decompositions\n\nA decomposition splits the time series into its underlying components:\n\nTrend-cycle\nSeasonal pattern(s)\n\nAnd what’s left of it we simply call it a “remainder component”.\nIn general, there are two types of decompositions:\n\nAdditive decomposition\n\ny_t = T_t + S_t + R_t\n\nMultiplicative decomposition\n\ny_t = T_t \\times S_t \\times R_t \\\\\n\n\nWhich one should you use?\n\n\n\nIf the seasonal variation is roughly constant over time, use an additive decomposition.\nIf the seasonal variation increases or decreases with the level of the series, use a multiplicative decomposition.\nIf you’re unsure, you can try both and see which one provides a better fit.\n\nA multiplicative decomposition is equivalent to an additive decomposition of the log-transformed series:\n\ny_t = T_t \\times S_t \\times R_t\n\nis equivalent to\n\n\\log(y_t) = \\log(T_t) + \\log(S_t) + \\log(R_t)"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#seasonally-adjusted-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#seasonally-adjusted-series",
    "title": "Time Series Decomposition",
    "section": "Seasonally adjusted series",
    "text": "Seasonally adjusted series\n\nOne use of decomposition is to obtain a seasonally adjusted series, which is the original series with the seasonal component removed.\nSeasonally adjusted series can be useful for:\n\nIdentifying and analyzing the trend-cycle component without the influence of seasonal fluctuations.\nMaking comparisons across different time periods without seasonal effects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor an additive decomposition, the seasonally adjusted series is given by: \ny_t - S_t\n\nFor a multiplicative decomposition, the seasonally adjusted series is given by: \n\\frac{y_t}{S_t}"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "Classical decomposition",
    "text": "Classical decomposition\n\nThe classical decomposition is a simple method for decomposing time series data. It assumes that the components of the time series are additive or multiplicative.\n\n\nThe trend-cycle component is estimated using a moving average.\n\nAn m order moving average is given by:\n\n\\hat{T}_{t}=\\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}\n\nwhere k = (m-1)/23.\n\nThen, the seasonal component is estimated by averaging the detrended values for each season.\nFinally, the remainder component is obtained by subtracting the trend-cycle and seasonal components from the original series."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-1",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model()"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-2",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-2",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model(\n    classical = classical_decomposition() \n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-3",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-3",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model(\n    classical = classical_decomposition(y, type = \"additive\") \n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-4",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-4",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail |&gt; \n  model(\n    classical = classical_decomposition(y, type = \"additive\") \n  ) |&gt; \n  components()"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-5",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-5",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\n1mexretail_dcmp &lt;- mexretail |&gt;\n2  model(\n3    classical = classical_decomposition(y, type = \"additive\")\n  ) |&gt; \n4  components()\n\n5mexretail_dcmp\n\n\n1\n\nWe start with our original tsibble.\n\n2\n\nInside the model() function, we specify the type of models we want to use.\n\n3\n\nIn any model used, the first thing we need to specify is our forecast variable. Then, depending on the model used, we can specify additional parameters. The model() function yields a mable4, which is a table that contains the fitted models for each time series in the tsibble.\n\n4\n\nThe components() function is used to extract the components of the decomposition (trend-cycle, seasonal, and remainder) from the fitted models in the mable. It also provides the seasonally adjusted series.\n\n5\n\nFinally, we store the result."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-7",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#example-of-a-classical-decomposition-7",
    "title": "Time Series Decomposition",
    "section": "Example of a classical decomposition",
    "text": "Example of a classical decomposition\n\nmexretail_dcmp |&gt; \n  autoplot()"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#problems-of-using-a-classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#problems-of-using-a-classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "Problems of using a Classical decomposition",
    "text": "Problems of using a Classical decomposition\n\nThe trend-cycle component is not estimated at the beginning and end of the series. This can be problematic if you want to forecast the series.\nIt also tends to over-smooth rises and falls.\nIt assumes that the seasonal component is constant over time, which may not be the case in many real-world scenarios.\nIt is not robust to outliers, which can significantly affect the estimates of the components.\n\n\n\n\n\n\n\n\nWarning\n\n\nIt is not recommended to use classical decomposition for forecasting because of these issues."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-decomposition-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-decomposition-1",
    "title": "Time Series Decomposition",
    "section": "STL decomposition",
    "text": "STL decomposition\n\nSTL (Seasonal and Trend decomposition using Loess) is a more advanced method for decomposing time series data5. It uses locally weighted regression (loess) to estimate the trend-cycle and seasonal components. STL is more flexible than classical decomposition and can handle changes in the seasonal component over time.\n\n\nIt can handle any type of seasonality (not just fixed periods).\nIt can handle changes in the seasonal component over time.\nIt is robust to outliers.\nIt can be used for forecasting.\nIt provides a way to control the smoothness of the trend and seasonal components through parameters.\n\n\n\n\n\n\n\n\nSTL cannot automatically handle calendar or holiday variations.\nIt only provides methods for additive models. If your data has multiplicative seasonality, you should log-transform the data before applying STL."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\n\nmexretail_stl &lt;- mexretail |&gt;                                \n  model(                                                      \n    stl = STL(y)\n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-1",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-1",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\n\nmexretail_stl &lt;- mexretail |&gt;                                \n  model(                                                      \n    stl = STL(y ~ trend(window = NULL))\n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-2",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-2",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\n\nmexretail_stl &lt;- mexretail |&gt;                                \n  model(                                                      \n    stl = STL(y ~ trend()) \n  )"
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-3",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#stl-in-r-using-fable-3",
    "title": "Time Series Decomposition",
    "section": "STL in R using fable",
    "text": "STL in R using fable\n\nThe code is basically the same as for the classical decomposition. We just need to change the model used inside the model() function.\n\n\nmexretail |&gt;                                \n  model(                                                      \n1    stl = STL(y ~\n2                trend(window = NULL) +\n3                season(window = \"periodic\"),\n4              robust = TRUE)\n  ) |&gt; \n  components() |&gt; \n  autoplot()\n\n\n1\n\nInside the STL() function, we can specify the formula for the decomposition, or don’t specify it at all. See ?STL for more details.\n\n2\n\nThe trend() function is used to specify the trend component of the decomposition. The window argument controls the smoothness of the trend component. A larger window results in a smoother trend.\n\n3\n\nThe season() function is used to specify the seasonal component of the decomposition. The window argument controls the smoothness of the seasonal component. Setting it to “periodic” means that the seasonal component will be fixed over time.\n\n4\n\nThe robust argument, when set to TRUE, makes the STL decomposition more robust to outliers in the data, so the effect of such values is sent to the residual component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting formulas in R\n\n\nIn R, we use “\\sim” instead of “=” in formula specification, i.e., y \\sim mx + b."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#footnotes",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp_pres.html#footnotes",
    "title": "Time Series Decomposition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA time series can have multiple seasonal patterns.\nThey usually last at least 2 years.\nIn R, you can compute any moving average by using the slider::slide_dbl() function.\nshort for “model table”\nThere are other decomposition methods primarily used by official statistics agencies, such as X-11, X-12-ARIMA, and TRAMO/SEATS. However, these methods are not as widely used in the forecasting community as STL. For more on these, see this."
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html",
    "title": "Time Series Decomposition",
    "section": "",
    "text": "Code\n1library(tidyquant)\nlibrary(plotly)\n\n\n\n1\n\nIn addition to the regular packages, here we’ll use tidyquant and plotly",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#ts-patterns",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#ts-patterns",
    "title": "Time Series Decomposition",
    "section": "1.1 TS Patterns",
    "text": "1.1 TS Patterns\nTime series can have distinct patterns:\n\n\nTrend: A long-term increase/decrease in the data.\nSeasonal: Fluctuations in the time series with a fixed and known period1.\nCycles: More commonly known as “Business cycles”, refer to rises and falls that are not of a fixed frequency2.\nChanges in variability: Changes in the spread of the data over time, i. e., an increase/decrease in the variance as the level of the series increases/decreases.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#components-of-a-time-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#components-of-a-time-series",
    "title": "Time Series Decomposition",
    "section": "1.2 Components of a Time Series",
    "text": "1.2 Components of a Time Series\nA time series can be decomposed into the following components:\n\n\nSeasonal component (S): The repeating short-term cycle in the series.\nTrend-cycle component (T): The long-term progression of the series.\nResidual component (R): The residuals or “noise” left after removing the seasonal and trend-cycle components.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#log-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#log-transformations",
    "title": "Time Series Decomposition",
    "section": "2.1 Log transformations",
    "text": "2.1 Log transformations\n\nSeries in levelsLog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations and adjustments help us simplify the patterns in our data, and can improve our forecasts’ accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog transformations are often useful when the data presents an increasing/decreasing variation with the level of the series.\nLog transformations are very interpretable: changes in a log value are percent changes on the original scale.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#box-cox-transformations",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#box-cox-transformations",
    "title": "Time Series Decomposition",
    "section": "2.2 Box-Cox transformations",
    "text": "2.2 Box-Cox transformations\n\nw_t= \\begin{cases}\\log \\left(y_t\\right) & \\text { if } \\lambda=0 \\\\ \\left(\\operatorname{sign}\\left(y_t\\right)\\left|y_t\\right|^\\lambda-1\\right) / \\lambda & \\text { otherwise }\\end{cases}\n\n\nIn a Box-Cox transformation, the log is always a natural logarithm. The other case is just a power transformation with scaling.\n\nWhat happens when \\lambda = 1?\n\n\n\n\n\n\nYou should choose a value of \\lambda that makes the size of the seasonal variation the same throughout the series.\n\n\n\n\n2.2.1 How can we choose the value of \\lambda?\nWe can use the guerrero feature to choose an optimal lambda.\n\n\nCode\naus_production |&gt; \n  features(Gas, features = guerrero)\n\n\n# A tibble: 1 × 1\n  lambda_guerrero\n            &lt;dbl&gt;\n1           0.110",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#calendar-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#calendar-adjustments",
    "title": "Time Series Decomposition",
    "section": "3.1 Calendar adjustments",
    "text": "3.1 Calendar adjustments\n\nClosing price and volumeMonthly aggregationMonthly total and mean and volume\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngoogle_month &lt;- google |&gt; \n  index_by(month = yearmonth(date)) |&gt; \n  summarise(\n    trading_days = n(),\n    monthly_volume = sum(volume),\n    mean_volume = mean(volume)\n  )\n\ngoogle_month\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of trading days in a month can vary due to weekends and holidays, and not because of any economic reason.\nUsing the monthly total volume can be misleading, as months with more trading days will naturally have higher total volumes.\nUsing the mean volume per trading day helps to standardize the data, making it easier to compare across months.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#population-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#population-adjustments",
    "title": "Time Series Decomposition",
    "section": "3.2 Population adjustments",
    "text": "3.2 Population adjustments\n\n\n\n\n\n\nIs the Mexican economy really that similar Australia’s economy? Is Iceland’s economy really that small?\n\n\nA greater GDP can be interpreted as having a larger economy, and a better life standard, but this is not always the case.\nComparing GDP across countries with different population sizes can be misleading.\nGDP is often used to measure the economic performance of a country, but it doesn’t account for population size.\nThe higher the population, the higher the GDP tends to be, simply because there are more people contributing to the economy.\nA more meaningful comparison can be made by looking at GDP per capita, which divides the GDP by the population size.\n\n\n\nPopulationGDP per capita\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the fabletools package.\n  Please report the issue at &lt;https://github.com/tidyverts/fabletools/issues&gt;.\n\n\n\n\n\n\nThe population sizes of these countries are very different.\n\n\n\n\n\n\n\n\n\n\nGDP per capita provides a more accurate representation of the economic well-being of individuals in a country.\nIt is clear now that Iceland and Australia have a much higher GDP per capita compared to Mexico, indicating a higher standard of living for its residents.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#inflation-adjustments",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#inflation-adjustments",
    "title": "Time Series Decomposition",
    "section": "3.3 Inflation adjustments",
    "text": "3.3 Inflation adjustments\n\n\nInflation is the rate at which the general level of prices for goods and services is rising, and subsequently, purchasing power is falling.\nTo make meaningful comparisons of economic data over time, it is essential to adjust for inflation.\nThis adjustment is typically done using a price index, such as the Consumer Price Index (CPI). In Mexico, the National Consumer Price Index (INPC) is used. INEGI provides this data.\n\n\n\n3.3.1 Inflation adjustment formula\n\nx_t = \\frac{y_t}{z_t} * z_{2010}\n\n\nwhere:\n\ny_t is the original value at time t (nominal value).\nz_t is the price index at time t (e.g., INPC).\nz_{2010} is the price index in the base year (2010 in this case).\nx_t is the inflation-adjusted value at time t (real value).\n\n\n\n\n3.3.2 Inflation adjustment example\n\nNominal valuesReal valuesNominal vs. Real values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\naus_economy &lt;- global_economy |&gt;\n  filter(Code == \"AUS\")\n\n\nprint_retail &lt;- print_retail |&gt; \n  left_join(aus_economy, by = \"Year\") |&gt;\n  mutate(Adjusted_turnover = Turnover / CPI)",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#types-of-decompositions",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#types-of-decompositions",
    "title": "Time Series Decomposition",
    "section": "4.1 Types of Decompositions",
    "text": "4.1 Types of Decompositions\n\nA decomposition splits the time series into its underlying components:\n\nTrend-cycle\nSeasonal pattern(s)\n\nAnd what’s left of it we simply call it a “remainder component”.\nIn general, there are two types of decompositions:\n\n\n4.1.1 Additive decomposition\n\ny_t = T_t + S_t + R_t\n\n\n\n4.1.2 Multiplicative decomposition\n\ny_t = T_t \\times S_t \\times R_t \\\\\n\n\nWhich one should you use?\n\n\n\nIf the seasonal variation is roughly constant over time, use an additive decomposition.\nIf the seasonal variation increases or decreases with the level of the series, use a multiplicative decomposition.\nIf you’re unsure, you can try both and see which one provides a better fit.\n\nA multiplicative decomposition is equivalent to an additive decomposition of the log-transformed series:\n\ny_t = T_t \\times S_t \\times R_t\n\nis equivalent to\n\n\\log(y_t) = \\log(T_t) + \\log(S_t) + \\log(R_t)",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#seasonally-adjusted-series",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#seasonally-adjusted-series",
    "title": "Time Series Decomposition",
    "section": "4.2 Seasonally adjusted series",
    "text": "4.2 Seasonally adjusted series\n\nOne use of decomposition is to obtain a seasonally adjusted series, which is the original series with the seasonal component removed.\nSeasonally adjusted series can be useful for:\n\nIdentifying and analyzing the trend-cycle component without the influence of seasonal fluctuations.\nMaking comparisons across different time periods without seasonal effects.\n\n\n\n\n\n\n\n\n\nFor an additive decomposition, the seasonally adjusted series is given by: \ny_t - S_t\n\nFor a multiplicative decomposition, the seasonally adjusted series is given by: \n\\frac{y_t}{S_t}",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#classical-decomposition",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#classical-decomposition",
    "title": "Time Series Decomposition",
    "section": "4.3 Classical decomposition",
    "text": "4.3 Classical decomposition\n\nThe classical decomposition is a simple method for decomposing time series data. It assumes that the components of the time series are additive or multiplicative.\n\n\nThe trend-cycle component is estimated using a moving average.\n\nAn m order moving average is given by:\n\n\\hat{T}_{t}=\\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}\n\nwhere k = (m-1)/23.\n\nThen, the seasonal component is estimated by averaging the detrended values for each season.\nFinally, the remainder component is obtained by subtracting the trend-cycle and seasonal components from the original series.\n\n\n4.3.1 Example of a classical decomposition\n\n\nCode\n1mexretail_dcmp &lt;- mexretail |&gt;\n2  model(\n3    classical = classical_decomposition(y, type = \"additive\")\n  ) |&gt; \n4  components()\n\n5mexretail_dcmp\n\n\n\n1\n\nWe start with our original tsibble.\n\n2\n\nInside the model() function, we specify the type of models we want to use.\n\n3\n\nIn any model used, the first thing we need to specify is our forecast variable. Then, depending on the model used, we can specify additional parameters. The model() function yields a mable4, which is a table that contains the fitted models for each time series in the tsibble.\n\n4\n\nThe components() function is used to extract the components of the decomposition (trend-cycle, seasonal, and remainder) from the fitted models in the mable. It also provides the seasonally adjusted series.\n\n5\n\nFinally, we store the result.\n\n\n\n\n\n  \n\n\n\n\n\nCode\nmexretail_dcmp |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Problems of using a Classical decomposition\n\n\nThe trend-cycle component is not estimated at the beginning and end of the series. This can be problematic if you want to forecast the series.\nIt also tends to over-smooth rises and falls.\nIt assumes that the seasonal component is constant over time, which may not be the case in many real-world scenarios.\nIt is not robust to outliers, which can significantly affect the estimates of the components.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIt is not recommended to use classical decomposition for forecasting because of these issues.",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  },
  {
    "objectID": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#footnotes",
    "href": "docs/modules/module_1/02_ts_dcmp/ts_dcmp.html#footnotes",
    "title": "Time Series Decomposition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA time series can have multiple seasonal patterns.↩︎\nThey usually last at least 2 years.↩︎\nIn R, you can compute any moving average by using the slider::slide_dbl() function.↩︎\nshort for “model table”↩︎\nThere are other decomposition methods primarily used by official statistics agencies, such as X-11, X-12-ARIMA, and TRAMO/SEATS. However, these methods are not as widely used in the forecasting community as STL. For more on these, see this.↩︎",
    "crumbs": [
      "1. Forecasting models based on decomposition methods",
      "1.2 Time Series Decomposition"
    ]
  }
]